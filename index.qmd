---
title: "Beyond counting clients: Developing a measure of clinician workload with machine learning"
shorttitle: "Short Title in Running Header"
author:
  - name: Shauna Heron
    corresponding: true
    orcid: 0000-0002-9262-6718
    email: sheron@laurentian.ca
    affiliations:
      - name: Laurentian University
        department: Department of Psychology
        address:  
        city: Sudbury
        region: ON
        country: Canada
        id: lu
      - name: Laurentian University
        department: Department of Mathematics & Computer Science
        address:  
        city: Sudbury
        region: ON
        country: Canada
        id: lumsc
  - name: Michael Emond
    department: Department of Psychology
    affiliations:
      - ref: lu
    role:
      - supervision
  #    - editing
  - name: Luc Rousseau
    affiliations:
      - ref: lu
    role:
  #     - supervision
  #     - editing
      - advisory committee
  - name: Kalpdrum Passi
    affiliations:
       - ref: lumsc
    role:
       - supervision
  #     - editing
       - advisory committee
  - name: Nicholas Schwabe
    affiliations: Compass
    role:
  #     - conceptualization
  #     - editing
       - advisory committee
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    study-registration: ~
    data-sharing: ~
    related-report: ~
    conflict-of-interest: " "
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
abstract: "As child and youth mental health (CYMH) providers face increasing demands for service, an ability to anticipate and optimize staff caseloads is critical to maintaining provider well-being and delivering equitable, high-quality care. Yet so far, there are a lack of efficient and reliable tools to support this decision making in a way that is cost-effective, fair and accounts for individual client need. Manual review of client records, necessary to fairly and efficiently allocate new clients and monitor current workload is untenable in the face of the same workforce shortages. With this gap in mind, we propose examining the utility of leveraging machine learning algorithms trained on electronic mental health records (EHR) to estimate the number of hours per week that individual clients contribute to a provider's work. Specific objectives include: (i) identify the best features to predict client-related work (caseweight) from structured demographic, administrative and assessment EHRs at the earliest stages of client contact (i.e., intake screener scores) and at intervals throughout treatment (i.e., visit counts,  days since last contact, says since last crisis); ii) compare tree-based and neural network machine learning algorithms in their ability to predict client-related work; iii) compare the utility of modelling a continuous index of work (hours per week) compared to a classification of work intensity (i.e., low, medium, high); (iv) explore the potential for early and ongoing prediction of case weight based on individual client need."
language: 
  citation-last-author-separator: "&"
floatsintext: true
keywords: [workload, caseload, case management, data science, machine learning, organizational psychology]
bibliography: bibliography.bib
link-citations: true
format:
  apaquarto-html:
    comments:
      hypothesis: true
  apaquarto-docx: default
#  apaquarto-pdf: default
#    documentmode: man
---

```{r}
#| echo: false
#| warning: false
library(fontawesome)
library(usethis)
#
```

This is a test
Amidst growing demand for child and youth mental health services in Ontario and beyond, human resource challenges have been identified as a significant area of concern [@childrensmentalhealthontario2022]. In Ontario, a 2020 survey of community child and youth mental health (CYMH) centres revealed that 83% of agencies reported staffing shortages–59% of them direct-service, clinical roles (i.e., psychologists, psychotherapists, and social workers)--which is a concern, as without an adequate workforce, children, youth and their families experience longer wait times and gaps in service that impact access to treatment [@childrensmentalhealthontario2022]. Illustratively, the same CMHO survey reported that 28,000 children and youth in Ontario were waiting up to 2.5 years for mental health services, some even "aging out" of the system before they are off the wait list [@cmho2019; @cmho2020; @cymhlac2019]. With over 70% of mental health and addiction problems starting before age seventeen, this is a problem[@cmho2019; @cmho2020]. Not only is a critical opportunity for early intervention missed, but individual and family stress related to mental health challenges are compounded, increasing the burden to a public health care system, where in Ontario, hospitalization of youth with mental health and addictions issues are up by an estimated 90% [@childrensmentalhealthontario2022; @cymhlac2019]. At the same time, when demand outpaces staffing, existing providers are having to manage higher client volumes containing more complex cases, perpetuating a cycle of provider burnout, absenteeism and high turnover [@comeau2019; @king2009]. For this reason, the ability to anticipate and monitor the contribution of individual cases to the overall work of providers is critical to improving client outcomes and minimizing provider burnout [@king2004; @king2004a; @king2000a].

According to recent reports by the Auditor General on Child and Youth Mental Health, a key issue limiting agencies’ ability to meet rising demand is the challenge of establishing provider-to-client workload ratios that accurately reflect the varying needs and levels of intervention required by different clients [@officeoftheauditorgeneralofontario2016; @officeoftheauditorgeneralofontario2018]. Ideally, caseloads should decrease as case complexity increases; however, the resources required to manually evaluate each child’s needs across hundreds of cases are beyond what most public agencies can support (CMHO, 2019). As a result, cases are often assigned under the assumption that each one requires a similar level of effort, leading to inequitable case distribution. Consequently, some clinicians consistently manage a higher proportion of complex cases than others [@king2009; @childrensmentalhealthontario2022]. For example, an agency might target 20 cases per provider for counseling services, meaning that providers with fewer than 20 cases are considered to have “room” for more, regardless of case complexity.

Given this state of affairs, if there was an automated tool that could estimate a weight for each case based on information in the client records, agencies might more efficiently evaluate and manage provider caseloads, which in turn might reduce workload-associated provider burnout and improve client outcomes. However, the development of sophisticated predictive tools to augment clinical decision making has been hampered by several factors: i) a lack of resources across the public health sector generally; ii) limits imposed by a mostly paper-based client record system, iii) disagreements on how “workload” should be defined and iv) lack of computing power and expertise in modeling complex, electronic health data. However, the transition of CYMH services in Ontario from paper-based health records to electronic records, combined with increased computational power and advances in computer science have opened the door to leveraging EHR with algorithms to better anticipate and manage healthcare resources and outcomes.

With this gap in mind, the current research proposes to explore the feasability of predicting the work associated with a given case at different points in the client timeline to examine whether such predictions could provide added value to clinical practice. The assumption underlying the research, is that there are historical patterns that predict future mental health resource use and that such patterns can be identified in electronic mental health records (EHR), despite its sparseness, noise, errors and systematic bias.

## Case-mix Review

Across health domains (e.g., psychiatric and emergency medicine) various strategies have been employed to manage provider workload by determining service levels with client characteristics like symptom severity or prior diagnoses [@johnson1998; @tran2019]. These systems assume that although the needs of each individual in a population will be unique, there will be shared characteristics that determine the type of treatment they need (e.g., family counselling versus substance use treatment). These groups represent the mix of cases in a given caseload which is often used to estimate cost, based on the types of care the population needs and the time involved in servicing those needs [@tran2019; @johnson1998].

Case-mix classification systems have been used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use [@tran2019; @johnson1998]. Casemix algorithms assume that though the needs of an individual will be unique, there are shared characteristics that determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically these systems are informed by information contained in case records. At the agency level, case records contain a variety of information, including provider-level information like the number of direct and indirect-hours attributable to individual clients, as well as client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services).

Typically, case-mix systems take one of two approaches to classification [@cmho2019]. Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight attached to it (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population. For example, a client accessing long-term counseling and therapy services might be assigned a different weight in terms of expected resource use than a client accessing a one-session brief service. Index systems on the other hand, combine different case characteristics to provide a value that maps to expected resource use or acuity of needs (e.g. a case weight or case complexity score that ranges from 0, the least complex, to 1 the most complex) [@tran2019]. Indexing systems are often used to triage cases by assigning a score to new clients based on answers to an intake assessment. Often there is a threshold score above which clients are considered acute and may receive services more quickly, at the same time scores that are below a specific threshold may not qualify for publicly funded services at all. For instance, a youth reporting thoughts of suicide or other self-harming behaviour might be scored higher than a youth reporting problems focusing in school.

While the term *algorithm* gives the impression that these are sophisticated mathematical models, most often they are conceptual, rules-based frameworks that rely on manual review of client files, rather than an automated, data-driven tool. Although research has begun to explore how machine learning could automate and streamline classification processes, prior work has focused primarily on inpatient, acute-care settings, which differ markedly from community-based out-patient settings [@tran2019] where clear diagnostic criteria and predictable recovery paths are more variable. For instace the healing process for a broken arm has a relatively fixed timeline and treatment protocol, but recovery from anxiety or depression is more nuanced and individualized, making it inherently more complicated to model.

The difficulties inherent in modelling electronic mental health data specifically are underscored by the fact that only a handful of studies have looked at solving this problem despite an urgent need. Indeed, a 2019 scoping review of casemix literature in community-based mental health care found only a single case that looked at data-driven methods to predict mental health care resource [@tran2019; @martin2020]. In that study, researchers modeled 4573 client records from eleven UK outpatient CYMH agencies, comparing cluster analysis, regression trees and a conceptual classification based on clinical best practice guidelines to predict the number of appointments a client attended in treatment [@martin2020], finding the data-driven classification no more clinically meaningful than conceptual classification in accounting for number of appointments and there was little evidence to support the idea that either client complexity or context factors (with the exception of school attendance problems) were linked to overall appointment counts [@martin2020]. Moveover, the models failed to explain significant variation in resource provision between workers despite clients exhibiting similar characteristics. Data quality problems and omission of important individual-level factors were cited as potential points of failure but suggesting their results merited further testing and development.

In a related cohort, a group of researchers tried to predict the work associated with client features at a community-based mental health centre for the elderly [@baillon2009]. Using an 8-item self-designed case weighting scale (CWS) researchers identified factors staff felt contributed to demand for time. A multiple regression model was used to assign different weightings to predictors based on the strength of its relationship with the outcome (an estimation of time spent on each client logged over a four-week period). The resulting coefficients were then added to a spreadsheet and used to predict the total time a client would utilize in a four-week period following the first appointment based on the 8 characteristics. Though the model was reported a success, accounting for 58% of the variance in time spent on client-related work, the sample consisted of only 87 cases leaving it unclear how accurate the model really was [@baillon2009; @mansournia2021]. Moreover, inter-rater and re-rater reliability results indicated that the assessment, whether from a client's self-report or a professional's clinical opinion, did not necessarily relate to the amount of time needed by clients [@baillon2009].

### Machine learning, a novel approach to modeling case-mix

Considering the challenges outlined by prior research in modeling the high-dimensional, sparse data characteristic of EHRs, we next looked to a growing body of research leveraging machine learning algorithms to model electronic health data. Machine learning (ML) is a branch of artificial intelligence that uses statistical techniques that enable computers to learn patterns from data without explicit instructions [@nielsen2016]. In mathematical terms, machine learning algorithms use statistical techniques to optimize a model’s parameters. This process involves minimizing a loss function that quantifies the difference between the model’s predictions and the actual data. For example, in supervised learning, the goal is to find a function f(x) that maps input features X to an output Y such that the predicted outcomes are as close as possible to the true outcome [@nielsen2016]. This approach is highly effective for complex tasks like image recognition, natural language processing, and predictive analytics, where traditional rule-based programming is infeasible due to the high dimensionality and variability of the data. ML algorithms are particularly well-suited to modeling the complex, high-dimensional data found in EHRs [@chen2023; @an2023].

Within the inpatient mental health domain, machine learning has mostly been used to predict specific events like substance relapse, self-harm and suicide risk. However a recent study leveraged ML to build a model that *continuously* monitors patient records to predict crisis-relapse over a 28 day period [@garriga2022]. The winning XGBoost regression demonstrated good accuracy in distinguishing between cases who were likely and unlikely to experience a crisis in the next 28 days. Specifically, the model could correctly differentiate those at risk from those not at risk about 80% of the time [@garriga2022]. Morever, in a subsequent post-hoc case study, healthcare professionals rated the predictions produced by the model valuable for managing patient care in 64% of cases, helping them to prioritize patients more effectively and potentially prevent crises [@garriga2022]. Though the author's did not model the resource use directly as we hope to do, 'crisis-risk' served as a proxy for work. By predicting crises, they could anticipate the increased resource-demand which they hoped could inform better case prioritization and managment.

## The current study

Building on insights from @garriga2022, this research aims to explore the feasibility of estimating the number of weekly direct hours a case may require, assessed at 28-day intervals. The underlying assumption is that historical patterns can reliably predict future mental health resource use and that such patterns are identifiable in electronic mental health records (EHR), despite challenges such as sparsity, noise, errors, and systematic bias.

To test these assumptions, we will use a retrospective, deidentified dataset from a large child and youth mental health (CYMH) agency in Ontario, encompassing clients served from 2019 to the end of 2023. Although largely exploratory, the study will be guided by several hypotheses. First, based on @garriga2022's research, we expect that predictions will be weakest early in the client journey when EHR information is limited to an intake screener and basic demographics. However, as more data accumulates across the treatment timeline, we anticipate that prediction accuracy will improve.

Furthermore, we hypothesize that for new clients, mental health acuity features such as externalizing behaviors and a history of self-harm or suicide attempts will be stronger predictors of required resources. For known clients, however, we anticipate that time-based factors, such as number of no-shows or number of crisis events will be more predictive of workload needs. 

Finally, we expect the winning machine learning algorithm to outperform the baseline model which mimics a conceptual classification system and is the standard way that resource use is estimated by agencies today (e.g., counseling and therapy will require more provider-hours than a brief service) [@childrensmentalhealthontario2022]. 
Finally, we expect the winning machine learning algorithm to outperform the baseline model, which is intended to mimic the conceptual way that resource use is estimated by agencies today (e.g., counseling and therapy will require more provider-hours than a brief service) [@childrensmentalhealthontario2022]. 

# Methodology

Considering we want to predict resource use (hours of direct and indirect service) at regular stages in the client journey, only cases with a completed initial screener will be included in the analysis. Final counts after screening will be reported and added to the flowchart before analysis. The study will be conducted at Compass Child and Youth Family Services which is the largest CYMH agency in northern Ontario, serving a culturally and socially diverse population of children, youth and families. The study will utilize a retrospective dataset containing deidentified cases with completed intake assessments who were active between January 1, 2019 and December 31, 2024. The general flow of clients through Compass is illustrated in @fig-client-selection.

![Clinical pathway](images/client_pathway.svg){#fig-client-selection apa-note="Flow chart of client selection process. Clients who complete an intake assessment will be screened for inclusion. Only clients referred to Counselling and Therapy (CT) services will be included in the analysis. Predictions will include: client-related work at follow-up assessment and client-related work at end of treatment."}

## Data Security

Given the sensitivity of mental health data, ensuring data privacy and security by obtaining the necessary ethical approvals and maintaining transparency throughout the research process, will be strictly enforced. The necessary approvals from relevant ethics boards will be obtained. An exemption must be granted by both the agency (Compass) and Laurentian's institutional review board for the use of de-identified data.

De-identified clinical data will be acquired from an electronic health information system belonging to Compass. The EHR database is maintained by the institution. Data will be de-identified at extraction using the Health Insurance Portability and Accountability Act Safe Harbor Method [@rightsocr2012]. This means that names, addresses, birthdates, postal codes and any other directly identifying information will be stripped from the dataset before any analysis begins. As an added precaution, unique client identification codes will be encrypted with a hashing system that makes it near impossible to reverse engineer the code to obtain original IDs. Furthermore, the data will not leave the custody of Compass and will only be analyzed by the principal researcher within a password-protected machine belonging to Compass.

The reporting of model results, summary statistics and other visualizations will only include metrics associated with the performance of predictors and the models themselves, never individual scores or any other identifying information that could be linked to clients or smaller subgroups of clients. Furthermore, the researchers will seek approval from Compass before results are shared or utilized in any report or presentation.

## Dataset

The de-identified data will include approximately 6000 records containing demographics information, referrals, diagnoses, risk and well-being assessments and crisis events for all outpatients. Cases younger than 5 and older than 17 years will be excluded as Compass' core services are only offered to children and youth under 18. There are no plans to exclude cases based on any other feature, including diagnoses, however if for whatever reason this changes, it will be outlined in the documentation.

## Procedure

The following steps outline the proposed process which will consist of four phases: 1) data cleaning, preprocessing and exploration; 2) data splitting; 3) aggregating workload proxies (direct and indirect services hours) that will be used as stand-ins for actual workload; 4) identifying and aggregating indicators of workload (i.e.,independent variables/features) that will be used to model our proxies; 5) modeling the relationship between the indicators and proxies with machine learning algorithms of varying complexity; and 6) evaluating the accuracy of predictions on the unseen test data (see @fig-procedure-flow).

![Experimental procedure](images/experimental-modelling-procedure.svg){#fig-procedure-flow apa-note="Flowchart of the experimental procedure. Data will be split into training and test sets by client ID. The training set will be used to train the models using 10-fold group cross-validation, while the test set will act as a control group to evaluate the models' performance on unseen data."}

## Data Preprocessing

After deidentification, data preprocessing will involve cleaning, joining dataframes, handling missing values if necessary, and aggregating features across weeks. All decisions we make in regard to missing data, data normalization or any other changes will be decided on a case by case basis and reported in our final paper. Pending approval from the host agency, the Python data scripts will also be made publicly available.

Importantly, feature engineering–the creation of new predictors based on existing variables in the dataset–will occur *after* data splitting on the training data to minimize the risk of data leakage that could inadvertently occur when creating new variables from the full dataset [@sheetal2023]. The main criterion for inclusion in the model will be the variable's availability in the electronic health record (EHR) system at intake. There will be four primary types of predictor: i) time based; ii) count based; iii) recent information; iv) static and semi-static information.

With the exception of static information like presenting concern or referral source, all EHR data will include the associated date and time. The date and time refer to the moment when the specific event or assessment occurred—that is, the date and time that there was a contact with a client. To prepare the data for the modeling task, each client's case records will be consolidated at a weekly level according to the date associated with the record. Following this process, evenly spaced time series will be generated for each client spanning from their first interaction with Compass to the study’s final week. The features and labels generated for each week will be computed using the data from dates prior to that week. Static data that is susceptible to change over time (for example, postal code or school board information) will be removed to mitigate the risk of retrospective leakage.

![](images/paste-1.png)

### Outcome (target) generation

**Caseweight prediction modeling and evaluation.** The caseweight prediction task will be defined as both a continuous regression problem to be performed on a weekly basis and a continuous classification problem on the same timeline (low, medium, high intensity). For each week, the model will estimate the weekly hours needed during the upcoming 28 days. A rolling window approach will be applied to allow for a periodic update of the caseweight by incorporating newly available data (or the absence of it) at the beginning of each week. This approach is common in settings where predictions are to be used in real time and when data are continuously updated [@garriga2022].

To construct a continuous case-weight prediction target, the sum of client-related direct and indirect hours logged by clinicians and associated with a specific program and client will be aggregated at the same weekly level as the features, based on the time recorded by the worker prior to that week. We also intend to examine which measure of client-related time is most stable and reliable over time: the summed direct and indirect time or either on its own.

### Features (predictors) generation.

We will extract features from a total possible feature set of approximately 250 features. See @tbl-predictors for a list of proposed feature groupings and variables that we aim to include. Informed by @garriga2022's methodology, extraction will be performed according to six data characteristics:

**Static or semi-static features.** demographics data will be represented as constant values attributed to each case with age treated as a special case that changes each year.

**Diagnosis features.** Client will be assigned their latest valid diagnosed developmental disability or psychological disorder (if contained in the record) or an 'un-diagnosed' label and then seperated into diagnostic groups according to the latest valid diagnosed disorder at the last week of the training set to avoid leakage into the validation and test sets. Any codes created for this feature will be added to the final paper.

**EHR weekly aggregations.** Records related to client-agency interactions will be aggregated on a weekly basis for each client. The resulting features will constitute counts of each type interaction and one-hot encoded according to their categorization. One-hot coding is a type of dummy variable where if a specific event did occur there will be a 1 for that week and if not a value of 0 will be assigned to the feature related to the corresponding event for the corresponding week.

**Time-elapsed features.** At each client-week, for each type of interaction and category, we will construct a feature that counts the number of elapsed weeks since the last occurrence of the corresponding event. If the client never experienced such an event type up to that point in time, an NA value will be used.

**Last crisis episode descriptors.** For each crisis episode, a set of descriptors will be used to build feature for the subsequent weeks until the next crisis events occurs. If the client never had a crisis event up until that point in time, NA values will be used.

**Last assessment descriptors.** For each assessment item, a set of descriptors will be used to build a feature for the subsequent weeks until the next assessment occurs. All clients will have at least one assessment to be included in the study. A penalty will reduce the value of assessment scores the further away from the assessment date they are for each aggregated week.

**Status features.** For specific records, characterized by a start and end date, features for the corresponding weeks will be built by assigning their corresponding value (or category); otherwise they are set to NA.

In addition to record-based features, we will also add the week number (of a year 1-52) to account for seasonality effects.

```{r}
#| echo: false
#| warning: false
#| label: tbl-predictors
#| tbl-cap: Planned Features (Predictors)
#| tbl-cap-location: top
library(tidyverse)
library(gt)
library(gtExtras)
library(readxl)

table <- read_excel("Predictors.xlsx")
table |> gt() |> gt_theme_pff() |> fmt_missing(columns = everything(), missing_text = " ")

```

### Data Splitting

To maximize the generalizability of our models, our aim is to apply a time-based 80/10/10 training/validation/test split depending on the amount of data left over after data cleaning. Roughly, training data will start in the first week of January 2019 and end the last week of April 2023. Validation data will start in the first week of May 2023 and end in the last week of October 2023. Test data will start in the first week of November 2024 and end at the end of April 2024. Data from the first 6 months of the COVID-19 pandemic may be omitted due to disruptions in normal service delivery that continued until new policies and procedures could be implemented depending on the irregularity of these data.

Timed based cross-validation will be used to tune model parameters. The cross-validation folds will be created with a portion of the training data, subdividing it into 10 subsets, or "folds," that will preserve the same time structure. Evaluations will be conducted on a weekly basis and each week's results will be used to build confidence intervals on the evaluated metrics. All reported results will be computed using the test set if not otherwise indicated. The test set will act as a control group to evaluate the models' performance on "unseen data" at the very end of the training and tuning processes. By keeping the test set separate and untouched during training, we ensure that our final evaluation provides a better estimation of how the models will perform in practice. This final step is crucial for assessing the models' generalizability and for identifying any overfitting that may have occurred during training.

A final and complete list of all variables will be included in the final report.

### Model Selection

We plan to utilize the following supervised machine-learning algorithms for both regression and classification problems: i) Random Forest (RF) for its ability to handle large datasets with high dimensionality; ii) XGBoost, known for its predictive accuracy on tabular datasets, iii) Random Forest; iv) Feed Forward Neural Network, and finally iv) linear and logistic regression will be used to model our clinical baseline. All algorithms will be trained on the same aggregated training set and cross validation folds and evaluated on the same test set. These algorithms were chosen based on their success modeling similarly complex, tabular data types and may grow to include other models in the final paper [@salditt2023; @sheetal2023]. See @tbl-models for a list of proposed models.

**Machine Learning Models and Classifiers.** We plan to utilize versions of the following supervised machine-learning algorithms depending on the specific target outcome. Since we will be modeling and comparing a continuous and a classification outcome, this approach will allow us to assess performance across different types of predictions. XGBoost, an implementation of gradient boosting machines (GBMs), will serve as our primary algorithm due to its ability to handle missing data and robustness to scaling factors, which will eliminate the need for imputation or scaling. GBMs build a sequence of decision trees, where each tree improves on the performance of prior iterations, making them well-suited for our caseweight prediction task. To benchmark performance, we will compare XGBoost to a selection of state-of-the-art machine learning classifiers, including logistic regression, naive Bayes, random forest, and neural networks (specifically, multi-layer perceptrons), all of which have been successfully applied to similar prediction tasks with electronic health records (EHRs). For these classifiers, we will apply standard scaling and imputation as needed to ensure comparable conditions. We will conduct 100 hyperparameter optimization trials for each classifier to identify optimal parameters, with detailed search spaces provided in the supplementary materials.

**Hyperparameter Tuning and Feature Selection.** To optimize the models’ hyperparameters, we will maximize the area under the receiver operating characteristic curve (AUROC) for classification outcomes and the mean squared error (MSE and MAE) for continuous outcomes using a Bayesian optimization approach. Specifically, we will use Hyperopt, a sequential model-based optimization algorithm that applies Bayesian optimization via the Tree-structured Parzen Estimator, which accommodates a variety of distributions across search spaces. This flexibility will make Hyperopt particularly effective for tuning hyperparameters across all classifiers. We will use the same approach for feature selection, grouping features by information gain and adding a binary indicator to determine whether each feature should be selected for the model. See @tbl-predictors for details on feature groupings and the feature selection.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-models
#| tbl-cap: Planned Machine Learning Models
#| tbl-cap-location: top
table2 <- read_excel("Predictors.xlsx", sheet=2)
table2 |> gt() |> gt_theme_pff() |> fmt_missing(columns = everything(), missing_text = " ")
```

## Validation and Testing

Final models will be statistically compared and evaluated on the test set using appropriate performance metrics depending on whether it is a regression task (mean absolute error or root mean squared error) or classification task (accuracy, precision, recall and area under the curve). The evaluations will help determine the accuracy, generalizability and robustness of each model [@salditt2023; @wang2021]. Final models will also be analyzed to identify which predictors were the most important in terms of estimate client-related work.

Furthermore, to enhance the interpretability of our model, we plan to implement SHapley Additive exPlanations (SHAP) for feature analysis [@lundberg]. SHAP is a method that helps quantify the contribution of each feature to the model's predictions, providing insights into how specific client characteristics and historical data points influence predicted weekly clinician hours. In a mental health care setting, interpretability is essential, as decisions directly impact client care and resource allocation [@feretzakis2024]. Clinicians and administrators need to understand not only the predicted workload but also the driving factors behind each prediction to ensure fair, personalized, and transparent decision-making . For instance, if certain factors like recent diagnoses or patterns of no-shows are highly influential, this can guide intervention strategies and inform staffing decisions tailored to client needs. SHAP's ability to provide such detailed, interpretable explanations makes it a critical tool for ensuring that the model’s predictions are aligned with clinical understanding and ethical care practices [@feretzakis2024].

## Software and Tools

A Python kernel will be used for model building and evaluation (Khun & Wickham 2020). Quarto Markdown will be used for documentation and reproducibility inside Positron IDE [@vanrossum1995].

# Limitations and Challenges

While our study aims to enhance understanding of client-related workload over time based on historical and real-time changes in client needs, several limitations should be acknowledged. First, our data is derived from a specific subset of the population—young people with mental health concerns in community outpatient settings—which may limit the generalizability of our findings to other demographics or healthcare settings. Additionally, although we are employing machine learning techniques to handle the complexity of electronic health data, these methods are not immune to biases present in the data itself. Systematic biases in the initial data collection process, such as underreporting, data entry errors, or misclassification, could influence the model's predictions.

Moreover, our reliance on electronic health records means that the quality and completeness of the data are contingent upon the accuracy and thoroughness of data entry made by providers. Missing data and inconsistencies are inherent challenges that could affect the robustness of our models. Additionally, many of the scale scores may be influenced by the subjective interpretation of the provider who administered the assessment. While we will attempt to reduce these issues, there is no guarantee that all biases can be fully mitigated.

Another limitation is the exclusion of provider-side variables from our models. While this decision is aimed at maximizing fairness in allocation of cases, it also means that potentially valuable information about resource utilization influenced by provider characteristics is not considered. This could impact the comprehensiveness and accuracy of our workload predictions. In future iterations, it might be interesting to introduce a feedback loop where staff perception of workload is accounted for with a weekly or monthly "caseload satisfaction" measure.

Finally, while predictive accuracy and interpretability are crucial, a prospective cohort study would be necessary as a next step to evaluate how effectively the model supports clinical decision-making in practice. Such a study would allow us to track how predictions influence clinician workload distribution and client outcomes over time, providing a deeper understanding of its practical benefits and potential drawbacks in a live clinical setting. @garriga2022 demonstrated this approach effectively, showing that prospective cohort studies can offer insights into the model’s impact on workflow, clinician satisfaction, and client care quality. In future research, implementing a cohort study could help validate the model’s usefulness and refine it for improved applicability in mental health care settings.

# Conclusion

In conclusion, this research represents a crucial step toward addressing the complex and growing demands within mental health services with a data-driven approach. By developing a machine learning model to predict clinician workload, we aim to offer actionable insights that support fair resource distribution and responsive service delivery. Our approach will not only contribute to the field of mental health care by enhancing our understanding of workload drivers but also aligns with the pressing need for scalable, automated and efficient care solutions. Ultimately, this research has the potential to improve outcomes for clinicians and clients alike, ensuring that mental health care services are equipped to meet the needs of vulnerable populations with greater precision and equity.

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

# Appendix

TABLES GO HERE

![Modeling Caseweight--client-related work](images/predicting_caseweight.svg){#fig-caseweightmodel apa-note="Using indicators of client-related work (e.g. depression scores, anxiety scores, etc.) in the electronic health record (EHR)to predict workload proxies. Adapted from *Predictors of Workload*, by @wang2021."}