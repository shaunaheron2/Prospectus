---
a client may require in the coming weeks---
title: "Beyond counting clients: Developing a measure of clinician workload with machine learning"
shorttitle: " "
author:
  - name: Shauna Heron
    corresponding: true
    orcid: 0000-0002-9262-6718
    email: sheron@laurentian.ca
    affiliations:
      - name: Laurentian University
        department: Department of Psychology
        address:  
        city: Sudbury
        region: ON
        country: Canada
        id: lu
      - name: Laurentian University
        department: Department of Mathematics & Computer Science
        address:  
        city: Sudbury
        region: ON
        country: Canada
        id: lumsc
  - name: Michael Emond
    department: Department of Psychology
    affiliations:
      - ref: lu
    role:
      - supervision
  #    - editing
  - name: Luc Rousseau
    affiliations:
      - ref: lu
    role:
  #     - supervision
  #     - editing
      - advisory committee
  - name: Kalpdrum Passi
    affiliations:
       - ref: lumsc
    role:
       - supervision
  #     - editing
       - advisory committee
  - name: Nicholas Schwabe
    affiliations: Compass
    role:
  #     - conceptualization
  #     - editing
       - advisory committee
author-note:
  status-changes: 
    affiliation-change: ~
    deceased: ~
  disclosures:
    study-registration: ~
    data-sharing: ~
    related-report: ~
    conflict-of-interest: " "
    financial-support: ~
    gratitude: ~
    authorship-agreements: ~
abstract: "As child and youth mental health (CYMH) providers face increasing service demands, anticipating and optimizing staff caseloads is critical to maintaining provider well-being and delivering equitable, high-quality care. However, there are a lack of efficient and reliable tools to support this decision-making in a way that accounts for variable client need and is cost-effective and fair. Manual review of client records, necessary to fairly and efficiently allocate new clients and monitor existing caseloads, is untenable in the face of the same workforce shortages. With this gap in mind, we propose examining the utility of leveraging machine learning algorithms trained on electronic mental health records (EHRs) to estimate the number of provider hours a client may require in the coming weeks. Specific objectives include: (i) identifying the features that best predict client-related provider hours from structured demographic, administrative and assessment EHRs at the earliest stages of client contact (i.e., intake screener scores) and at weekly intervals throughout treatment (i.e., aggregated visit counts,  days since the last contact); ii) compare tree-based and neural network machine learning algorithms in their ability to predict client-related provider hours; iii) compare the utility of modelling a continuous index of needed provider hours compared to a classification of the same (i.e., low, medium, high); (iv) conduct interpretability analyses to identify and explain the contributions of individual features to model predictions."
language: 
  citation-last-author-separator: "&"
floatsintext: true
keywords: [workload, caseload, case management, data science, machine learning, organizational psychology]
bibliography: bibliography.bib
link-citations: true
format:
  apaquarto-html:
    comments:
      hypothesis: true
  apaquarto-docx: default
#  apaquarto-pdf: default
#    documentmode: man
---

```{r}
#| echo: false
#| warning: false
library(fontawesome)
library(usethis)
#
```

Amidst the growing demand for child and youth mental health services, human resource challenges have been identified as a significant barrier to providing timely, high-quality care [@worldme2022; @childrensmentalhealthontario2022]. In 2020, a survey of Ontario community child and youth mental health (CYMH) centres revealed that 83% of agencies reported staff vacancies, 59% of them direct service, front-line positions (i.e., social workers, psychologists, and psychotherapists). This is a concern, as without an adequate and qualified workforce, children, youth and their families experience longer wait times, causing gaps in service that ultimately impact outcomes [@childrensmentalhealthontario2020; @comeau2019]. Illustratively, the same CMHO survey reported that 28,000 children and youth in Ontario were waiting up to 2.5 years for mental health services, some even "aging out" of the system before they were off the wait list [@childrensmentalhealthontario2020; @cymhlac2019].

With over 70% of mental health and addiction problems starting before age seventeen, any delay in access to service is a problem [@worldme2022; @cmho2019; @childrensmentalhealthontario2020]. Not only are critical opportunities for early intervention missed, but individual and family stress related to mental health challenges are compounded, increasing the burden to a public health care system, where in Ontario, hospitalization of youth with mental health and addiction issues has increased over the last 30 years by an estimated 90% [@childrensmentalhealthontario2020; @cymhlac2019]. At the same time, when demand outpaces staffing, existing providers often end up managing higher client volumes containing more complex cases, which can perpetuate a cycle of provider burnout, absenteeism and high turnover [@comeau2019; @king2009]. For this reason, the ability to anticipate and monitor the caseloads of providers is critical to improving client outcomes and minimizing provider burnout [@king2000; @king2009].

According to recent reports by the Auditor General of Ontario on Child and Youth Mental Health, a vital issue limiting agencies' ability to meet rising demand is the challenge of monitoring client-to-provider workload ratios in a way that accounts for individual client needs [@officeoftheauditorgeneralofontario2016; @officeoftheauditorgeneralofontario2018]. Ideally, as case complexity increases, the overall number of cases in a provider's portfolio (case count) should decrease; however, the administrative resources required to manually evaluate each case across dozens of caseloads are beyond what most public agencies can support [@cmho2019]. As a result, cases are often assigned under the assumption that each requires a similar level of effort [@cmho2019]. As a consequence, some clinicians consistently manage a higher proportion of complex cases than others [@king2009; @childrensmentalhealthontario2022]. For example, an agency might set a target of 20 cases per provider for counselling services, meaning that providers with fewer than 20 cases have "room" for more, regardless of how many complex cases they have in their overall portfolio.

This "casecount" approach to determining caseloads can result in significant disparities in work, particularly for more experienced clinicians who may be assigned more complex cases due to their expertise [@cmho2019; @king2000]. Complex cases may include those with severe behavioural challenges, high-risk family situations, or co-occurring mental health and developmental disorders, often requiring additional phone calls to coordinate with schools or other community supports, more frequent consultations with other professionals, longer or more detailed treatment plans, and extended documentation time [@cmho2019; @king2009]. Without a systematic way to monitor workload beyond case counts, administrators may unknowingly overburden some staff, assuming they have the capacity for more cases when they may already be overburdened [@king2009]. A reliance on providers to self-report when they feel overwhelmed creates an uneven system where some clinicians silently manage unsustainable workloads, which can lead to burnout and diminished care quality [@cmho2019; @king2009].

Given this state of affairs, if there was a data-driven tool that could quantify workload based on client complexity rather than counts, it might support clinical decision-makers in a fairer distribution of work [@king2009; @tran2019]. However, the development of sophisticated data-driven predictive tools to aid in clinical decision-making has been hampered by several factors: i) a lack of resources across the public health sector generally [@childrensmentalhealthontario2022; @officeoftheauditorgeneralofontario2018], ii) limits imposed by paper-based client record systems [@developi2019], iii) disagreements on how "workload" should be defined [@developi2019], and iv) lack of computing power and expertise in modelling complex electronic health record data in ways that remain transparent and interpretable [@xiao2018; @garriga2022]. However, the recent transition of CYMH services in Ontario from paper-based health records to electronic records, combined with increased computational power and advances in computer science, has opened the possibility of leveraging EHRs with machine algorithms to improve client outcomes.

With this gap in mind, the current research proposes to explore the feasibility of estimating the time that a given client might need from a provider at intervals across the treatment timeline using information contained in the EHR with the eventual goal of testing whether such predictions provide actual added value to clinical practice. The research assumes that historical patterns predict future mental health resource use and that such patterns can be identified in electronic mental health records (EHR) despite their inherent sparseness and systematic bias [@garriga2022].

## Case-mix History

Across health domains, particularly emergency medicine, various strategies have been employed to manage provider workload by mapping service levels to client characteristics like symptom severity or prior diagnoses [@johnson1998; @tran2019]. Case-mix classification systems have been used in the healthcare sector to help payers and agencies monitor costs by categorizing clients based on their expected resource use [@tran2019; @johnson1998]. Casemix algorithms assume that though the needs of an individual will be unique, shared characteristics determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically, these systems are informed by information contained in patient (case) records. At the agency level, case records contain various information, including provider-level information like the number of direct and indirect hours associated with individual clients and client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services).

Typically, case-mix systems take one of two approaches to classification [@cmho2019]. Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population [@tran2019; @johnson1998]. For example, a client accessing long-term counselling and therapy services might be assigned a greater weight in terms of expected resource use than a client accessing a one-session brief service. Index systems, on the other hand, combine different case characteristics to provide a value that maps to expected resource use or acuity of needs (e.g. a case weight or case complexity score that ranges from 0, the least complex, to 1, the most complex) [@tran2019; @developi2019]. Indexing systems are often used to triage cases by assigning a score to new clients based on answers to an intake assessment. Often, there is a threshold score above which clients are considered acute and may receive services more quickly; at the same time, scores below a specific threshold may not qualify for publicly funded services at all. For instance, a youth reporting thoughts of suicide or other self-harming behaviour will likely index higher than a youth reporting problems remaining focused in school [@cmho2019].

Case-mix algorithms are typically conceptual, rules-based frameworks that rely on predefined factors known or hypothesized to affect client care needs [@tran2019]. These frameworks are informed by clinical expertise, existing research, or policy guidelines and often use well-defined variables such as demographic characteristics, diagnoses, or treatment types. In contrast, data-driven frameworks employ empirical analysis, leveraging statistical or machine learning techniques to identify patterns and groupings in client populations without relying on prior assumptions [@tran2019; @martin2020; @garriga2022]. A data-driven approach holds the potential to uncover novel insights that conceptual frameworks may overlook.

While a hybrid approach—combining conceptual expertise for clinical validity with data-driven methods for automation and insight discovery—is ideal, the complexity of modelling EHR data has limited the development of reliable data-driven frameworks, particularly in mental health service delivery [@tran2019]. Existing research has primarily focused on acute, inpatient hospital settings, which differ substantially from community-based outpatient care [@tran2019; @aminizadeh2023; @garriga2022]. In inpatient settings, conditions often have clear diagnostic criteria and predictable recovery trajectories, such as the relatively fixed timeline and treatment protocol for a broken arm. In contrast, recovery from mental health conditions like anxiety or depression is inherently more subjective, making modelling these data significantly more challenging [@garriga2023].

The challenges of modelling electronic mental health data are underscored by the limited body of research addressing this problem despite its urgency [@tran2019]. A 2019 scoping review of case-mix literature in community-based mental health care identified only one study that employed data-driven methods to predict mental health care resource needs in children and youth populations [@tran2019; @martin2020]. That study analyzed 4,573 client records from 11 UK outpatient CYMH agencies, comparing a conceptual 'clinical-judgement' framework to cluster analysis and negative binomial regression to predict the number of appointments a client would attend during treatment [@martin2020]. While the data-driven classification did as well as the conceptual classification, the researchers suggest that data quality issues (systematic errors introduced by data entry or subjective ratings) and omission of important individual-level factors that were not contained in the EHR impacted the accuracy of their models [@martin2020].

In a related cohort, researchers attempted to predict the workload associated with client characteristics at a community-based mental health center for the elderly, aiming to develop a more accurate representation of workload than simple case counts could provide [@baillon2009]. Using an eight-item, self-designed Case Weighting Scale (CWS), they identified factors that staff perceived as contributing to time demands. After an initial assessment, clinicians would complete the CWS for each client, assigning scores based on factors such as family support, communication difficulties or risk of harm to self or others. These scores were input into a multiple regression model, which generated an estimate of the total time the client would need over four weeks. The model accounted for 58% of the variance in time spent on client-related work, which they considered a success. However, the sample size of only 87 cases raises concerns about the model’s generalizability and accuracy [@baillon2009]. Additionally, inter-rater and re-rater reliability results suggested that the assessments, whether derived from client self-reports or clinicians’ professional opinions, did not consistently align with the time required for client care [@baillon2009]. Nevertheless, the study does provide a basis for understanding how client characteristics might be leveraged to predict workload in mental health care settings–particularly with more sophisticated models.

### Machine learning, a novel approach to modeling case-mix

Building on the limitations of traditional approaches like regression-based models in the Case Weighting Scale (CWS) study, machine learning (ML) offers a promising alternative for predicting mental health resource needs. Unlike conventional methods, ML algorithms learn directly from data without prior programming and are equipped to handle the high-dimensional nature of EHRs making them well-suited for mapping complex relationships between client features, such as depression scores or prior no-shows with outcomes like weekly service hours [@an2023a; @chen2023]. Supervised ML models, aim to optimize a function f(x) that predicts an outcome Y (e.g., hours per week) from input features X, minimizing the difference between predictions and actual data. ML’s ability to uncover patterns in messy data presents a clear advantage for addressing the challenges of modelling client characteristics to predict workload [@an2023; @chen2023].

Within the mental health domain, ML has mainly been used to predict specific events like substance relapse [@kinreich2021], self-harm, and suicide risk [@walsh2017; @simon2018]. For example, @kinreich2021 used ML to predict a change in drinking behaviour in a population diagnosed with alcohol use disorder (AUD). Combining features like brain connectivity, genetic risk scores and demographic information like age, they achieved 86% accuracy in identifying patients whose AUD had gone into remission, enabling clinicians to provide targeted interventions such as additional counselling sessions or closer monitoring [@kinreich2021]. Another study leveraged ML to monitor patient records and predict crisis relapse in 28-day windows based on EHR data[@garriga2022]. The top-performing XGBoost model correctly differentiated those at risk from those not at risk for crisis relapse about 80% of the time [@garriga2022]. In a subsequent post-hoc case study, clinicians rated the predictions as useful for managing patient care in 64% of cases; reporting the estimates helped prioritize patients more effectively, potentially preventing crises [@garriga2022]. Though the authors did not model resource use directly as we hope to do, 'crisis risk' served as a proxy for work. By predicting crises, they aimed to anticipate increased resource demand, allowing for better-informed case prioritization and management. Together, these examples demonstrate the utility of ML in identifying high-risk situations, highlighting its potential to enhance resource planning and improve care delivery in mental health settings.

## The current study

Building on insights from @garriga2022, the current research aims to explore the feasibility of estimating the number of weekly provider hours a case may require, assessed at 28-day intervals. The underlying assumption is that historical patterns can reliably predict future mental health resource use, such as provider hours, and that these patterns are identifiable in electronic mental health records (EHR).

To test these assumptions, we will analyze a retrospective, deidentified dataset from a large child and youth mental health (CYMH) agency in Ontario, Canada, encompassing data from clients served between 2019 and early 2024. Although largely exploratory, the study will be guided by several hypotheses. First, as informed by @garriga2022, we hypothesize that workload prediction will be weakest early in the client journey when available EHR data is limited to intake screener results and basic demographic information. However, as more data accumulates over the course of treatment—such as session attendance and crisis events—we anticipate prediction accuracy will significantly improve.

Consistent with @garriga2022's, we expect that for new clients, factors such as a lack of family support and risk of harm to self or others will most strongly predict provider hours needed. For known clients, we hypothesize that time-based factors, such as the frequency of no-shows and the number of crisis events, will be more predictive of workload demands.

Finally, we expect that the winning machine learning algorithm will outperform a baseline model designed to reflect how agencies typically estimate resource needs today. This baseline model will rely on the conceptual approach often used in practice, where resource allocation is based on the type of service a client is accessing (e.g., counselling and therapy services being assigned greater weight than brief interventions) [@cmho2019]. By comparing these approaches, the study aims to evaluate the extent to which data-driven machine-learning models can enhance workload prediction in CYMH settings.

# Methodology

## Overview

This study aims to estimate the weekly provider hours needed (direct and indirect service) at regular stages in the client journey using machine learning predictive models. The analysis will utilize a retrospective dataset from Compass Child and Youth Family Services, the largest CYMH agency in northern Ontario. Compass serves a culturally and socially diverse population of children, youth, and families, making it a representative setting for this study.

## Data Set

The dataset will include de-identified client records with completed initial intake assessments for clients active between January 1, 2019, and December 31, 2024. Only cases with a completed initial screener will be included to ensure the availability of baseline data for generating meaningful predictions. Cases younger than five and older than 17 will be excluded, as Compass' core services are only offered to children and youth under 18. There are no plans to exclude cases based on any other feature, including diagnoses; however, if, for whatever reason, this changes, it will be outlined in the documentation. The de-identified data will include approximately 6000 EHRs containing hundreds of data points such as demographic information, referrals, diagnoses, risk and well-being assessments and crisis events for all outpatients. The final report will include all variables left over after the initial variable reduction. For an overview of the data flow from raw electronic health records (EHRs) to the derived weekly features used in the predictive model structure, see @fig-datastructure.

![Data Flow Pipeline](images/datastructure.svg){#fig-datastructure apa-note="Data flow from raw client records to the derived features used in the predictive model. The top section represents the raw data structure containing rows of client-specific information, including dates, programs, contact types, and contact durations. The middle section visualizes a sample client timeline, mapping key events such as assessment, no-shows, face-to-face contacts, and discharges, which are stored in the EHR. The bottom section shows the weekly aggregate feature set created from these events, with features such as days since last contact and direct hours that were logged for that case in the week prior. The weekly aggregates will be used for model selection and training to predict weekly workload (e.g., weekly caseweight)" fig-align="left" width="90%"}

## Data Security

Given the sensitivity of mental health data, strict privacy and security measures will be enforced throughout the research process. Necessary ethical approvals will be obtained from relevant ethics boards, including both Compass Child and Youth Family Services and Laurentian University's institutional review board. An exemption for the use of de-identified data will also be required from both institutions.

De-identified clinical data will be extracted from Compass's electronic health information system, which the agency maintains. The data will be de-identified at extraction using the Health Insurance Portability and Accountability Act (HIPAA) Safe Harbor Method [@rightsocr2012]. This process removes all directly identifying information, such as names, addresses, birth dates, and postal codes. Unique client identification codes will also be encrypted using a hashing system to prevent re-identification.

To further enhance security, the dataset will remain under the custody of Compass at all times. Data analysis will be conducted solely by the principal researcher on a password-protected machine belonging to Compass. Model results, summary statistics, and visualizations will only include aggregate metrics, focusing on predictor and model performance. No individual scores or identifiers linked to clients or small subgroups will be reported. Approval from Compass will be obtained before any findings are disseminated in external reports or presentations.

## Data Preprocessing

After de-identification, data preprocessing will include cleaning, joining data frames and handling missing values. Decisions regarding missing data will be made on a case-by-case basis, with details on imputation or exclusion documented in the final report. Any data normalization procedures will also be reported. To ensure reproducibility, the Python data scripts used for preprocessing will be publicly available.

All data points will include an associated date and time, reflecting the moment a specific event or assessment occurred. These timestamps will guide the aggregation of each client's case records into weekly evenly spaced time series for each client, spanning their first interaction with Compass to the last (see @fig-datastructure). Features and labels for each week will be computed at the start of the week from data that was aggregated the week before, ensuring temporal consistency and avoiding data leakage. Additionally, static data prone to change over time (e.g., postal code or school board information) will be excluded to mitigate the risk of retrospective leakage. Retrospective data leakage occurs when information from the future (relative to the prediction point in time) inadvertently influences the model during training or evaluation. This typically happens in retrospective studies where datasets contain time-stamped records, and the temporal order of events is not carefully maintained during data preprocessing or feature engineering.

## Data Splitting and Cross-Validation

To maintain temporal consistency and maximize the generalizability of the models, the plan is to conduct a time-based 80/10/10 split for training, validation, and testing with careful thought to seasonal aspects of our data. Typically, fewer clients access services in the summer months than in the months in which they attend school. For this reason, utilizing only a half year of data for testing would risk influencing predictions. Data splitting will be based on chronological order, as follows:

Training Data: January 2019 to March 2023 - 79.69% Validation Data: April 2023 to September 2023 - 9.38% Test Data: October 2023 to April 2024. - 10.94%

Data from the first six months of the COVID-19 pandemic may need to be excluded, depending on its irregularity in terms of any unusual impact on service delivery. This will be addressed during data cleaning, with details reported in the final documentation.

Time-based cross-validation will be implemented to tune model parameters and ensure robust evaluation. Cross-validation is a method used to assess how well a model is likely to perform on unseen data. Cross-validation divides the training data into sequential, time-based subsets, or "folds," preserving the data's chronological order. For each fold, the model parameters will be tuned on earlier time periods and tested on later ones, simulating real-world prediction scenarios where past data is used to forecast future outcomes. "Tuning model parameters" involves adjusting **hyperparameters**, which are internal settings that control how the model learns from the data. Examples include the depth of a decision tree, the number of trees in a random forest, or the learning rate in a neural network. The goal is to find the combination of hyperparameters that minimizes the error between the model's predictions and the actual values. This ensures that the model’s generalizability to new, unseen data has been thoroughly tested, while still accounting for the temporal nature of the dataset.

The test set will act as an unseen control to evaluate the final models' performance at the very end after training and tuning. It will remain untouched during model development to provide an estimation of how the models will perform in real-world scenarios. Keeping the test set separate and untouched during training ensures that our final evaluation provides a better estimation of how the models will perform in practice. This final step is crucial for assessing the models' generalizability and for identifying any over-fitting that may have occurred during training.

## Feature Generation (Independent Variables) {#features}

Features will be extracted from a possible set of approximately 400 variables. A complete list of proposed feature groupings and variables is provided in @tbl-predictors. Following the methodology outlined in @garriga2022, feature extraction will be categorized into six main types:

**Static or Semi-Static Features.** Demographic data will be represented as fixed values for each case. Age will be treated as a particular case, recalculated annually to reflect changes over time.

**Diagnostic Features.** Each client will be assigned their most recent valid diagnosis if any (e.g., developmental disability, psychological disorder, or "undiagnosed"). Diagnoses will be grouped by category, using the latest valid entry up to the end of the training period to prevent data leakage. The final report will document any classification codes generated for these features.

**EHR Weekly Aggregations.** Weekly records of client-agency interactions will be aggregated for each client. These aggregated features will include counts of interaction types (e.g., appointments, no-shows) and one-hot encoded variables indicating whether a specific event occurred within the week. For one-hot encoding, a value of 1 indicates the event occurred, while 0 indicates it did not.

**Time-Elapsed Features.** For each event type and week, a feature will record the number of days since the last occurrence of the event. If the event has never occurred up to that point, the feature will be set to NA.

**Last Crisis Episode Descriptors.** Details from the most recent crisis episode (e.g., type, severity, resolution) will be used to create features for subsequent weeks until the next crisis occurs. If no crisis has occurred, the feature will be set to NA.

**Last Assessment Descriptors.** Features will be created for each assessment item based on the most recent assessment data, with values decaying over time to reflect diminishing relevance. This decay will apply until the next assessment occurs. All clients will have at least one assessment to ensure inclusion in the study.

**Status Features.** For records with a start and end date (e.g., program intake and discharge), features will assign values (or categories) corresponding to the active weeks. The feature will be set to NA for weeks where the record is not applicable.

**Seasonality Effects.** In addition to record-based features, we will add a week number (1-52) to account for seasonality effects each year.

A final and complete list of all variables will be included in the final report.

```{r}
#| echo: false
#| warning: false
#| label: tbl-predictors
#| tbl-cap: Planned Features (Predictors)
#| tbl-cap-location: top
library(tidyverse)
library(gt)
library(gtExtras)
library(readxl)

table <- read_excel("Predictors.xlsx")
table |> gt() |> gt_theme_pff() |> fmt_missing(columns = everything(), missing_text = " ")

```

## Target Generation (Dependent Variable)

The prediction task will involve two modelling approaches: a continuous regression problem to estimate weekly provider hours and a classification problem to categorize workload intensity into low, medium, and high levels. Examining both approaches allows for flexibility in how predictions are used in practice [@wang2021]. The continuous regression model provides precise estimates of weekly hours, which are valuable for detailed planning and resource allocation. In contrast, the classification model simplifies workload prediction into actionable categories, which may be more practical for agencies to integrate into decision-making workflows, especially in contexts where exact estimates are less critical or more challenging to act on [@wang2021].

Predictions will be generated weekly, with the model estimating the average weekly provider hours required for the upcoming 28 days using information from weeks prior. A rolling window approach will be applied to support periodic updates, incorporating newly available data (or the absence of data) at the beginning of each week. This approach, commonly used in real-time predictive systems, allows for continuous refinement of predictions as additional information becomes available [@garriga2022].

The target variable for the regression task will be constructed by aggregating client-related direct and indirect hours logged by clinicians every Friday. These hours will be summed at the weekly level, corresponding to the feature engineering timeline, and aligned with the time recorded prior to each prediction week to prevent data leakage. We will also examine the stability and reliability of the target measure in two forms: the combined total of direct and indirect hours and the number of direct hours on its own, which may be a more stable measure of client-related work than non-direct hours which clinicians may not log consistently.

## Model Selection

A range of supervised machine learning algorithms were selected to address both regression (continuous provider hours) and classification (categories of provider hours) tasks. Models were selected based on their suitability for handling high-dimensional, tabular datasets like electronic health records (EHRs).

Random Forest (RF) is an ensemble learning method that constructs multiple decision trees during training and outputs either the most common classifications or the average predictions from individual trees. RF was chosen for its ability to handle large datasets with numerous features, effectively manage missing data, and capture complex, non-linear relationships. Its built-in feature importance metrics also enhance interpretability, making it a strong candidate for understanding which variables drive predictions.

XGBoost, a highly efficient implementation of gradient boosting machines (GBMs), was selected due to its superior predictive accuracy, scalability, and ability to handle sparse datasets with missing values. Gradient boosting combines weak learners (typically decision trees) iteratively, optimizing for residual errors at each step to minimize a specified loss function. XGBoost’s regularization techniques, such as shrinkage and column sampling, help prevent overfitting, while its computational efficiency makes it well-suited for large datasets.

Feed-forward neural networks (FNNs), a class of deep learning models, were included for their flexibility in modelling complex non-linear interactions among variables. FNNs consist of interconnected layers of nodes, each applying an activation function to transform input data. These networks are particularly useful when relationships between variables are intricate and not easily captured by tree-based methods.

Recurrent neural networks (RNNs) were added to leverage the sequential nature of the dataset. Unlike FNNs, RNNs include recurrent connections that allow the model to retain information about previous inputs, enabling it to capture temporal dependencies in time-series data. This makes RNNs particularly well-suited for tasks where past events influence future outcomes, such as predicting changes in weekly provider workload based on prior patterns.

Furthermore, a baseline model will be implemented to replicate how new clients are typically assigned in practices without sophisticated case-mix algorithms for comparison. The baseline will rely on a simplified feature set, containing the programming they are accessing and their age. By evaluating all of the models against this baseline, we can better estimate whether machine learning approaches offer any improvement over traditional methods of estimating provider workload.

Each model will be trained on the same training set and evaluated using identical cross-validation splits to ensure consistency in comparisons. Hyperparameter optimization will be conducted for all algorithms, with 100 trials per model, focusing on minimizing Mean Absolute Error (MAE) for regression tasks and maximizing the Area Under the Receiver Operating Characteristic Curve (AUROC) for classification tasks. This process will ensure that the models are fine-tuned to achieve optimal performance.

All models will be compared against the baseline and one another to assess relative performance across both regression and classification tasks. The supplementary materials will document detailed hyperparameter search spaces and tuning procedures. [@salditt2023; @sheetal2023].

## Validation and Testing

Final models will be statistically compared and evaluated on the test set using appropriate performance metrics depending on whether it is a regression task (mean absolute error or root mean squared error) or classification task (accuracy, precision, recall and area under the curve). The evaluations will help determine each model's accuracy, generalizability and robustness [@salditt2023; @wang2021]. Final models will also be analyzed to identify which predictors were the most important in terms of estimating client-related work.

Furthermore, to enhance the interpretability of our model, we plan to implement SHapley Additive exPlanations (SHAP) for feature analysis [@lundberg]. SHAP is a method that helps quantify the contribution of each feature to the model's predictions, providing insights into how specific client characteristics and historical data points influence predicted weekly clinician hours. Interpretability is essential in a mental health care setting, as decisions directly impact client care and resource allocation [@feretzakis2024]. Clinicians and administrators need to understand not only the predicted workload but also the driving factors behind each prediction to ensure fair, personalized, and transparent decision-making. For instance, if certain factors like recent diagnoses or patterns of no-shows are highly influential, this can guide intervention strategies and inform staffing decisions tailored to client needs. SHAP's ability to provide such detailed, interpretable explanations makes it a critical tool for ensuring that the model’s predictions are aligned with clinical understanding and ethical care practices [@feretzakis2024].

## Software and Tools

Python will be used as the primary programming language for model development and evaluation with support from R Statistical Software [@vanrossum1995]. Quarto Markdown will facilitate documentation and ensure reproducibility, with all workflows executed within the Positron IDE environment [@positron2024]. Positron is a next-generation data science integrated development environment (IDE) developed by Posit PBC. It is built on Code OSS and designed to support multiple programming languages, including R and Python, providing an extensible and familiar environment for reproducible authoring and publishing [@positron2024].

# Limitations and Challenges

While we aim to test the feasibility of leveraging electronic health records (EHRs) and machine learning to develop a dynamic case-mix system capable of accurately predicting client-related work based on client need in an interpretable and transparent way. At the same time, several limitations must be acknowledged. First, the data is derived from a specific subset of the population—young people with mental health concerns in a community outpatient setting in northern Ontario—which may limit the generalizability of our findings to other demographics, communities or healthcare environments. Additionally, while machine learning techniques are employed to address the complexity of EHR data, these methods are not immune to biases inherent in the data itself. Systematic issues in data collection, such as underreporting, data entry errors, or misclassification, could potentially impact the accuracy and reliability of the model’s predictions. Additionally, assessment scores may be influenced by the subjective interpretation of the provider who administered the assessment. While we will attempt to reduce these issues, there is no guarantee that all biases can be fully mitigated.

Another limitation is the exclusion of provider-side variables from our models. This decision is intended to maximize fairness in case allocation, given that such data is typically unavailable in the EHR for new clients [@tran2019]. However, omitting provider IDs and characteristics such as clinical experience or preferred modality may overlook factors that significantly influence the hours a provider spends with each client [@tran2019]. This exclusion could reduce the comprehensiveness and accuracy of workload predictions. However, future plans could explore including provider-side variables to control for the impact that providers have on the time spent servicing each case.

Moreover, this study does not address the distinction between actual workload—quantifiable hours spent on direct and indirect services—and perceived workload, which reflects a provider's subjective assessment of their caseload demands [@king2004; @king2009]. For example, two providers with similar actual workloads may perceive their workload differently due to factors such as stress, time management skills, or case complexity. Incorporating a feedback loop to capture staff perceptions of work, potentially through a weekly or monthly "caseload satisfaction" measure, could help bridge this gap by allowing the model to account for discrepancies between objective measures and subjective experiences, providing a better understanding of how provider characteristics and perceptions influence workload dynamics, provider burnout and resource utilization [@king2009].

Finally, while predictive accuracy and interpretability are crucial, a prospective cohort study will be necessary to evaluate how effectively the final model supports clinical decision-making in practice [@garriga2023]. Such a study would allow us to track how predictions influence clinician workload distribution, clinicians perceived workload and client outcomes over time, providing a clearer understanding of its practical benefits and potential drawbacks in a live clinical setting. @garriga2022 demonstrated this approach, showing that prospective cohort studies can offer insights into a model’s impact on workflow, clinician satisfaction, and client care quality. In future research, implementing a cohort study could help validate the final model’s usefulness and refine it for improved applicability in mental health care settings.

Finally, while predictive accuracy and interpretability are critical to this study, a prospective cohort study would be an essential next step to evaluate the model’s real-world effectiveness in supporting clinical decision-making. Such a study would enable us to observe how workload predictions influence clinician workload distribution and client outcomes over time, providing a more comprehensive understanding of the model’s practical benefits and limitations in a live clinical setting. For example, @garriga2022 demonstrated the value of this approach by using a prospective cohort study to examine the impact of predictive models on workflow, clinician satisfaction, and care quality. Future research could adopt a similar methodology to validate the model’s utility and refine it for broader and more effective implementation in mental health care settings.

# Conclusion

The proposed study is a significant step toward developing a data-driven approach to workload management in community-based child and youth mental health services. By leveraging EHRs with machine learning, this research tests the feasibility of using predictive models to estimate clinician workload with historical and real-time client data. Through rigorous methodology—including transparent feature engineering, time-based cross-validation, and a comparative analysis of multiple machine learning algorithms—this study aims to generate individualized estimates of client-related work while ensuring interpretability and ethical relevance.

Central to this effort is a commitment to transparency, achieved through techniques such as SHAP (SHapley Additive exPlanations) to interpret model predictions. These techniques maximize the likelihood that predictions are not only clinically relevant but also trusted by clinical decision-makers [@lundberg]. By bridging the gap between advanced machine learning methods and practical applications in mental health care, this study directly supports the broader goal of improving outcomes for clients and the clinicians who care for them [@feretzakis2024].

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

# Appendix

RESULTS TABLES WILL GO HERE.