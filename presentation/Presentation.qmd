---
title: 'Beyond Counting Clients'
subtitle: 'Modeling a Measure of Clinician Workload with Machine Learning'
format: 
  revealjs:
    theme: simple
    incremental: true
---

```{r setup, include=FALSE}
#| echo: false

library(tidyverse)
library(DiagrammeR)
```

## Statement of Problem

::: columns
::: {.column width="70%"}
::: incremental
-   Providers reporting maxed caseloads

-   case complexity increasing over time

-   long waitlists for service

-   at the same time, caseloads at historic lows
:::
:::

::: {.column width="30%"}
Right column
:::
:::

::: notes
quantify the weight of cases to account for the reported increased complexity and find a way to determine who has the capacity to be assigned more youth.
:::

## Workload vs Caseload vs Caseweight

**Caseload** refers to the number of clients assigned to a worker at any given time.

**Workload** refers to the amount of time a provider spends on a case.

**Caseweight** is a measure of client-related workload; in other words, the time required to support a client thats impacts the absolute number of clients a provider can support within their caseload.
This will serve as our dependent variable.

::: notes
Traditionally, Compass used caseloads to determine whether a provider could support additional youth.
Explain why bad
:::

## Provider Time as a Measure of Workload

-   Direct Time

-   Non-Direct Time

-   Non-Client Related Time

::: notes
The other major method of assessing workload is looking at worker’s time.

At Compass staff have to enter their entire days in the client information system.
This type of approach was suggested by the Knowledge Institute on Child and Youth Mental Health and Addictions when they were trying to examine how to develop caseload/workload guidelines.
At Compass we built dashboard that provides leaders information on staff time on a weekly basis.

So things seem straightforward enough.
We can use client’s clinical assessment data and staff’s time entry data to build an algorithm that predicts the number of hours a client requires per week (we call this the caseweight).
However, as we begun doing some data exploration we encountered a bit of a dilemma.

Because worker’s must to input all their time, there is never really “empty space” that a new client could fill.
So we looked at our broad time entry categories.
There is direct time which we defined as any time spent with the client or their caregivers, non-direct time which is in-service of clients but involves work like documentation or case management and lastly we have non-client related time which is not attributable to specific clients so things like agency meetings and training.

We noticed is that staff seemed to input non-direct time and non-client related time inconsistently.
We realized that acted kind of like a gas.
It would expand or compress based on the worker’s available time.

In discussion we believed direct time would in our setting have the strongest most direct relationship to staff’s overall workload and client need and would be less prone to over-reporting.
:::

## Existing Case Management Algorithms

-   often used to evaluate total cost of an episode of care (e.g. total number of appointments, or total days the file is open)
-   others use proxies for service utilization like the number of services provided or the number of different types of services; which could be related but it distinct from workload
-   other tools can distinguish between levels of care but do not necessarily account for the workload involved in a specific level of care

::: notes
While several case management algorithms exist, they come with a number of limitations - often used to evaluate total cost of an episode of care (e.g. total number of appointments, or total days the file is open) - others use proxies for service utilization like the number of services provided or the number of different types of services; which could be related but it distinct from workload - other tools can distinguish between levels of care but do not necessarily account for the workload involved in a specific level of care
:::

## Research Question

Can a machine learning model trained on client data usefully predict the caseweight of a new intake?

## Informant Study

Wang et al. (2020) found via linear regression proved that the indicators explained a substantial amount of variance of the proxies (four out of five proxies were modeled with R2 \> 0.80).
Classification algorithms also showed success in classifying a patient as having high or low task demand

## Summary

Efforts has been made on improving patient assignment at triage to improve clinician workload management.
o Data contained within the EHR have the potential to support automatic patient-related workload prediction.
o Using indicators that are available in the EHR data, one can potentiallypredict patient-related workload at the early stage of patientvisit and update the prediction as the visit proceeds.
o The predicted workload can potentially be used in assigning new patients to clinicians in a way that better balanced workload, or to identify clinicians that are overloaded

## Informant Recommendations

## Hypotheses

H\^1 - Model Performance Hypothesis: Machine learning models will do a better job of predicting the hours a client might need per week than regression models.

H2 - Feature Importance Hypothesis: Specific intake features, such as initial severity of symptoms and demographic factors, will be significant predictors of caseweight.

## How will we know?

-   We will use mae and rmse to evaluate all models

## Model Interpretability

-   Wang et al., suggested future work should investigate which indicators contributed most to the models, as well as why certain models showed better performance.

-   Importance of indicators can be calculated using black box auditing techniques \[25\], and compared across different algorithms and different proxies.

-   Interpretability of machine learning models have received increasing attention, and is particularly important in healthcare context in improving its transparency \[26,27\].

-   Knowing which indicators were most helpful can also potentially decrease the number of indicators used in future models, which is beneficial to the ease of deployment.

## Dependent Variable - Caseweight

Add characteristics of our outcome here

## Independent Variables

-   Screener scale scores
-   Screener items
-   Demographics Information

## Predictor Selection

-   necessary that all items, scales and information is available at intake

## Machine Learning

![](images/Presenting%20Numerical%20Data%20Education%20Presentation%20in%20Blue%20Cream%20Yellow%20Bold%20Geometric%20Style%20(6).png)

::: notes
So Machine Learning!
What is it?

Broadly, ML algorithms are a set of mathematical rules that look for relationships and patterns in data that can then be used to predict future relationships.
When we use the term model, we are referring to an algorithm that has already been trained on a dataset to make specific predictions.

In our case, as Nick outlined, we were interested in learning whether a machine learning model trained on client data, could usefully predict the caseweight of a new intake.
:::

## Model Building

![](images/Presenting%20Numerical%20Data%20Education%20Presentation%20in%20Blue%20Cream%20Yellow%20Bold%20Geometric%20Style%20(3).png)

## Data Splitting

![](images/Presenting%20Numerical%20Data%20Education%20Presentation%20in%20Blue%20Cream%20Yellow%20Bold%20Geometric%20Style%20(7)-01.png)

## Cross Validation

![](images/Presenting%20Numerical%20Data%20Education%20Presentation%20in%20Blue%20Cream%20Yellow%20Bold%20Geometric%20Style%20(5)-01.png)

## Model Flow

![](images/Presenting%20Numerical%20Data%20Education%20Presentation%20in%20Blue%20Cream%20Yellow%20Bold%20Geometric%20Style.png)

## Plan

```{r}
#| label: fig-modelbuilding
#| fig-width: 7
#| echo: false
#| fig-cap: |
#|   Indicators of Caseweight Modelling Proxy of Caseweight

DiagrammeR::grViz("digraph G {

  rankdir = LR;
  size='7,5'; ratio=fill; node[fontsize=24]; 
 
  subgraph cluster_0 {
    style=filled;
    color=lightgrey;
    node [style=filled,color=white, fontsize=24, shape=circle];
    x1[label = 'x@_{1}'] x2[label = 'x@_{2}'] x3[label = 'x@_{...}'] x4[label = 'x@_{n}'];
    label = 'Indicators of Caseweight'
  }
  
  model[shape=square, style=filled, color='.7 .3 1.0', fontsize=24, fontcolor=White];

  subgraph cluster_1 {
    node [style=filled, fontsize=24,height=.5];
    y1[label = 'y@^{p}', shape=circle];
    label = 'Caseweight Proxy';
    color=invis;

  }
  
    subgraph cluster_2 {
    node [style=filled, fontsize=24,height=.5];
    y2[label = 'y', shape=circle];
    label = 'Actual Caseweight';
    color=invis;
    lheight=6

  }

  x1 -> model [arrowsize=0.5];
  x2 -> model [arrowsize=0.5];
  x3 -> model [arrowsize=0.5];
  x4 -> model [arrowsize=0.5];
  model -> y1 [arrowsize=0.5];
  
  y1 -> y2 [style=dotted, arrowhead=none];
  


}")


```

::: notes
Our vision for the caseweight model is to use the client’s most recent clinical assessment data and information on their past service utilization to predict using our algorithm or model how much direct time (on average) that client is expected to require per week.

We can see for any given worker how much of their time is currently being spent doing direct client care.
If they are below a certain threshold then we can assign clients in a manner that will keep a balanced workload and hopefully ensure they have capacity to support these clients.
There are of course linguistic, cultural, and clinical factors managers will have to match during assignment as well.

But our aim is to give clinical leaders the evidence they need to make informed decisions about case assignments.

This is where machine learning has its benefits.
It can help us distill and make sense of highly complex non-linear systems.
:::

## Algorithm Fairness

Machine learning is being used in the child and youth mental health sector in Ontario but it is in many ways in its infancy.

We are still far away from implementation and as we progress implementation actually gets further away.
One of the major areas of work we need to do is around algorithm fairness.
We need examine the relationship between demographic groups and the model’s variables, outcomes, and the potential decisions.

## Interpretability

Another important piece of work is around model interpretability and awareness.
Some machine learning models can be relatively simple to understand like a decision tree, it is easy to see visually what variables resulted in the predicted intensity.
However, models like a random forest can become more opaque and difficult to understand – we might be able to provide partial explanations but it wouldn’t be reasonable to expect an analyst let alone a clinician to understand the specific reasons why one prediction differs from another.

## Algorithm Governance

And while I’ve said machine learning in the CYMH sector in Ontario is in many ways in its infancy– algorithms are not new.
Many organizations are using algorithm and may not realize it, which is itself a challenge and risk.

Over the past two decades there has been an explosion of work examining algorithm governance, implementation, and auditing.
There are countless resources out there now for organizations to use to audit, assess, and implement algorithms.
However, to my knowledge that work has not made its way into our sector in a major way.

For example, some of the standardize assessment tools currently available have scores that are calculated using algorithms that were developed with machine learning methods such as interactive tree builders.
However, in my experience these algorithms do not have a substantive body of work conceptualizing or assessing fairness.
They also do not appear to undergo auditing.
I’ve not heard of agencies doing algorithm impact assessments either.
The idea is that prior to implementing or using an algorithm that provides some type of score or informs decision making we should be doing our due diligence.

Has the algorithm been assessed for fairness?
How interpretable and understandable is the algorithm?
How was the algorithm developed?
What does it actually predict?
How will we actually use it?
What are the expected impacts of the algorithm?
What safeguards do we have in place to identify harm?

This is can be challenging and time consuming.
I believe we are at a time where we need to as individual organizations and a sector start asking and planning for these questions AND If we already are using algorithms to inform decision making we should be going back and reviewing these questions.
We may in good faith trust tool developers and vendors, however, their context is different.
Developing an algorithm as a research project is different from implementing one that can have material impacts on a family.

## Refs

Caseload guidelines working group (2019).
Developing caseload/workload guidelines for Ontario’s child and youth mental health sector.
Ottawa, ON: Ontario Centre of Excellence for Child and Youth Mental Health.
Collins, G. S., Dhiman, P., Ma, J., Schlussel, M. M., Archer, L., Van Calster, B., Harrell, F. E., Martin, G. P., Moons, K. G. M., van Smeden, M., Sperrin, M., Bullock, G. S., & Riley, R. D.
(2024).
Evaluation of clinical prediction models (part 1): From development to external validation.
BMJ, e074819.
https://doi.org/10.1136/bmj-2023-074819 Johnson, N., Moharana, S., Harrington, C., Andalibi, N., Heidari, H., & Eslami, M.
(2024).
The fall of an algorithm: Characterizing the dynamics toward abandonment.
Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, 337–358.
https://doi.org/10.1145/3630106.3658910 Martin, P., Davies, R., Macdougall, A., Ritchie, B., Vostanis, P., Whale, A., & Wolpert, M.
(2017).
Developing a case mix classification for child and adolescent mental health services: The influence of presenting problems, complexity factors and service providers on number of appointments. 
Journal of Mental Health, 29(4), 431–438.
https://doi.org/10.1080/09638237.2017.1370631 Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020).
Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing.
Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 33–44.
https://doi.org/10.1145/3351095.3372873 Tran, N., Poss, J. W., Perlman, C., & Hirdes, J. P. (2019).
Case-mix classification for mental health care in community settings: A scoping review.
Health Services Insights, 12, 1178632919862248.
https://doi.org/10.1177/1178632919862248
