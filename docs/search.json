[
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Background",
    "text": "Background\nAccording to recent reports by the Auditor General of Ontario on Child and Youth Mental Health, a vital issue limiting agencies’ ability to meet rising demand is the challenge of monitoring client-to-provider workload ratios in a way that accounts for individual client needs (Auditor General of Ontario, 2016, 2018). Ideally, as case complexity increases, the overall number of cases in a provider’s portfolio (case count) should decrease; however, the administrative resources required to manually evaluate each case across dozens of caseloads are beyond what most public agencies can support (CMHO, 2019). As a result, cases are often assigned under the assumption that each requires a similar level of effort (CMHO, 2019). As a consequence, some clinicians consistently manage a higher proportion of complex cases than others (CMHO, 2022; King, 2009). For example, an agency might set a target of 20 cases per provider for counselling services, meaning that providers with fewer than 20 cases have “room” for more, regardless of how many complex cases they have in their overall portfolio.\nThis “casecount” approach to determining caseloads can result in significant disparities in work, particularly for more experienced clinicians who may be assigned more complex cases due to their expertise (CMHO, 2019; King et al., 2000). Complex cases may include those with severe behavioural challenges, high-risk family situations, or co-occurring mental health and developmental disorders, often requiring additional phone calls to coordinate with schools or other community supports, more frequent consultations with other professionals, longer or more detailed treatment plans, and extended documentation time (CMHO, 2019; King, 2009). Without a systematic way to monitor workload beyond case counts, administrators may unknowingly overburden some staff, assuming they have the capacity for more cases when they may already be overburdened (King, 2009). A reliance on providers to self-report when they feel overwhelmed creates an uneven system where some clinicians silently manage unsustainable workloads, which can lead to burnout and diminished care quality (CMHO, 2019; King, 2009).\nGiven this state of affairs, if there was a data-driven tool that could quantify workload based on client complexity rather than counts, it might support clinical decision-makers in a fairer distribution of work (King, 2009; Tran et al., 2019). However, the development of sophisticated data-driven predictive tools to aid in clinical decision-making has been hampered by a lack of resources across the public health sector generally (Auditor General of Ontario, 2018; CMHO, 2022), limits imposed by paper-based client record systems (CMHO, 2019), and iv) lack of computing power and expertise in modelling complex electronic health record data in ways that are transparent and interpretable (Garriga et al., 2022; Xiao et al., 2018). However, the recent transition of CYMH services in Ontario from paper-based health records to electronic records, combined with increased computational power and advances in computer science, has opened the possibility of leveraging EHRs with machine algorithms to improve client outcomes.\nWith this gap in mind, the current research proposes to explore the feasibility of estimating the time that a given client might need from a provider at intervals across the treatment timeline using information contained in the EHR with the eventual goal of testing whether such predictions provide actual added value to clinical practice. The research assumes that historical patterns predict future mental health resource use and that such patterns can be identified in electronic mental health records (EHR) despite their inherent sparseness and systematic bias (Garriga et al., 2022).\n\nCase-mix History\nAcross healthcare domains, particularly emergency medicine, various strategies have been employed to manage provider workload by mapping service levels to client characteristics like symptom severity or prior diagnoses (Johnson et al., 1998; Tran et al., 2019). Case-mix classification systems have been used in the healthcare sector to help payers and agencies monitor costs by categorizing clients based on their expected resource use (Johnson et al., 1998; Tran et al., 2019). Case-mix algorithms assume that though the needs of an individual will be unique, shared characteristics determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically, these systems are informed by information contained in patient (case) records. At the agency level, case records contain various information, including provider-level information like the number of direct and indirect hours associated with individual clients and client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services) (CMHO, 2019).\nTypically, these systems take one of two approaches to classification (CMHO, 2019). Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population (Johnson et al., 1998; Tran et al., 2019). For example, a client accessing long-term counselling and therapy services might be assigned a greater weight in terms of expected resource use than a client accessing a one-session brief service. Index systems, on the other hand, combine different case characteristics to provide a value that maps to expected resource use or acuity of needs (e.g. a case weight or case complexity score that ranges from 0, the least complex, to 1, the most complex) (CMHO, 2019; Tran et al., 2019). Indexing systems are often used to triage cases by assigning a score to new clients based on answers to an intake assessment. Often, there is a threshold score above which clients are considered acute and may receive services more quickly; at the same time, scores below a specific threshold may not qualify for publicly funded services at all. For instance, a youth reporting thoughts of suicide or other self-harming behaviour will likely index higher than a youth reporting problems remaining focused in school (CMHO, 2019).\nCase-mix algorithms are typically conceptual, rules-based frameworks that rely on predefined factors known or hypothesized to influence client care needs (Tran et al., 2019). These frameworks are guided by clinical expertise, existing research, or policy guidelines and often utilize well-defined variables, such as demographic characteristics, diagnoses, or treatment types, to estimate resource use. In contrast, data-driven frameworks employ empirical analysis, leveraging statistical or machine learning techniques to identify patterns and groupings in client populations without relying on prior assumptions (Garriga et al., 2022; Martin et al., 2020; Tran et al., 2019).\nData-driven approaches offer the potential to uncover novel insights that conceptual frameworks may miss. For example, a machine learning model could reveal previously unrecognized patterns within client populations, enabling more precise and effective resource allocation (Garriga et al., 2022; Sheetal et al., 2023). However, these approaches also introduce challenges, including a reliance on high-quality data and the risk of embedding biases present in the data into the analysis (Chen et al., 2023).\nFor this reason, a hybrid approach—combining conceptual expertise for clinical validity with data-driven methods for automation and insight discovery—is considered ideal (Garriga et al., 2022). This approach leverages the strengths of both frameworks, providing clinically valid insights while enabling automated and novel pattern recognition. However, the complexity of modeling EHR data, particularly in mental health service delivery, has hindered the development of reliable data-driven frameworks (Tran et al., 2019).\nExisting research has largely focused on acute, inpatient hospital settings, where conditions often have clear diagnostic criteria and predictable recovery trajectories, such as the fixed timeline and treatment protocol for a broken arm (Aminizadeh et al., 2023; Garriga et al., 2022; Tran et al., 2019). In contrast, community-based outpatient mental health care presents unique challenges. Recovery from conditions like anxiety or depression is inherently more subjective and individualized, with fewer standardized recovery paths, making the modeling of these data significantly more complex (Tran et al., 2019).\n\n\nResearch challenges\nThe challenges inherent in modelling electronic mental health data are underscored by the limited body of research addressing this problem despite its urgency (Tran et al., 2019). A 2019 scoping review of case-mix literature in community-based mental health care identified only one study that employed data-driven methods to predict mental health care resource needs in children and youth populations (Martin et al., 2020; Tran et al., 2019). That study analyzed 4,573 client records from 11 UK outpatient CYMH agencies, comparing a conceptual ‘clinical-judgement’ framework to cluster analysis and negative binomial regression to predict the number of appointments a client would attend during treatment (Martin et al., 2020). While the data-driven classification did as well as the conceptual classification, the researchers suggest that data quality issues (systematic errors introduced by data entry or subjective ratings) and omission of important individual-level factors that were not contained in the EHR impacted the accuracy of their models (Martin et al., 2020). This finding underscores the need for improved data quality and the inclusion of all relevant individual-level factors available to enhance the accuracy of workload prediction in mental health care settings.\nIn a related cohort, researchers attempted to predict the workload associated with client characteristics at a community-based mental health center for the elderly, aiming to develop a more accurate representation of workload than simple case counts could provide (Baillon et al., 2009). Using an eight-item, self-designed Case Weighting Scale (CWS), they identified factors that staff perceived as contributing to time demands. After an initial assessment, clinicians would complete the CWS for each client, assigning scores based on factors such as family support, communication difficulties or risk of harm to self or others. These scores were input into a multiple regression model, which generated an estimate of the total time the client would need over four weeks(Baillon et al., 2009). The model accounted for 58% of the variance in time spent on client-related work, which they considered a success. However, the sample size of only 87 cases raises concerns about the model’s generalizability and accuracy (Baillon et al., 2009). Additionally, inter-rater and re-rater reliability results suggested that the assessments, whether derived from client self-reports or clinicians’ professional opinions, did not consistently align with the time required for client care (Baillon et al., 2009). Nevertheless, the study does provide a basis for understanding how client characteristics might be leveraged to predict workload in mental health care settings–particularly with more sophisticated models.\n\n\nMachine learning, a novel approach to modeling case-mix\nBuilding on the limitations of traditional approaches like regression-based models in the Case Weighting Scale (CWS) study, machine learning (ML) offers a promising alternative for predicting mental health resource needs. Unlike conventional methods, ML algorithms learn directly from data without prior programming and are equipped to handle the high-dimensional nature of EHRs making them well-suited for mapping complex relationships between client features, such as depression scores or prior no-shows with outcomes like weekly service hours (An et al., 2023; Chen et al., 2023; Sheetal et al., 2023). Supervised ML models aim to optimize a function f(x) that predicts an outcome Y (e.g., hours per week) from input features X (client-level factors), minimizing the difference between predictions and actual data. For example, the mean squared error (MSE) cost function is used to evaluate how well a machine learning model’s predictions match the actual values in regression tasks. It calculates the average of the squared differences between the predicted values \\hat{y}​ and the actual values y^i​ then gradient descent will calculate the derivative of J(θ) with respect to the model’s parameters (e.g., weights and biases) updating them iteratively until it finds the minimum possible error. See Equation 1. \nJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\hat{y}_i \\right)^2\n\\tag{1}\nWithin the mental health domain, ML has mainly been used to predict specific events like substance relapse (Kinreich et al., 2021), self-harm, and suicide risk (Simon et al., 2018; Walsh et al., 2017). For example, Kinreich et al. (2021) used ML to predict a change in drinking behaviour in a population diagnosed with alcohol use disorder (AUD). Combining features like brain connectivity, genetic risk scores and demographic information like age, they achieved 86% accuracy in identifying patients whose AUD had gone into remission, enabling clinicians to provide targeted interventions such as additional counselling sessions or closer monitoring (Kinreich et al., 2021). Another study leveraged ML to monitor patient records and predict crisis relapse in 28-day windows based on EHR data (Garriga et al., 2022). The top-performing tree-based XGBoost model correctly differentiated those at risk from those not at risk for crisis relapse about 80% of the time (Garriga et al., 2022). In a subsequent post-hoc case study, clinicians rated the predictions as useful for managing patient care in 64% of cases; reporting the estimates helped prioritize patients effectively, potentially preventing crises (Garriga et al., 2022). Although the authors did not model resource use directly as we hope to do, ‘crisis risk’ served as a proxy for work. By predicting crises, they aimed to anticipate increased resource demand, allowing for better-informed case prioritization and management. Together, these examples demonstrate the potential of ML in identifying high-risk situations, highlighting its potential to enhance resource planning and improve care delivery in mental health settings (Garriga et al., 2022; Wang et al., 2021)."
  },
  {
    "objectID": "index.html#the-current-study",
    "href": "index.html#the-current-study",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "The current study",
    "text": "The current study\nBuilding on insights from Garriga et al. (2022) work, the current research aims to explore the feasibility of applying machine learning to EHRs to estimate the number of weekly provider hours a case may require, assessed at 28-day intervals. The underlying assumption is that historical patterns can reliably predict future mental health resource use, like provider hours, and that these patterns are identifiable in electronic mental health records (EHR) (Tran et al., 2019).\nTo test these assumptions, we will analyze a retrospective, deidentified dataset from a large child and youth mental health (CYMH) agency in Ontario, Canada, encompassing data from clients served between 2019 and early 2024. Although largely exploratory, the study will be guided by several hypotheses. First, as informed by Garriga et al. (2022), we hypothesize that workload prediction will be weakest early in the client journey when available EHR data is limited to intake screener results and basic demographic information. However, as more data accumulates over the course of treatment—such as session attendance and crisis events—we anticipate prediction accuracy will significantly improve.\nConsistent with Garriga et al. (2022)’s work, we expect that for new clients, factors such as a lack of family support and risk of harm to self or others will most strongly predict provider hours needed. For known clients, we hypothesize that time-based factors, such as the frequency of no-shows and the number of crisis events, will be more predictive of workload demands (Wang et al., 2021).\nFinally, we expect that the winning machine learning algorithm will outperform a baseline model designed to reflect how agencies typically estimate resource needs today. This baseline model will rely on the conceptual approach often used in practice, where resource allocation is based on the type of service a client is accessing (e.g., counselling and therapy services being assigned greater weight than brief interventions) (CMHO, 2019). By comparing these approaches, the study aims to evaluate the extent to which data-driven machine-learning models can support workload prediction in CYMH settings."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Overview",
    "text": "Overview\nThis study aims to estimate the weekly provider hours needed (direct and indirect service) at regular stages in the client journey using machine learning predictive models. The analysis will utilize a retrospective dataset from Compass Child and Youth Family Services, the largest CYMH agency in northern Ontario. Compass serves a culturally and socially diverse population of children, youth, and families, making it a representative setting for this study."
  },
  {
    "objectID": "index.html#data-set",
    "href": "index.html#data-set",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Set",
    "text": "Data Set\nThe dataset will include de-identified client records with completed initial intake assessments for clients active between January 1, 2019, and December 31, 2024. Only cases with a completed initial screener will be included to ensure the availability of baseline data for generating meaningful predictions. Cases younger than five and older than 17 will be excluded, as Compass’ core services are only offered to children and youth under 18. There are no plans to exclude cases based on any other feature, including diagnoses; however, if, for whatever reason, this changes, it will be outlined in the documentation. The de-identified data will include approximately 6000 EHRs containing hundreds of data points such as demographic information, referrals, diagnoses, risk and well-being assessments and crisis events for all outpatients. The final report will include all variables left over after the initial variable reduction. For an overview of the data flow from raw electronic health records (EHRs) to the derived weekly features used in the predictive model structure, see Figure 1.\n\n\n\n\nFigure 1\n\n\nData Flow Pipeline\n\n\n\n\n\n\n\n\nNote. Data flow from raw client records to the derived features used in the predictive model. The top section represents the raw data structure containing rows of client-specific information, including dates, programs, contact types, and contact durations. The middle section visualizes a sample client timeline, mapping key events such as assessment, no-shows, face-to-face contacts, and discharges, which are stored in the EHR. The bottom section shows the weekly aggregate feature set created from these events, with features such as days since last contact and direct hours that were logged for that case in the week prior. The weekly aggregates will be used for model selection and training to predict weekly workload (e.g., weekly caseweight)"
  },
  {
    "objectID": "index.html#data-security",
    "href": "index.html#data-security",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Security",
    "text": "Data Security\nGiven the sensitivity of mental health data, strict privacy and security measures will be enforced throughout the research process. Necessary ethical approvals will be obtained from relevant ethics boards, including both Compass Child and Youth Family Services and Laurentian University’s institutional review board. An exemption for the use of de-identified data will also be required from both institutions.\nDe-identified clinical data will be extracted from Compass’s electronic health information system, which the agency maintains. The data will be de-identified at extraction using the Health Insurance Portability and Accountability Act (HIPAA) Safe Harbor Method (OCR, 2012). This process removes all directly identifying information, such as names, addresses, birth dates, and postal codes. Unique client identification codes will also be encrypted using a hashing system to prevent re-identification.\nTo further enhance security, the dataset will remain under the custody of Compass at all times. Data analysis will be conducted solely by the principal researcher on a password-protected machine belonging to Compass. Model results, summary statistics, and visualizations will only include aggregate metrics, focusing on predictor and model performance. No individual scores or identifiers linked to clients or small subgroups will be reported. Approval from Compass will be obtained before any findings are disseminated in external reports or presentations."
  },
  {
    "objectID": "index.html#data-preprocessing",
    "href": "index.html#data-preprocessing",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nAfter de-identification, data preprocessing will include cleaning, joining data frames and handling missing values. Decisions regarding missing data will be made on a case-by-case basis, with details on imputation or exclusion documented in the final report. Any data normalization procedures will also be reported. To ensure reproducibility, the Python data scripts used for preprocessing will be publicly available.\nAll data points will include an associated date and time, reflecting the moment a specific event or assessment occurred. These timestamps will guide the aggregation of each client’s case records into weekly evenly spaced time series for each client, spanning their first interaction with Compass to the last (see Figure 1). Features and labels for each week will be computed at the start of the week from data that was aggregated the week before, ensuring temporal consistency and avoiding data leakage. Additionally, static data prone to change over time (e.g., postal code or school board information) will be excluded to mitigate the risk of retrospective leakage (Garriga et al., 2022). Retrospective data leakage occurs when information from the future (relative to the prediction point in time) inadvertently influences the model during training or evaluation. This typically happens in retrospective studies where datasets contain time-stamped records, and the temporal order of events is not carefully maintained during data preprocessing or feature engineering."
  },
  {
    "objectID": "index.html#data-splitting-and-cross-validation",
    "href": "index.html#data-splitting-and-cross-validation",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Splitting and Cross-Validation",
    "text": "Data Splitting and Cross-Validation\nTo maintain temporal consistency and maximize the generalizability of the models, the plan is to conduct a time-based 80/10/10 split for training, validation, and testing with careful thought to seasonal aspects of our data. Typically, fewer clients access services in the summer months than in the months in which they attend school. For this reason, utilizing only a half year of data for testing would risk biasing predictions. Data splitting will be based on chronological order roughly, as follows:\nTraining Data: January 2019 to March 2023 (79.69%)\nValidation Data: April 2023 to September 2023 (9.38%)\nTest Data: October 2023 to April 2024 (10.94%)\nData from the first six months of the COVID-19 pandemic may need to be excluded, depending on its irregularity in terms of any unusual impact on service delivery. This will be addressed during data cleaning, with details reported in the final documentation.\n\nCross-Validation\nTime-based cross-validation will be implemented to tune model parameters and ensure robust evaluation. Cross-validation is a method used to assess how well a model is likely to perform on unseen data (An et al., 2023; Sheetal et al., 2023). Cross-validation divides the training data into sequential, time-based subsets, or “folds,” preserving the data’s chronological order. For each fold, the model parameters will be tuned on earlier time periods and tested on later ones, simulating real-world prediction scenarios where past data is used to forecast future outcomes. “Tuning model parameters” involves adjusting hyperparameters, which are internal settings that control how the model learns from the data (Sheetal et al., 2023). Examples include the depth of a decision tree, the number of trees in a random forest, or the learning rate in a neural network. The goal is to find the combination of hyperparameters that minimizes the error between the model’s predictions and the actual values (see Equation 1). This maximizes the model’s generalizability to new, unseen data, while still accounting for the temporal nature of the dataset.\nThe validation set will be used to break any ties between models, without having to tap into the final test set which acts as a control to evaluate the final models’ performance after training and tuning. The test set will remain entirely untouched during feature engineering and model development so as to provide as unbiased an estimation of how the models will perform in real-world scenarios as possible. This final step is crucial for assessing the models’ generalizability and for identifying any over-fitting (when a model performs significantly better on the training data than than the test data) that may have occurred during training (An et al., 2023)."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Feature Generation (Independent Variables)",
    "text": "Feature Generation (Independent Variables)\nFeatures–otherwise known as predictors or variables–will be extracted from a total possible set of approximately 400 variables. A complete list of proposed feature groupings and variables is provided in Table 1. A full list of all variables included in the analysis will be reported in the final thesis. Following the methodology outlined in Garriga et al. (2022), feature extraction will be categorized into six main types:\nStatic or Semi-Static Features. Demographic data will be represented as fixed values for each case. Age will be treated as a particular case, recalculated annually to reflect changes over time.\nDiagnostic Features. Each client will be assigned their most recent valid diagnosis if any (e.g., developmental disability, psychological disorder, or “undiagnosed”). Diagnoses will be grouped by category, using the latest valid entry up to the end of the training period to prevent data leakage. The final report will document any classification codes generated for these features.\nEHR Weekly Aggregations. Weekly records of client-agency interactions will be aggregated for each client. These aggregated features will include counts of interaction types (e.g., appointments, no-shows) and one-hot encoded variables indicating whether a specific event occurred within the week. For one-hot encoding, a value of 1 indicates the event occurred, while 0 indicates it did not.\nTime-Elapsed Features. For each event type and week, a feature will record the number of days since the last occurrence of the event. If the event has never occurred up to that point, the feature will be set to NA.\nLast Crisis Episode Descriptors. Details from the most recent crisis episode (e.g., type, severity, resolution) will be used to create features for subsequent weeks until the next crisis occurs. If no crisis has occurred, the feature will be set to NA.\nLast Assessment Descriptors. Features will be created for each assessment item based on the most recent assessment data, with values decaying over time to reflect diminishing relevance. This decay will apply until the next assessment occurs. All clients will have at least one assessment to ensure inclusion in the study.\nStatus Features. For records with a start and end date (e.g., program intake and discharge), features will assign values (or categories) corresponding to the active weeks. The feature will be set to NA for weeks where the record is not applicable.\nSeasonality Effects. In addition to record-based features, we will add a week number (1-52) to account for seasonality effects each year.\nA final and complete list of all variables will be included in the final report.\n\n\n\n\n\nTable 1\n\n\nPlanned Features (Predictors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime based\nCount based\nLatest contact\nStatic/semi-static information\n\n\n\n\nWeeks since last crisis event\nCount of crisis events\nDepression acuity score\nAge, gender, school district\n\n\nWeeks since first contact\nCount of no-shows\nIdentified substance use\nADHD/Autism diagnosis\n\n\nWeeks since last no-show\nCounts of substances used\nCurrent services accessed\nMental health diagnosis\n\n\nWeeks since last contact\nCounts of phone calls\nIdentified symptoms\nLearning disability diagnosis\n\n\nWeeks since substance misuse identified\nCount of previous completed services\nPreviously indicated risk\n\n\n\n\nWeeks since self/harm identified\nDischarge and referral counts\nRecent contact with child welfare\n\n\n\n\nWeeks since suicide risk identified\nNumber of current services\nRecent psychological consult\n\n\n\n\nWeeks since last discharge\n\n\nRecent diagnosis\n\n\n\n\nWeeks since last crisis episode\n\n\nCurrent caregiver and family support\n\n\n\n\nWeeks since first visit\n\n\nIdentified thoughts of suicide\n\n\n\n\nWeeks since referral"
  },
  {
    "objectID": "index.html#target-generation-dependent-variable",
    "href": "index.html#target-generation-dependent-variable",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Target Generation (Dependent Variable)",
    "text": "Target Generation (Dependent Variable)\nThe prediction task will involve two modelling approaches: a continuous regression problem to estimate weekly provider hours and a classification problem to categorize workload intensity into low, medium, and high levels. Examining both approaches allows for flexibility in how predictions are used in practice (Wang et al., 2021). The continuous regression model provides precise estimates of weekly hours, which are valuable for detailed planning and resource allocation. In contrast, the classification model simplifies workload prediction into actionable categories, which may be more practical for agencies to integrate into decision-making workflows, especially in contexts where exact estimates are less critical or more challenging to act on (Wang et al., 2021).\nPredictions will be generated weekly, with the model estimating the average weekly provider hours required for the upcoming 28 days using information from weeks prior (see Figure 1). A rolling window approach will be applied to support periodic updates, incorporating newly available data (or the absence of data) at the beginning of each week. This approach, commonly used in real-time predictive systems, allows for continuous refinement of predictions as additional information becomes available (Garriga et al., 2022).\nThe target variable for the regression task will be constructed by aggregating client-related direct and indirect hours logged by clinicians every Friday at Compass. These hours will be summed at the weekly level, corresponding to the feature engineering timeline, and aligned with the time recorded prior to each prediction week. We will also examine the stability and reliability of the target measure in two forms: the combined total of direct and indirect hours and the number of direct hours on its own, which may be a more stable measure of client-related work than non-direct hours which clinicians may not log consistently."
  },
  {
    "objectID": "index.html#model-selection",
    "href": "index.html#model-selection",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Model Selection",
    "text": "Model Selection\nA range of supervised machine learning algorithms were selected to address both regression (continuous provider hours) and classification (categories of provider hours) tasks. Models were selected based on their suitability for handling high-dimensional, tabular datasets like electronic health records (EHRs) and informed by the research shared so far (Garriga et al., 2022; Sheetal et al., 2023; Wang et al., 2021).\nRandom Forest (RF) is an ensemble learning method that constructs multiple decision trees during training and outputs either the most common classifications or the average predictions from individual trees. RF was chosen for its ability to handle large datasets with numerous features, effectively manage missing data, and capture complex, non-linear relationships. Its built-in feature importance metrics also enhance interpretability, making it a strong candidate for understanding which variables drive predictions (An et al., 2023).\nXGBoost, a highly efficient implementation of gradient boosting machines (GBMs), was selected due to its superior predictive accuracy, scalability, and ability to handle sparse datasets with missing values. Gradient boosting combines weak learners (typically decision trees) iteratively, optimizing for residual errors at each step to minimize a specified loss function. XGBoost’s regularization techniques, such as shrinkage and column sampling, help prevent overfitting, while its computational efficiency makes it well-suited for large datasets (An et al., 2023).\nFeed-forward neural networks (FNNs), a class of deep learning models, were included for their flexibility in modelling complex non-linear interactions among variables. FNNs consist of interconnected layers of nodes, each applying an activation function to transform input data. These networks are particularly useful when relationships between variables are intricate and not easily captured by tree-based methods (Su et al., 2020).\nRecurrent neural networks (RNNs) were added to leverage the sequential nature of the dataset. Unlike FNNs, RNNs include recurrent connections that allow the model to retain information about previous inputs, enabling it to capture temporal dependencies in time-series data. This makes RNNs particularly well-suited for tasks where past events influence future outcomes, such as predicting changes in weekly provider workload based on prior patterns (Dabas, 2024; Su et al., 2020).\nFurthermore, informed by Garriga et al. (2022), a baseline model will be implemented to replicate how new clients are typically assigned in agencies without sophisticated case-mix algorithms. The baseline will rely on a simplified feature set containing the programming the client is accessing (i.e., brief service versus counselling and therapy or crisis intervention) and their age. By evaluating all of the models against this baseline, we can better estimate whether machine learning approaches offer any improvement over traditional methods of estimating provider workload and assigning new clients.\nEach model will be trained on the same training set and evaluated using identical cross-validation splits to ensure consistency in comparisons. Hyperparameter optimization will be conducted for all algorithms, with 100 trials per model, focusing on minimizing Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) for regression tasks and maximizing the Area Under the Curve (AUC) for classification tasks. Specific metrics may vary depending on the model and outcome being evaluated. However, all of these choices and the resulting metrics will be shared in the final thesis."
  },
  {
    "objectID": "index.html#validation-and-testing",
    "href": "index.html#validation-and-testing",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Validation and Testing",
    "text": "Validation and Testing\nAll models will be compared against the baseline and one another to assess relative performance across both regression and classification tasks. The supplementary materials will document detailed hyperparameter search spaces and tuning procedures. (Salditt et al., 2023; Sheetal et al., 2023).\nFinal models will be statistically compared and evaluated on the test set using appropriate performance metrics depending on whether it is a regression task (mean absolute error or root mean squared error) or classification task (accuracy, precision, recall and area under the curve). The evaluations will help determine each model’s accuracy, generalizability and robustness (Salditt et al., 2023; Wang et al., 2021). Final models will also be analyzed to identify which predictors were the most important in terms of estimating client-related work.\nFurthermore, to enhance the interpretability and transparency of our models, we plan to implement SHapley Additive exPlanations (SHAP) for feature analysis (Lundberg & Lee, 2017). SHAP is a method that helps quantify the contribution of each feature to the model’s predictions, providing insights into how specific client characteristics and historical data points influence predicted weekly clinician hours. Interpretability is essential in a mental health care setting, as decisions directly impact client care and resource allocation (Feretzakis et al., 2024). Clinicians and administrators need to understand not only the predicted workload but also the driving factors behind each prediction to ensure fair, personalized, and transparent decision-making. For instance, if certain factors like recent diagnoses or patterns of no-shows are highly influential, this can guide intervention strategies and inform staffing decisions tailored to client needs. SHAP’s ability to provide such detailed, interpretable explanations makes it a critical tool for ensuring that the model’s predictions are aligned with clinical understanding and ethical care practices (Feretzakis et al., 2024)."
  },
  {
    "objectID": "index.html#software-and-tools",
    "href": "index.html#software-and-tools",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Software and Tools",
    "text": "Software and Tools\nPython will be used as the primary programming language for model development and evaluation with support from R Statistical Software (Van Rossum & Drake, 1995). Quarto Markdown will facilitate documentation and ensure reproducibility, with all workflows executed within the Positron IDE environment (Positron, 2024). Positron is a next-generation data science integrated development environment (IDE) developed by Posit PBC. It is built on Code OSS and designed to support multiple programming languages, including R and Python, providing an extensible and familiar environment for reproducible authoring and publishing (Positron, 2024)."
  }
]