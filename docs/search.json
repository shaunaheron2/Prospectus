[
  {
    "objectID": "Caseweight Prospectus.html#caseload-versus-workload",
    "href": "Caseweight Prospectus.html#caseload-versus-workload",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Caseload versus workload",
    "text": "Caseload versus workload\nIn Ontario, between 16% and 24% of CYMH agencies reported average caseloads per worker that were at least 50% larger than provincial averages for the same services (CMHO, 2019), which is a problem since high caseloads (more than 20 to 30 clients) are associated with a range of negative outcomes for providers, their clients and the agencies they work for. High caseloads have been linked with self-reported burnout (Morse et al., 2012), poor work engagement, low job satisfaction (Green et al., 2014) and poorer treatment outcomes (Garman et al., 2002). Provider burnout, is characterized by emotional exhaustion, depersonalization and a decreased sense of self efficacy which impacts a providers perceived ability to handle job-related stressors (Kim et al., 2018). Provider burnout is particularly pronounced in community mental health settings where caseloads are typically larger and rates of clinical complexity and co-morbidity are higher (Tran et al., 2019).\nAt the same time, newer research suggests the more important factor influencing client-related work may be the mix of cases rather than flat counts (King et al., 2000). For instance, King et al. (2000) found that counts were not as predictive of clinician burnout as the mix of cases. He posited that providers adapt to high caseloads by simply doing less for each case, suggesting a potential ‘dose-response’ relationship between a provider’s time and their effectiveness which might explain the poorer outcomes associated with high caseloads (Kim et al., 2018; King, 2009). To illustrate: two clinicians could have identical caseloads as far as counts but the amount of work necessary to deliver services could vary wildly between them. For example, one clinician may easily manage a higher number of low-complexity cases but become overwhelmed with just a few high-complexity cases.These findings are supported by the work o"
  },
  {
    "objectID": "Caseweight Prospectus.html#a-measure-of-client-related-work",
    "href": "Caseweight Prospectus.html#a-measure-of-client-related-work",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "A measure of client-related work",
    "text": "A measure of client-related work\nGiven this state of affairs, if we had a way to predict the work associated with a given client in a way that doesn’t tax an already overburdened system (i.e., requiring dedicated staff to carefully evaluate each case before clinical assignment) agencies might more efficiently manage provider caseloads. However modeling client characteristics, let alone predicting the work that is driven by them, has proved difficult so far (CMHO, 2019; King, 2009; Tran et al., 2019). Despite initiatives such as the Quadruple Aim Framework meant to improve health outcomes, reduce costs, and improve provider work-life balance, the CYMH sector continues to face challenges in establishing a consistent, standardized measurement system, let alone a case assignment system that can accurately reflect the demands placed on staff (Arnetz et al., 2020; CMHO, 2019). Nevertheless, the new guidelines have informed policies meant to improve the tracking of client-related work which has enabled several agencies to expand efforts to understand and track workload in their own organization."
  },
  {
    "objectID": "Caseweight Prospectus.html#how-do-we-model-client-complexity",
    "href": "Caseweight Prospectus.html#how-do-we-model-client-complexity",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "How do we model client complexity?",
    "text": "How do we model client complexity?\nAcross health domains (e.g., psychiatric, emergency medicine, community based mental health) various strategies have been employed to manage provider workload by providing different levels of service determined by presenting characteristics like symptom severity or prior diagnoses (Johnson et al., 1998; Tran et al., 2019). This system assumes that though the needs of each individual in a population will be unique, there will be shared characteristics that determine the type of treatment they will need (e.g., family counselling versus substance use treatment). These groups represent the mix of cases or “case-mix” which can be viewed as a proxy for the types of care needs of the population. Case-mix classification systems are most often used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use (CMHO, 2019). At a local level, like a single agency, the data within a case mix may contain the activity of individual providers like direct-hours attributable to individual clients and other work, and client-level characteristics like diagnoses, treatment history and current presenting symptoms (e.g., crisis intervention versus brief services). It can also contain demographics information like age, school district and the number of services offered. These data points are often stored in an electronic health care database and are captured using some kind of classification system.\nCase-mix classification systems usually take one of two approaches to modeling client-related work: grouping or index. Grouping systems assign people into classes in terms of their expected resource use, with each group having a specific weight attached to it (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population. Index systems on the other hand, combine different characteristics of a case to provide a continuous, numerical value which maps to expected resource use relative to an average case (Tran et al., 2019).\nWhile case-mix systems are widely used in the medical domain, to date, most mental health case-mix classification systems focus on acute care in hospital or other inpatient settings which are distinctly different than community based care in several ways. Typically, community based care involves a team of providers offering a wider range of services (e.g., urgent care, crisis intervention, brief services orlonger-term treatment like counselling and therapy as well as group programs and day treatment provided in educational settings), often without clear diagnoses or well-defined treatment protocols, making it a more complicated endeavor to model.\nIllustratively, a 2019 scoping review of case-mix classification systems developed specifically for community-based mental health care settings revealed significant gaps in the research–especially in the child and youth domain (Tran et al., 2019). The single case that looked at case-mix classification to predict mental health care resource in a CYMH community setting remains the only case of its kind (Martin et al., 2020; Tran et al., 2019). In that study, researchers modeled 4573 client records from eleven UK outpatient CYMH agencies, comparing cluster analysis, regression trees and a conceptual classification based on clinical best practice guidelines in their ability to accurately predict the number of appointments a client attended in treatment (Martin et al., 2020). While the researchers found the conceptual classification was as clinically meaningful as data-driven classification in accounting for number of appointments, they found little evidence to support the idea that client complexity or context factors (with the exception of school attendance problems) were linked with overall appointment counts (Martin et al., 2020). Moverover, the models did not do well in explaining the considerable variation in resource provision between workers. Researchers cited data quality problems and omission of important individual-level factors that they suggested merited further testing and development.\nIn the adult mental health domain, another group of researchers tried to predict the workload associated with clients at a community-based mental health centre for the elderly (Baillon et al., 2009). Using an 8-item case weighting scale (CWS) that identified factors staff felt contributed to demand for staff time, they built a multiple regression model that assigned different weightings to each item based on the strength of its relationship with the outcome (an estimation of time spent on each client logged over a four-week period). The model was then used to predict the total time a client would utilize in a four-week period following the first appointment. Though the model was reported as a success, accounting for 58% of the variance in time spent on client-related work, the sample consisted of only 87 cases and relied on a statistical method not suitable for evaluating agreement between model predictions and actual observations, leaving it unclear how accurate the model actually was (Baillon et al., 2009; Mansournia et al., 2021). Moreover, inter-rater and re-rater reliability results indicated that the assessment, whether from a client’s self-report or a professional’s clinical opinion, did not necessarily relate to the amount of time needed by clients (Baillon et al., 2009).\n\nCase-mix in other domains\nConsidering the lack of research in the CYMH domain, we looked to several measures of workload intensity have been developed to manage caseloads in other specialties. For example, in general psychiatry, researchers have used factors like sociodemographics, functional ability, and caregiver and social network characteristics to predict service utilization. Although many of these models lacked adequate evaluations and would not be suitable for all client groups, Tran et al. (2019), recommend that it may be useful to experiment with case-mix systems developed in other settings. One model that the authors of the review suggested may be a good candidate for testing in community settings is Canada’s System for Classification of In-Patient Psychiatry (SCIPP) (Perlman et al., 2013; Tran et al., 2019). The SCIPP algorithm is a grouping methodology that sorts patients according to clinical characteristics obtained from standardized interRAI assessment data to estimate resource use (Hirdes et al., 2020; Perlman et al., 2013).\n\n\nMeasuring Client-Related Work\ninterRAI is an international research network that develops clinical standards across a variety of health and social service settings that have developed a large toolkit of instruments used by health organizations worldwide to assess people at the point of care across a variety of domains including emergency medicine, emergency psychiatry and children and youth mental health. The collected data are meant to be used at the agency level for quality improvement activities, benchmarking, program planning and resource planning and at the system level to compare health data across regions and provinces (Hirdes et al., 2002). In Canada, interRAI is partnered with the Canadian Institute for Health Information (CIHI) who act as a custodian of interRAI standards for Canada and houses and monitors the collected interRAI data. Many interRAI instruments are used across Canada and internationally, but the Children and Youth Mental Health assessment (ChYMH) represents the first assessment they designed specifically for children and youth (Stewart & Hamza, 2017).\nIn Ontario, two instruments are most often used in the CYMH sector: the Child and Youth Mental Health Screener+ (ChYMH-S) and the more comprehensive full ChYMH and its variants. The primary use of the CHYM-S is to support decision making related to triaging, placement and service utilization while the full ChYMH and its associated Collaborative Action Plans (CAPs) are meant to assess, repond to and monitor the strengths, preferences and mental health needs of clients in in-patient and out-patient treatment. The ChYMH products are currently being utilized in over 60 mental health agencies across Ontario.\nThe full ChYMH includes over 400 items meant to build a comprehensive picture of a child/youth’s strengths, needs, functioning and areas of risk to inform care planning (Stewart et al., 2022), while the ChYMH-S is comprised of 106 items and is intended as a brief screener to identify young people who are in need of more comprehensive assessment. The screener is administered via a semi-structured interview to children and youth between 4-18 years in a variety of settings and is intended to take 15-20 minutes to complete. The current study will focus on screener data collected at intake.\nThe ChYMH-S includes 34 administrative and tracking items, 26 mental health indicators, 5 substance use indicators, 9 questions related to harm to self and others, 6 behaviour focused items, 1 cognitive item as well as items that look to track social relations, anxiety levels, medications, living arrangements, diagnoses, physical conditions, past interventions and current and past strengths and resilience (Stewart et al., 2022). The Depressive Severity Index, Anxiety Scale, Disruptive/Aggressive Behaviour Scale, Hyperactive/Distraction Scale and Internalizing/Externalizing scales are all included in the ChYMH-S. See table for full list of items. The instrument and the various scales and items it comtains have been tested extensively in research worldwide, demonstrating reliable face, content, construct and predictive validity [lots of detail could go here–please inform as to how much–should this go in the methodology section?] .\nImportantly, several ChYMH scales have demonstrated good predictive validity. For example, data from over 5000 children and youth placed in psychiatric settings in Ontario found that the Agressive Behaviour Scale was predictive of multiple control interventions, while the Severity of Self Harm Scale (SOS) has proven useful in predicting admission for risk of self-harm in youth between 10-17 years. Moreover, individuals who score higher on scales like the Hyperactive/Distraction scale were more likely to have a provisional diagnosis of ADHD (Stewart et al., 2022).\nThough the various scales and items of the ChyMH demonstrate some predictive utility, it remains unclear how well these items might predict the actual work required to serve a given client. Within the CYMH domain we found one example of interRAI data being used to develop an algorithm to predict resource cost for children and youth with developmental disabilities with a cluster analysis (Stewart et al., 2020). Though the resulting Child and Youth Resource Index (ChYRI) could only explain 30% of the variance in per diem costs for community-based services (Stewart et al., 2020), the algorithm was deemed a success and is still in use today. However, a lack of explanation of how the analysis was conducted, as well as public availability of resulting fit statistics, makes it unclear how and where the algorithm could be improved.\nIt is important to note, in the current thesis, we are not interested in predicting client-related work for cost reasons, but instead our efforts are driven by the need to reduce wait times and by requests from clinicians to develop a resource allocation system that accounts not only for current capacity in regards to counts, but differences in need and thereby work between clients so that caseload assignment is more equitable and mindful of the work that is already on each clinicians plate.\n\n\nA machine learning approach to modeling case-mix\nConsidering many of the problems cited by past research in modeling the high-dimensional, sparse data characteristic of most client information systems, we looked to a growing body of research in the medical domain that takes a novel approach to modeling case-mix through machine learning algorithms.\nA study that stood out to us was one that examined the feasibility of developing a visual representation of the work attributable to individual patients in a hospital setting (Benda et al., 2018). The idea was to build a live, dynamic visualization that could be used to compare workload across clinicians to improve patient assignment. The display was driven by an algorithm that predicted patient-level work based on a combination of clinical assessment scores and the number of orders or “events” (e.g., tests, phone calls, diagnoses) placed in a patient’s electronic health record (Benda et al., 2018). This approach was particularly interesting to us as the home agency for the current study uses a digital dashboard to monitor caseloads already–though it is a static system that is more useful for monitoring the past than predicting future caseload. Although clinicians evaluated their tool positively, the algorithm underlying the display was found to inadequately account for actual workload, suggesting more refinements were needed (Benda et al., 2018).\nBuilding on Benda et al. (2018)’s study, Wang et al. (2021) focused on improving the underlying algorithm with several machine learning algorithms known for their robustness in modeling sparse, heterogeneous data features found in electronic health records. Both regression and classification algorithms were used to model several proxies for workload: i) overall length of stay, ii) number of events (e.g., tests ordered, medication administered), iii) density of events (count of events divided by length of stay) and iv) a binary outcome indicating high versus low demand patients. The accuracy of prediction for low versus high length of stay was 70% with information from the first hour, 73% from the first two hours and 83% with data from the entire visit. Though the domain (emergency medecine versus community based mental health) and temporal aspects are different (hospital stay versus outpatient mental health treatment), Wang et al. (2021) ’s work demonstrates the potential for machine learning techniques to predict client-related work from information collected at intake–which is exactly what we are interested in doing."
  },
  {
    "objectID": "Caseweight Prospectus.html#themes-in-the-literature",
    "href": "Caseweight Prospectus.html#themes-in-the-literature",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Themes in the literature",
    "text": "Themes in the literature\nIn planning our approach to modeling client-related workload, several themes emerged in the literature that we hope to address in our research.\nFirst the most commonly cited problem in …. was the difficulties in modeling EMHD.\nFirst, we address a gap in case-mix research by focusing on the specific needs of younger people served by mental health providers in a community mental health outpatient setting. Our study targets child and youth mental health services specifically, addressing the unique requirements of this demographic.\nWe also intend to tackle the issue of inconsistent outcome measures. Previous studies often used flat proxy measures for workload, such as length of stay or number of appointments which fails to capture the variance in work intensity throughout the treatment period (CMHO, 2019; Martin et al., 2020). To address this, we will calculate a resource use measure on a per-diem basis, predicting resource use (direct and indirect time logged) per week (Wang et al., 2021). This method, supported by the System for Classification of In-Patient Psychiatry (SCIPP) developed in Canada, might better reflect the intensity of work than flat counts (CMHO, 2019).\nAdditionally, we will address several limitations related to client-related workload indicators. In many studies, variables like gender or race are used to categorize client need, which can raise concerns about fairness and predictor bias. While race and gender may correlate with resource use, these correlations can be confounded by systemic marginalization factors that may increase the risk for mental health concerns (Gaines et al., 2003; Tran et al., 2019). To avoid perpetuating this marginalization, we will focus on variables that drive resource use directly such as scale scores or symptom ratings, and exclude variables thay may indicate race, culture or ethnicity. Following Tran et al. (2019)’s recommendation to favour direct measures of client need, we plan to use interRAI screener+ items and scores specifically (Hirdes et al., 2020).\nFurthermore, to avoid some of the challenges faced in prior research when including both client and provider-side drivers of work, as indicated in their casemix recommendations that suggest including provider-side variables (i.e. years of experience or preferred modality) in models risks reinforcing systemic unfairness in case distribution, we will focus solely on client-side variables to predict workload.\nModel evaluation and metrics were another critical area of weakness across the literature (Tran et al., 2019). Few studies employed robust cross-validation methods and even fewer tested their models on unseen data (Tran et al., 2019). We will utilize cross-validation folds within our training set for model building and hold back a test set of unseen data for final model evaluation, ensuring a control for determining model accuracy. We will also split our data group-wise ensuring that clients in the training set are not including in the test set. This will ensure that we have a more robust measure of how well our model would perform on “unseen” clients. We will also clearly outline our choice in metrics used to evaluate model performance and document all R code for reproducibility (see Methods).\nFinally, issues relating to the complexities involved in modeling electronic health data was evident across all studies we looked at. We attempt to address this issue by utilizing machine learning (ML) algorithms which are better-suited to handle the high-dimensionality and heterogeneity of electronic mental health (EMH) data (Joseph et al., 2023). EMH data poses unique challenges such as hidden clustering, non-independence of observations, missing data and outliers. Consequently, a significant amount of data wrangling and variable culling is necessary for traditional analysis, often resulting in significant data loss and models with limited generalizability. In contrast, ML methods, such as random forests, gradient boosting, and neural networks, can better manage non-independent observations, non-normal distributions, and multicollinearity among predictor variables–albeit with varying levels of interpretability (Zeleke et al., 2023).\n\nWhat is machine learning?\n\nMachine learning is a branch of artificial intelligence where computers are trained to perform tasks by identifying patterns within data instead of relying on explicitly programmed instructions. In mathematical terms, machine learning algorithms use statistical techniques to optimize a model’s parameters. This process involves minimizing a loss function that quantifies the difference between the model’s predictions and the actual data. For example, in supervised learning, the goal is to find a function F(x) that maps input features X to an output Y such that the predicted outcomes are as close as possible to the true outcome (Nielsen, 2016). This approach is highly effective for complex tasks like image recognition, natural language processing, and predictive analytics, where traditional rule-based programming is infeasible due to the high dimensionality and variability of the data.\n\nThis capability makes ML particularly suitable for analyzing electronic health data, where the data may be sparse, exhibit non-linearity, missing values, and complex interdependencies among variables (An et al., 2023; Chen et al., 2023). Unlike traditional methods that might only confirm specific linear relationships, machine learning algorithms can identify high-dimensional interactions and non-obvious patterns. Techniques such as regularization and cross-validation further enhance the robustness of these models, maximizing their ability to generalize well to unseen data (Chen et al., 2023).\nMoreover, ML excels in both predictive accuracy and discovering new relationships that might be validated in subsequent studies. By leveraging algorithms that optimize for predictive performance, such as gradient boosting or ensemble methods, researchers can uncover patterns with the potential to drive new hypotheses. This abductive reasoning, informed by the patterns identified in ML models, may advance research by providing new directions for future investigation (Sheetal et al., 2023)."
  },
  {
    "objectID": "Caseweight Prospectus.html#purpose",
    "href": "Caseweight Prospectus.html#purpose",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Purpose",
    "text": "Purpose\nThe current research aims to extend past research to the CYMH domain by comparing the ability of several machine learning algorithms to predict client-related workload based on information collected at intake. Additionally, we will explore the potential of early predictions to improve client assignment, balance workload, and potentially identify overloaded providers."
  },
  {
    "objectID": "Caseweight Prospectus.html#hypotheses",
    "href": "Caseweight Prospectus.html#hypotheses",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Hypotheses",
    "text": "Hypotheses\n– ask for advice here –\nThough the current study will be largely exploratory in nature, we will be guided by several hypotheses. First, given past research indicating machine learning techniques are better able to capture complexities and patterns within EMH data than simple linear regression, we predict that tree-based machine learning models will be more accurate predictors of client-related work than linear regression. Moreover, we anticipate that mental health acuity features such as depression and anxiety scores will contribute more substantially to explaining the variance in client-related work than age. This hypothesis stems from existing literature indicating that these variables are critical drivers of resource use (Perlman et al., 2013; Tran et al., 2019). Finally, we anticipate that models will more accurately predict caseweight (hours spent directly with the client, divided length of stay), than flat proxies (e.g., overall length of stay) as a ratio should reflect the frequency/intensity of demands better than a flat count of weeks in service or length of stay (Wang et al., 2021)."
  },
  {
    "objectID": "Caseweight Prospectus.html#data-security",
    "href": "Caseweight Prospectus.html#data-security",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Security",
    "text": "Data Security\nGiven the sensitivity of mental health data, ensuring data privacy and security by obtaining the necessary ethical approvals and maintaining transparency throughout the research process, will be strictly enforced. The necessary approvals from relevant ethics boards will be obtained. An exemption must be granted by both the agency (Compass) and Laurentian’s institutional review board for the use of de-identified data.\nDe-identified clinical data will be acquired from an electronic health information system belonging to Compass. The EHR database is maintained by the institution. Data will be de-identified at extraction using the Health Insurance Portability and Accountability Act Safe Harbor Method (OCR 2012). This means that names, addresses, birthdates, postal codes and any other directly identifying information will be stripped from the dataset before any analysis begins. As an added precaution, unique client identification codes will be encrypted with a hashing system that makes it near impossible to reverse engineer the code to obtain original IDs. Furthermore, the data will not leave the custody of Compass and will only be analyzed by the principal researcher within a password-protected machine belonging to Compass.\nThe reporting of model results, summary statistics and other visualizations will only include metrics associated with the performance of predictors and the models themselves, never individual scores or any other identifying information that could be linked to clients or smaller subgroups of clients. Furthermore, the researchers will seek approval from Compass before results are shared or utilized in any report or presentation."
  },
  {
    "objectID": "Caseweight Prospectus.html#procedure",
    "href": "Caseweight Prospectus.html#procedure",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Procedure",
    "text": "Procedure\nThe following steps outline the proposed process which will consist of four phases: 1) data collection, preprocessing and exploration; 2) identifying a list of workload proxies (output/dependent variables) that could be used as stand-ins for actual workload; 3) identifying and extracting indicators of workload (i.e.,independent variables/features) that could be used to model our proxies; 3) modeling the relationship between the indicators and proxies with algorithms of varying complexity; and 4) evaluating and comparing the models’ performance on a set of unseen data (see Figure 3).\n\nData Collection & Preprocessing\nAfter deidentification, data preprocessing will involve cleaning, joining dataframes, handling missing values if necessary, and aggregating features in monthly narrowing items to only information available at intake. All decisions we make in regard to missing data, data normalization or any other changes will be decided on a case by case basis and reported in our final paper. Moreover, the final report will include any Python code necessary to replicate these steps. Pending approval, the data scripts will also be made publicly available."
  },
  {
    "objectID": "Presentation.html#statement-of-problem",
    "href": "Presentation.html#statement-of-problem",
    "title": "Beyond Counting Clients",
    "section": "",
    "text": "Providers reporting maxed caseloads\ncase complexity increasing over time\nlong waitlists for service\nat the same time, caseloads at historic lows\n\n\n\nRight column\n\n\n\nquantify the weight of cases to account for the reported increased complexity and find a way to determine who has the capacity to be assigned more youth."
  },
  {
    "objectID": "Presentation.html#workload-vs-caseload-vs-caseweight",
    "href": "Presentation.html#workload-vs-caseload-vs-caseweight",
    "title": "Beyond Counting Clients",
    "section": "Workload vs Caseload vs Caseweight",
    "text": "Workload vs Caseload vs Caseweight\nCaseload refers to the number of clients assigned to a worker at any given time.\nWorkload refers to the amount of time a provider spends on a case.\nCaseweight is a measure of client-related workload; in other words, the time required to support a client thats impacts the absolute number of clients a provider can support within their caseload. This will serve as our dependent variable.\n\nTraditionally, Compass used caseloads to determine whether a provider could support additional youth. Explain why bad"
  },
  {
    "objectID": "Presentation.html#provider-time-as-a-measure-of-workload",
    "href": "Presentation.html#provider-time-as-a-measure-of-workload",
    "title": "Beyond Counting Clients",
    "section": "Provider Time as a Measure of Workload",
    "text": "Provider Time as a Measure of Workload\n\nDirect Time\nNon-Direct Time\nNon-Client Related Time\n\n\nThe other major method of assessing workload is looking at worker’s time.\nAt Compass staff have to enter their entire days in the client information system. This type of approach was suggested by the Knowledge Institute on Child and Youth Mental Health and Addictions when they were trying to examine how to develop caseload/workload guidelines. At Compass we built dashboard that provides leaders information on staff time on a weekly basis.\nSo things seem straightforward enough. We can use client’s clinical assessment data and staff’s time entry data to build an algorithm that predicts the number of hours a client requires per week (we call this the caseweight). However, as we begun doing some data exploration we encountered a bit of a dilemma.\nBecause worker’s must to input all their time, there is never really “empty space” that a new client could fill.\u000b \u000bSo we looked at our broad time entry categories. There is direct time which we defined as any time spent with the client or their caregivers, non-direct time which is in-service of clients but involves work like documentation or case management and lastly we have non-client related time which is not attributable to specific clients so things like agency meetings and training.\nWe noticed is that staff seemed to input non-direct time and non-client related time inconsistently. We realized that acted kind of like a gas. It would expand or compress based on the worker’s available time.\nIn discussion we believed direct time would in our setting have the strongest most direct relationship to staff’s overall workload and client need and would be less prone to over-reporting."
  },
  {
    "objectID": "Presentation.html#existing-case-management-algorithms",
    "href": "Presentation.html#existing-case-management-algorithms",
    "title": "Beyond Counting Clients",
    "section": "Existing Case Management Algorithms",
    "text": "Existing Case Management Algorithms\n\noften used to evaluate total cost of an episode of care (e.g. total number of appointments, or total days the file is open)\nothers use proxies for service utilization like the number of services provided or the number of different types of services; which could be related but it distinct from workload\nother tools can distinguish between levels of care but do not necessarily account for the workload involved in a specific level of care\n\n\nWhile several case management algorithms exist, they come with a number of limitations - often used to evaluate total cost of an episode of care (e.g. total number of appointments, or total days the file is open) - others use proxies for service utilization like the number of services provided or the number of different types of services; which could be related but it distinct from workload - other tools can distinguish between levels of care but do not necessarily account for the workload involved in a specific level of care"
  },
  {
    "objectID": "Presentation.html#research-question",
    "href": "Presentation.html#research-question",
    "title": "Beyond Counting Clients",
    "section": "Research Question",
    "text": "Research Question\nCan a machine learning model trained on client data usefully predict the caseweight of a new intake?"
  },
  {
    "objectID": "Presentation.html#informant-study",
    "href": "Presentation.html#informant-study",
    "title": "Beyond Counting Clients",
    "section": "Informant Study",
    "text": "Informant Study\nWang et al. (2020) found via linear regression proved that the indicators explained a substantial amount of variance of the proxies (four out of five proxies were modeled with R2 &gt; 0.80). Classification algorithms also showed success in classifying a patient as having high or low task demand"
  },
  {
    "objectID": "Presentation.html#summary",
    "href": "Presentation.html#summary",
    "title": "Beyond Counting Clients",
    "section": "Summary",
    "text": "Summary\nEfforts has been made on improving patient assignment at triage to improve clinician workload management. o Data contained within the EHR have the potential to support automatic patient-related workload prediction. o Using indicators that are available in the EHR data, one can potentiallypredict patient-related workload at the early stage of patientvisit and update the prediction as the visit proceeds. o The predicted workload can potentially be used in assigning new patients to clinicians in a way that better balanced workload, or to identify clinicians that are overloaded"
  },
  {
    "objectID": "Presentation.html#informant-recommendations",
    "href": "Presentation.html#informant-recommendations",
    "title": "Beyond Counting Clients",
    "section": "Informant Recommendations",
    "text": "Informant Recommendations"
  },
  {
    "objectID": "Presentation.html#hypotheses",
    "href": "Presentation.html#hypotheses",
    "title": "Beyond Counting Clients",
    "section": "Hypotheses",
    "text": "Hypotheses\nH^1 - Model Performance Hypothesis: Machine learning models will do a better job of predicting the hours a client might need per week than regression models.\nH2 - Feature Importance Hypothesis: Specific intake features, such as initial severity of symptoms and demographic factors, will be significant predictors of caseweight."
  },
  {
    "objectID": "Presentation.html#how-will-we-know",
    "href": "Presentation.html#how-will-we-know",
    "title": "Beyond Counting Clients",
    "section": "How will we know?",
    "text": "How will we know?\n\nWe will use mae and rmse to evaluate all models"
  },
  {
    "objectID": "Presentation.html#model-interpretability",
    "href": "Presentation.html#model-interpretability",
    "title": "Beyond Counting Clients",
    "section": "Model Interpretability",
    "text": "Model Interpretability\n\nWang et al., suggested future work should investigate which indicators contributed most to the models, as well as why certain models showed better performance.\nImportance of indicators can be calculated using black box auditing techniques [25], and compared across different algorithms and different proxies.\nInterpretability of machine learning models have received increasing attention, and is particularly important in healthcare context in improving its transparency [26,27].\nKnowing which indicators were most helpful can also potentially decrease the number of indicators used in future models, which is beneficial to the ease of deployment."
  },
  {
    "objectID": "Presentation.html#dependent-variable---caseweight",
    "href": "Presentation.html#dependent-variable---caseweight",
    "title": "Beyond Counting Clients",
    "section": "Dependent Variable - Caseweight",
    "text": "Dependent Variable - Caseweight\nAdd characteristics of our outcome here"
  },
  {
    "objectID": "Presentation.html#independent-variables",
    "href": "Presentation.html#independent-variables",
    "title": "Beyond Counting Clients",
    "section": "Independent Variables",
    "text": "Independent Variables\n\nScreener scale scores\nScreener items\nDemographics Information"
  },
  {
    "objectID": "Presentation.html#predictor-selection",
    "href": "Presentation.html#predictor-selection",
    "title": "Beyond Counting Clients",
    "section": "Predictor Selection",
    "text": "Predictor Selection\n\nnecessary that all items, scales and information is available at intake"
  },
  {
    "objectID": "Presentation.html#machine-learning",
    "href": "Presentation.html#machine-learning",
    "title": "Beyond Counting Clients",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\nSo Machine Learning! What is it?\nBroadly, ML algorithms are a set of mathematical rules that look for relationships and patterns in data that can then be used to predict future relationships. When we use the term model, we are referring to an algorithm that has already been trained on a dataset to make specific predictions.\nIn our case, as Nick outlined, we were interested in learning whether a machine learning model trained on client data, could usefully predict the caseweight of a new intake."
  },
  {
    "objectID": "Presentation.html#model-building",
    "href": "Presentation.html#model-building",
    "title": "Beyond Counting Clients",
    "section": "Model Building",
    "text": "Model Building"
  },
  {
    "objectID": "Presentation.html#data-splitting",
    "href": "Presentation.html#data-splitting",
    "title": "Beyond Counting Clients",
    "section": "Data Splitting",
    "text": "Data Splitting"
  },
  {
    "objectID": "Presentation.html#cross-validation",
    "href": "Presentation.html#cross-validation",
    "title": "Beyond Counting Clients",
    "section": "Cross Validation",
    "text": "Cross Validation"
  },
  {
    "objectID": "Presentation.html#model-flow",
    "href": "Presentation.html#model-flow",
    "title": "Beyond Counting Clients",
    "section": "Model Flow",
    "text": "Model Flow"
  },
  {
    "objectID": "Presentation.html#plan",
    "href": "Presentation.html#plan",
    "title": "Beyond Counting Clients",
    "section": "Plan",
    "text": "Plan\n\n\n\n\n\n\n\n\nFigure 1: Indicators of Caseweight Modelling Proxy of Caseweight\n\n\n\n\n\nOur vision for the caseweight model is to use the client’s most recent clinical assessment data and information on their past service utilization to predict using our algorithm or model how much direct time (on average) that client is expected to require per week.\nWe can see for any given worker how much of their time is currently being spent doing direct client care. If they are below a certain threshold then we can assign clients in a manner that will keep a balanced workload and hopefully ensure they have capacity to support these clients. There are of course linguistic, cultural, and clinical factors managers will have to match during assignment as well.\nBut our aim is to give clinical leaders the evidence they need to make informed decisions about case assignments.\nThis is where machine learning has its benefits. It can help us distill and make sense of highly complex non-linear systems."
  },
  {
    "objectID": "Presentation.html#algorithm-fairness",
    "href": "Presentation.html#algorithm-fairness",
    "title": "Beyond Counting Clients",
    "section": "Algorithm Fairness",
    "text": "Algorithm Fairness\nMachine learning is being used in the child and youth mental health sector in Ontario but it is in many ways in its infancy.\nWe are still far away from implementation and as we progress implementation actually gets further away. One of the major areas of work we need to do is around algorithm fairness. We need examine the relationship between demographic groups and the model’s variables, outcomes, and the potential decisions."
  },
  {
    "objectID": "Presentation.html#interpretability",
    "href": "Presentation.html#interpretability",
    "title": "Beyond Counting Clients",
    "section": "Interpretability",
    "text": "Interpretability\nAnother important piece of work is around model interpretability and awareness. Some machine learning models can be relatively simple to understand like a decision tree, it is easy to see visually what variables resulted in the predicted intensity. However, models like a random forest can become more opaque and difficult to understand – we might be able to provide partial explanations but it wouldn’t be reasonable to expect an analyst let alone a clinician to understand the specific reasons why one prediction differs from another."
  },
  {
    "objectID": "Presentation.html#algorithm-governance",
    "href": "Presentation.html#algorithm-governance",
    "title": "Beyond Counting Clients",
    "section": "Algorithm Governance",
    "text": "Algorithm Governance\nAnd while I’ve said machine learning in the CYMH sector in Ontario is in many ways in its infancy– algorithms are not new. Many organizations are using algorithm and may not realize it, which is itself a challenge and risk.\nOver the past two decades there has been an explosion of work examining algorithm governance, implementation, and auditing. There are countless resources out there now for organizations to use to audit, assess, and implement algorithms. However, to my knowledge that work has not made its way into our sector in a major way.\nFor example, some of the standardize assessment tools currently available have scores that are calculated using algorithms that were developed with machine learning methods such as interactive tree builders. However, in my experience these algorithms do not have a substantive body of work conceptualizing or assessing fairness. They also do not appear to undergo auditing. I’ve not heard of agencies doing algorithm impact assessments either. The idea is that prior to implementing or using an algorithm that provides some type of score or informs decision making we should be doing our due diligence.\nHas the algorithm been assessed for fairness? How interpretable and understandable is the algorithm? How was the algorithm developed? What does it actually predict? How will we actually use it? What are the expected impacts of the algorithm? What safeguards do we have in place to identify harm?\nThis is can be challenging and time consuming. I believe we are at a time where we need to as individual organizations and a sector start asking and planning for these questions AND If we already are using algorithms to inform decision making we should be going back and reviewing these questions. We may in good faith trust tool developers and vendors, however, their context is different. Developing an algorithm as a research project is different from implementing one that can have material impacts on a family."
  },
  {
    "objectID": "Presentation.html#refs",
    "href": "Presentation.html#refs",
    "title": "Beyond Counting Clients",
    "section": "Refs",
    "text": "Refs\nCaseload guidelines working group (2019). Developing caseload/workload guidelines for Ontario’s child and youth mental health sector. Ottawa, ON: Ontario Centre of Excellence for Child and Youth Mental Health. Collins, G. S., Dhiman, P., Ma, J., Schlussel, M. M., Archer, L., Van Calster, B., Harrell, F. E., Martin, G. P., Moons, K. G. M., van Smeden, M., Sperrin, M., Bullock, G. S., & Riley, R. D. (2024). Evaluation of clinical prediction models (part 1): From development to external validation. BMJ, e074819. https://doi.org/10.1136/bmj-2023-074819 Johnson, N., Moharana, S., Harrington, C., Andalibi, N., Heidari, H., & Eslami, M. (2024). The fall of an algorithm: Characterizing the dynamics toward abandonment. Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, 337–358. https://doi.org/10.1145/3630106.3658910 Martin, P., Davies, R., Macdougall, A., Ritchie, B., Vostanis, P., Whale, A., & Wolpert, M. (2017). Developing a case mix classification for child and adolescent mental health services: The influence of presenting problems, complexity factors and service providers on number of appointments.  Journal of Mental Health, 29(4), 431–438. https://doi.org/10.1080/09638237.2017.1370631 Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 33–44. https://doi.org/10.1145/3351095.3372873 Tran, N., Poss, J. W., Perlman, C., & Hirdes, J. P. (2019). Case-mix classification for mental health care in community settings: A scoping review. Health Services Insights, 12, 1178632919862248. https://doi.org/10.1177/1178632919862248"
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#caseload-versus-workload",
    "href": "Caseweight Prospectus_rousseaucomments.html#caseload-versus-workload",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Caseload versus workload",
    "text": "Caseload versus workload\nIt is important here to distinguish between caseload and workload. In the current paper, we refer to caseload as the number of active clients assigned to a provider at any given time, while workload is the amount of time a provider spends serving each case as well as attend other professional responsibilities such as supervision, professional development, and training (CMHO, 2019).\nIn the literature, high case loads have been linked with lower self-reported efficacy (King et al., 2000) and poorer clinical outcomes (Lloyd & King, 2004), while smaller caseloads with better clinical outcomes and higher retention rates (Berkel & Knies, 2015). These findings supports earlier evidence that suggest caseloads exceeding 20 to 30 clients can lead to “reactive” case management characterized by deficiencies in service planning and family support (Intagliata, 1982; Lloyd & King, 2004). At the same time, another line of inquiry suggests that the relationship between caseload and burnout may not be as straightforward as it seems (King et al., 2000, 2004). For example, King et al. (2000) did not find that case counts were predictive of clinician burnout, but the mix of cases in regards to individual client-level complexity, was. He posited that providers may adapt to high caseloads by simply doing less for each case, suggesting a potential ‘dose-response’ relationship between a provider’s time and their effectiveness (King, 2009). Importantly, King et al. (2000) ’s work suggests that the ratio of low to high need cases in any given caseload may be a more important predictor of provider stress than counts alone (King, 2009). This finding is particularly relevant to the CYMH sector where the mix of cases can vary widely in terms of complexity and acuity (CMHO, 2019).\nTo illustrate, consider two clinicians each with a caseload of 20 clients. Clinician A, due to their years of experience, has a greater proportion of complex clients requiring intensive weekly therapy sessions, complex case management and frequent crisis interventions, while clinician B’s clients may require simpler interventions and less frequent check-ins. Additionally, Clinician A may have the added responsibility of supervision and attending meetings which further increases their overall workload. Thus, even with the same caseload, the workload between them may differ significantly in regards to the intensity and nature of the services required by their clients and other professional duties."
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#a-measure-of-client-related-work",
    "href": "Caseweight Prospectus_rousseaucomments.html#a-measure-of-client-related-work",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "A measure of client-related work",
    "text": "A measure of client-related work\nGiven these findings, if we had the ability to accurately predict the workload associated with each client, agencies might better manage provider caseloads. However modeling client characteristics, let alone predicting the work that is driven by them, has proved difficult so far (CMHO, 2019; King, 2009; Tran et al., 2019). Despite initiatives like the Quadruple Aim Framework meant to improve health outcomes, reduce costs, and improve provider work-life balance, the CYMH sector continues to face challenges in establishing a consistent, standardized measurement system, let alone a dynamic case assignment system that can accurately reflect the demands placed on staff (Arnetz et al., 2020; CMHO, 2019). Nevertheless, the new guidelines have informed new policies meant to improve the tracking of client-related work which has enabled several agencies to expand efforts to understand and track workload in their own organization.\nAn important workload metric that was born out of the AG audit and recommendations was a policy change requiring all publicly funded CYMH service providers in Ontario to report all direct-time time spent in the service of clients (Ministry of Health, 2024). Direct-time is defined as the number of hours spent in face-to-face interactions, phone or video-based communications, and meetings with parents and caregivers while indirect hours involve client-related tasks like documentation, telephone calls, advocacy, and consultations. The sum of direct and indirect hours amounts to the overall “work” attributable to a given client–a metric we will use in our models as an approximation of the work attributed to an individual client.\nIt is important to stress that the direct and indirect hours logged are only a proxy for client-related work, as hours alone may not convey differences in emotional or mental effort required to treat different clients nor the stress associated with a case that may factor in to the actual workload experienced by the provider and associated with any given client that is more difficult to measure.\nCompass, the lead child and youth mental health agency in the districts of Sudbury and Manitoulin and the proposed site of the current study, utilizes a dashboard to monitor caseloads and the associated direct and indirect time logged by service providers. While the dashboard has been a useful tool to compare caseloads between clinicians and across teams, it offers less insight into how much work is associated with each case. Moreover, it doesn’t aid in assigning new cases beyond indicating which providers have more “space” in their caseload than others. For this reason, the agency wondered whether intake data like age and psychological screener scores could be used to quantify client complexity with a weight for each case that might be used to monitor caseloads as well as inform new case assignment."
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#how-do-we-model-client-complexity",
    "href": "Caseweight Prospectus_rousseaucomments.html#how-do-we-model-client-complexity",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "How do we model client complexity?",
    "text": "How do we model client complexity?\nAcross domains, various strategies have been employed to manage provider workload, many focusing on increasing client-flow by providing different levels of service determined by presenting characteristics like symptom severity or prior diagnoses (Johnson et al., 1998; Tran et al., 2019). A popular model is the case-mix classification system, which assigns clients to different categories based on their expected resource use (CMHO, 2019). Case-mix classification systems usually take one of two approaches to modeling client-related work: grouping or index. Grouping systems assign people into classes in terms of their expected resource use, with each group having a specific weight attached to it, representing expected resource use (e.g., high-need versus low-need) relative to the average case in the population; Index systems on the other hand, combine different characteristics of a case to provide a continuous, numerical value which represents expected resource use relative to an average case (e.g., total hours, total length of stay or cost associated with care) (Tran et al., 2019).\nIn reviewing the literature for the current study, we relied on a 2019 scoping review of case-mix classifications for community based mental health care as a starting point, hoping it would lead to discovering more recent work (Tran et al., 2019). Unfortunately, the single case that looked at case-mix classification to predict mental health care resource use in CYMH community settings remains the only case of its kind (Martin et al., 2020; Tran et al., 2019). In that study, researchers modeled 4573 client records from eleven UK outpatient community based child and youth mental agencies to predict the number of appointments a client might need (Martin et al., 2020). Three classification methods were compared: two data driven (cluster analysis and regression trees) and one conceptual (classification informed by clinical judgement) to predict the number of appointments a client might need. Contrary to what they expected, they found the classification algorithms ability to predict accurately on new data was weak, and not significantly better than clinical judgement (Martin et al., 2020). Moreover, they found little statistical evidence to support the idea that client complexity had much to do with differences in resource provision (Martin et al., 2020). Researchers cited several reasons for problems explaining the variance in resource use: data quality, omission of important individual-level factors and lack of standardization of practice between providers.\nIn a different population, another group of researchers tried to predict the workload associated with patients at a community based mental health centre for the elderly (Baillon et al., 2009). Using an 8-item case weighting scale (CWS) that identified factors staff felt contribute to demand for staff time, they built a multiple regression model to assign different weightings to each item based on the strength of its relationship with the outcome (an estimation of time spent on each client logged over a four week period). The model was then used to predict the total time a client would utilize in a four-week period following the first appointment. Though they reported the model as a success–the model accounting for 58% of the variation in time spent on client-related work–the sample was small, consisting of only 87 cases and relied on a statistical method inappropriate for evaluating agreement between model predictions and actual observations, leaving it unclear how accurate the model actually was (Baillon et al., 2009; Mansournia et al., 2021). Moreover, Inter-rater and re-rater reliability results indicated that the assessment, whether from a client’s self-report or a professional’s clinical opinion, did not necessarily relate to the amount of time needed by clients (Baillon et al., 2009).\n\nCase-mix in other domains\nThough there is less work in the CYMH domain, several measures of workload intensity have been developed to manage caseloads in other specialties, particularly in inpatient medical settings. For example, in general psychiatry, researchers have used factors like sociodemographics, functional ability, and caregiver and social network characteristics to predict service utilization. Though, many of these models similarly lacked adequate evaluations and would not be suitable for all client groups, Tran et al. (2019), recommend that it may be useful to experiment with case-mix systems developed in other settings.\nOne model that the authors of the review suggested may be a good candidate for testing in community settings is Canada’s System for Classification of In-Patient Psychiatry (SCIPP) (Perlman et al., 2013; Tran et al., 2019). The SCIPP algorithm is a grouping methodology that sorts patients according to clinical characteristics obtained from standardized interRAI assessment data to estimate resource use (Hirdes et al., 2020; Perlman et al., 2013).\n\n\nMeasuring Client-Related Work\ninterRAI is an international research network “dedicated to developing clinical standards across a variety of health and social service settings” that have developed a large toolkit of instruments that are used by health organizations worldwide to assess people at the point of care across a variety of domains including emergency medicine, emergency psychiatry and children and youth mental health. The resulting data are meant to be used at the agency level for quality improvement activities, benchmarking, program planning and resource planning and at the system level to compare health data across regions and provinces (Hirdes et al., 2002). In Canada, interRAI is partnered with the Canadian Institute for Health Information (CIHI) who acts as a custodian of interRAI standards for Canada and houses and monitors the collected interRAI data. Many interRAI instruments are used across Canada and internationally, but the ChYMH represents the first assessment specifically designed for children and youth (Stewart & Hamza, 2017).\nIn Ontario, two instruments are most often used in the CYMH sector: the Child and Youth Mental Health Screener+ (ChYMH-S) and the more comprehensive full ChYMH and its variants. The primary use of the CHYM-S is to support decision making related to triaging, placement and service utilization while the full ChYMH and its associated Collaborative Action Plans (CAPs) are meant to assess, repond to and monitor the strengths, preferences and mental health needs of clients in in-patient and out-patient treatment. It is currently being utilized in Ontario at over 60 mental health agencies.\nThe full ChYMH includes over 400 items meant to build a comprehensive picture of a child/youth’s strengths, needs, functioning and areas of risk to inform care planning (Stewart et al., 2022), while the ChYMH-S is comprised of 106 items and is intended as a brief screener to identify young people who are in need of more comprehensive assessment. The screener is administered via a semi-structured interview to children and youth between 4-18 years in a variety of settings and is intended to take 15-20 minutes to complete. The current study will focus on screener data collected at intake.\nThe ChYMH-S includes 34 administrative and tracking items, 26 mental health indicators, 5 substance use indicators, 9 questions related to harm to self and others, 6 behaviour focused items, 1 cognitive item as well as items that look to track social relations, anxiety levels, medications, living arrangements, diagnoses, physical conditions, past interventions and current and past strengths and resilience (Stewart et al., 2022). The Depressive Severity Index, Anxiety Scale, Disruptive/Aggressive Behaviour Scale, Hyperactive/Distraction Scale and Internalizing/Externalizing scales are all included in the ChYMH-S. See table for full list of items. The instrument and the various scales and items it comtains have been tested extensively in research worldwide, demonstrating reliable face, content, construct and predictive validity [lots of detail could go here–please inform as to how much!.\nImportantly, several ChYMH scales have demonstrated strong predictive validity. For example, data from over 5000 children and youth placed in psychiatric settings in Ontario found that the Agressive Behaviour Scale was predictive of multiple control interventions, while the Severity of Self Harm Scale (SOS) has proven useful in predicting admission for risk of self-harm in youth between 10-17 years. Moreover, individuals who score higher on scales like the Hyperactive/Distraction scale were more likely to have a provisional diagnosis of ADHD (Stewart et al., 2022).\nThough the various scales and items of the ChyMH have demonstrated some predictive utility, it remains unclear how well these items might predict the actual work required to serve any given client. Within the CYMH domain we found one example of interRAI data being used to develop an algorithm to predict resource cost for children and youth with developmental disabilities with cluster analysis (Stewart et al., 2020). Though the resulting Child and Youth Resource Index (ChYRI) could only explain 30% of the variance in per diem costs for community based services (Stewart et al., 2020), the algorithm was deemed a success. Nevertheless, a lack of explanation of how the analysis was conducted, makes it unclear where the algorithm could be improved.\nIt is important to note, we are not interested in predicting client-related work for budgetary reasons, but instead our efforts are in response to requests from clinicians themselves to develop a resource allocation system that accounts not only for current capacity in regards to counts, but the differences in need (and thereby work) between clients so that caseloads between clinicians are more equitable and mindful of the work that is already on their plate.\n\n\nA machine learning approach to modeling case-mix\nConsidering the limitations and lack of transparency in regards to the SCIPP and ChYRI development, we looked to academia and specifically the medical domain which has experimented with several ways of modelling electronic health data (EHD) (e.g. interRAI scores, test results, diagnoses) to predict patient-related work for managing professional caseloads and other hospital resources. ….https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8967950/.\nOne study that stood out to us, utilized…\nBuilding on Benda et al. (2018)’s study, Wang et al. (2021) focused on improving the underlying algorithm with various machine learning algorithms known for their robustness in modeling sparse, highly heterogeneous data features. To this end, both regression and classification algorithms were applied to model several proxies for workload (length of stay, number of events, and density of events) and high versus low demand patients. The accuracy of prediction for low versus high length of stay was 70% with information from the first hour, 73% from the first two hours and 83% with data from the entire visit. Though the domain and temporal aspects were different (hospital stay versus outpatient mental health treatment), their methodology demonstrates the potential of leveraging machine learning techniques to predict client-related workload from information collected at intake."
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#themes-in-the-literature",
    "href": "Caseweight Prospectus_rousseaucomments.html#themes-in-the-literature",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Themes in the literature",
    "text": "Themes in the literature\nIn planning our approach to modeling client-related workload, several themes emerged in the literature that we hope to address in our research.\nFirst the most commonly cited problem in …. was the difficulties in modeling EMHD.\nFirst, we address a gap in case-mix research by focusing on the specific needs of younger people served by mental health providers in a community mental health outpatient setting. Our study targets child and youth mental health services specifically, addressing the unique requirements of this demographic.\nWe also intend to tackle the issue of inconsistent outcome measures. Previous studies often used flat proxy measures for workload, such as length of stay or number of appointments which fails to capture the variance in work intensity throughout the treatment period (CMHO, 2019; Martin et al., 2020). To address this, we will calculate a resource use measure on a per-diem basis, predicting resource use (direct and indirect time logged) per week (Wang et al., 2021). This method, supported by the System for Classification of In-Patient Psychiatry (SCIPP) developed in Canada, might better reflect the intensity of work than flat counts (CMHO, 2019).\nAdditionally, we will address several limitations related to client-related workload indicators. In many studies, variables like gender or race are used to categorize client need, which can raise concerns about fairness and predictor bias. While race and gender may correlate with resource use, these correlations can be confounded by systemic marginalization factors that may increase the risk for mental health concerns (Gaines et al., 2003; Tran et al., 2019). To avoid perpetuating this marginalization, we will focus on variables that drive resource use directly such as scale scores or symptom ratings, and exclude variables thay may indicate race, culture or ethnicity. Following Tran et al. (2019)’s recommendation to favour direct measures of client need, we plan to use interRAI screener+ items and scores specifically (Hirdes et al., 2020).\nFurthermore, to avoid some of the challenges faced in prior research when including both client and provider-side drivers of work, as indicated in their casemix recommendations that suggest including provider-side variables (i.e. years of experience or preferred modality) in models risks reinforcing systemic unfairness in case distribution, we will focus solely on client-side variables to predict workload.\nModel evaluation and metrics were another critical area of weakness across the literature (Tran et al., 2019). Few studies employed robust cross-validation methods and even fewer tested their models on unseen data (Tran et al., 2019). We will utilize cross-validation folds within our training set for model building and hold back a test set of unseen data for final model evaluation, ensuring a control for determining model accuracy. We will also split our data group-wise ensuring that clients in the training set are not including in the test set. This will ensure that we have a more robust measure of how well our model would perform on “unseen” clients. We will also clearly outline our choice in metrics used to evaluate model performance and document all R code for reproducibility (see Methods).\nFinally, issues relating to the complexities involved in modeling electronic health data was evident across all studies we looked at. We attempt to address this issue by utilizing machine learning (ML) algorithms which are better-suited to handle the high-dimensionality and heterogeneity of electronic mental health (EMH) data (Joseph et al., 2023). EMH data poses unique challenges such as hidden clustering, non-independence of observations, missing data and outliers. Consequently, a significant amount of data wrangling and variable culling is necessary for traditional analysis, often resulting in significant data loss and models with limited generalizability. In contrast, ML methods, such as random forests, gradient boosting, and neural networks, can better manage non-independent observations, non-normal distributions, and multicollinearity among predictor variables–albeit with varying levels of interpretability (Zeleke et al., 2023).\n\nWhat is machine learning?\n\nMachine learning is a branch of artificial intelligence where computers are trained to perform tasks by identifying patterns within data instead of relying on explicitly programmed instructions. In mathematical terms, machine learning algorithms use statistical techniques to optimize a model’s parameters. This process involves minimizing a loss function that quantifies the difference between the model’s predictions and the actual data. For example, in supervised learning, the goal is to find a function F(x) that maps input features X to an output Y such that the predicted outcomes are as close as possible to the true outcome (Nielsen, 2016). This approach is highly effective for complex tasks like image recognition, natural language processing, and predictive analytics, where traditional rule-based programming is infeasible due to the high dimensionality and variability of the data.\n\nThis capability makes ML particularly suitable for analyzing electronic health data, where the data may be sparse, exhibit non-linearity, missing values, and complex interdependencies among variables (An et al., 2023; Chen et al., 2023). Unlike traditional methods that might only confirm specific linear relationships, machine learning algorithms can identify high-dimensional interactions and non-obvious patterns. Techniques such as regularization and cross-validation further enhance the robustness of these models, maximizing their ability to generalize well to unseen data (Chen et al., 2023).\nMoreover, ML excels in both predictive accuracy and discovering new relationships that might be validated in subsequent studies. By leveraging algorithms that optimize for predictive performance, such as gradient boosting or ensemble methods, researchers can uncover patterns with the potential to drive new hypotheses. This abductive reasoning, informed by the patterns identified in ML models, may advance research by providing new directions for future investigation (Sheetal et al., 2023)."
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#purpose",
    "href": "Caseweight Prospectus_rousseaucomments.html#purpose",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Purpose",
    "text": "Purpose\nIn summary, the current research aims to extend previous work in developing a measure of client-related work while addressing the recommendations and limitations of the same studies. We propose a quasi-experimental data science framework to test our hypotheses and evaluate the models’ predictive performance . Models will be trained on one set of data, while another set of “unseen” future data will act as a control to evaluate model performance.\nOur goal is to better understand the relationship between client-related workload indicators, workload proxies, and client complexity in the CYMH sector. Additionally, we will explore the potential of early predictions to improve client assignment, balance workload, and potentially identify overloaded providers. Given the pressing need in the CYMH sector for a case-management system that accurately and fairly assesses caseload we feel our proposed study can positively contribute to the findings."
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#hypotheses",
    "href": "Caseweight Prospectus_rousseaucomments.html#hypotheses",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe study will be guided by several hypotheses.\nFirst, given past research indicating machine learning techniques are better able to capture complexities and patterns within EMH data, we predict that tree-based machine learning models will be better predictors of client-related work across all workload proxies than linear regression or cluster analysis.\nSecond, we anticipate that mental health acuity features such as depression scores at intake, will be more significant predictors of client-related work than age alone. This hypothesis stems from existing literature indicating that these variables are critical determinants of resource use (Perlman et al., 2013; Tran et al., 2019).\nThird, we predict that workload indicators (independent variables) will explain a higher proportion of the variance in flat proxies (total hours in service or total days in service) than dynamic proxies (caseweight: total hours divided by the number of weeks). Though we believe a ratio better represents the intensity and frequency of demands placed on providers than flat counts alone, these scores may compound worker-level effects that the models will have difficulty accounting for (Wang et al., 2021)."
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#data-security",
    "href": "Caseweight Prospectus_rousseaucomments.html#data-security",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Security",
    "text": "Data Security\nGiven the sensitivity of mental health data, ensuring data privacy and security by obtaining necessary ethical approvals and maintaining transparency throughout the research process will be strictly enforced. Necessary approvals from relevant ethics boards will be obtained. An exemption must be granted by both the agency and Laurentian’s institutional review board for the use of de-identified data.\nDeidentified clinical data will be acquired from an electronic health information system belonging to Compass. The EHR database is maintained by the institution. Data will be deidentified at extraction using the Health Insurance Portability and Accountability Act Safe Harbor Method (OCR, 2012). This means that names, addresses, birthdates, full postal codes, clinical notes and any other directly identifying information will be stripped from the dataset before any analyses begins. As an added precaution, unique client identification codes will be encrypted with a hashing system that makes it near impossible to reverse engineer the code to obtain original IDs. Furthermore, the data will not leave the custody of Compass and will only be analyzed by the principal researcher within a password-protected machine belonging to Compass.\nThe reporting of model results, summary statistics and other visualizations will only include metrics associated with the performance of predictors and the models themselves, never individual scores or any other information that could be linked to clients or smaller subgroups of clients. Furthermore, the researchers will seek approval from Compass before results are shared or utilized in any report or presentation."
  },
  {
    "objectID": "Caseweight Prospectus_rousseaucomments.html#procedure",
    "href": "Caseweight Prospectus_rousseaucomments.html#procedure",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Procedure",
    "text": "Procedure\nThe following steps outline the proposed experimental process which will consist of four, broad phases: 1) data collection, preprocessing and exploration; 2) identifying a list of workload proxies (output/dependent variables) that could be used as stand-ins for actual workload; 3) identifying and extracting indicators of workload (i.e. independent variables/features) that could be used to model our proxies; 3) modeling the relationship between the indicators and proxies with algorithms of varying complexity; and 4) evaluating and comparing the models’ performance on a set of unseen data (see Figure 3).\n\nData Collection & Preprocessing\nAfter deidentification, data preprocessing will involve cleaning, joining dataframes, handling missing values, and narrowing items to only information available at intake. All decisions we make in regard to missing data, data normalization or any other changes will be reported in our final paper. Moreover, the final report will include the R code necessary to replicate these steps .\nImportantly, feature engineering–the creation of new predictors based on existing variables in the dataset–will occur after data splitting on the training data to minimize the risk of data leakage that could inadvertently occur when creating new variables from the full dataset (Sheetal et al., 2023).\n\nIndependent Variables (features/indicators of caseweight).\nThe main criterion for inclusion will be the variable’s availability in the electronic health record (EHR) system at intake. Information collectedincludes items from the interRAI Screener+ and full ChYMH screener. The interRAI ChYMH is a clinician-rated tool that is completed based on a semi-structured clinical interview [ADD BETTER DESCRIPTION HERE] and includes several scale scores, items as well as demographics information (Stewart & Hamza, 2017). A full list of all variables will be included in the final report.\n\n\n\n\nFigure 1\n\n\nClinical pathway\n\n\n\n\n\n\n\n\nNote. Flow chart of client selection process. Clients who complete an intake assessment will be screened for inclusion. Only clients referred to Counselling and Therapy (CT) services will be included in the analysis. Predictions will include: client-related work at follow-up assessment and client-related work at end of treatment.\n\nADD TABLE OF POTENTIAL INDICATOR VARIABLES\n\n\nDependent Variables (workload proxies).\nInformed by Wang et al. (2021), we intend to measure the influence of our workload indicators on Length of Service (weeks), Hours of Direct Time, Indirect Time and combined Direct and Indirect Time as well as a case density score (caseweight) calculated by dividing the number of hours (direct and indirect) spent with a client by the number of weeks in service (see Equation 1).\n\nCaseweight = \\frac{Hours Spent}{Weeks Open}\n\\tag{1}\nDensity scores should reflect the frequency/intensity of demands better than a flat count of weeks in service or length of stay (Wang et al., 2021). As such, we predict it will be a better indicator of work intensity, tempo and complexity. Length of Service (LoS) will be calculated as the number of days from assessment to the point at which they were discharged. LoS will not include the time a client waited for service on a waitlist as this reflects a provider-side driver of work that would bias our model. See Figure 2 for a visualization of the relationship between variables.\n\n\n\n\nFigure 2\n\n\nModeling Caseweight–client-related work\n\n\n\n\n\n\n\n\nNote. Using indicators of client-related work (e.g. depression scores, anxiety scores) in electronic health record (EHR)to predict workload proxies. Adapted from Predictors of Workload, by Wang et al. (2021).\n\nADD TABLE OF WORKLOAD PROXIES\nIf time and the data structure permits, we will attempt to construct a third “event count” proxy that tallies the total number of appointments, phone calls, and other case events associated with each case then divides by the number of weeks in service. Time and approval permitting, we would also like to consult with clinical managers to obtain a list of indicators that, in their expert opinion, would signal a client who could demand more work during the screening interview—these will be used to externally validate the variable choices of the models.\nWe will model our proxies as continuous “caseweights” and as classes split into two and three evenly distributed classes separately for classification. For example, a two class outcome (low versus high resource demand) will be split at the median.\nIn practice, regression will be used to model the number of hours per week a client will utilize across their episode of care, while binary classification will predict whether the stay is shorter or longer than the median for that program. Informed by Wang et al. (2021), we will first include data collected over the entire program length to determine whether the indicators have any utility in modelling the workload proxies. Then, we will use each client’s first assessment to predict case weight at follow up assessment (typically 3 months later). This will allow us to test whether a workload prediction in the earliest stages of a visit is even feasible.\n\n\n\nData Splitting\nTo ensure the robustness and generalizability of our models, data will be randomly split by groups (client ID). This method ensures that clients in the training set, which is used to train the models, are not included in the test set, thus preventing data leakage and providing a more unbiased evaluation of model performance (Figure 3).\nThe training set will be used to train the models using 10-fold group cross-validation. In group cross-validation, the training data is divided into 10 subsets, or “folds,” while preserving the grouping structure. This technique helps tune the models by iteratively training on nine folds and validating on the remaining one, ensuring each group is used for validation exactly once. Group cross-validation is particularly beneficial when dealing with repeated measurement data, as it maintains the integrity of the group structure and prevents information from the test folds from leaking into the training process.\nThe test set will act as a control group to evaluate the models’ performance on unseen data at the very end of the training process. By keeping the test set separate and untouched during training, we ensure that our final evaluation provides a better estimation of how the models will perform in practice. This step is crucial for assessing the models’ generalizability and for identifying any overfitting that may have occurred during training.\n\n\n\n\nFigure 3\n\n\nExperimental procedure\n\n\n\n\n\n\n\n\nNote. Flowchart of the experimental procedure. Data will be split into training and test sets by client ID. The training set will be used to train the models using 10-fold group cross-validation, while the test set will act as a control group to evaluate the models’ performance on unseen data.\n\n\n\nModel Selection\nWe propose testing both regression and classification algorithms to model our data. We plan to use the following supervised machine-learning algorithms: i) Random Forest (RF) for its ability to handle large datasets with high dimensionality; ii) XGBoost, known for its high performance and predictive accuracy on tabular datasets; iii) lasso regression to manage and select relevant predictors while handling high multicollinearity (this list may grow). The supervised machine-learning classification algorithms will be trained using the TidyModels suite of packages in R Studio. These algorithms were chosen based on their success modeling similarly complex, tabular data types (Salditt et al., 2023; Sheetal et al., 2023).\n\n\nValidation and Testing\nAll trained models will be statistically evaluated on the test set using the following performance metrics: i) Mean Absolute Error (MAE), and ii) Root Mean Squared Error (RMSE) for continues outcomes. For categorical outcomes, we will rely on accuracy and area under the curve (AUC). We will also look at precision and specificity, and for categorical outcomes with more than 2 categories, we will examine the F-1 Score. These evaluations will help determine the accuracy, generalizability and robustness of each model (Salditt et al., 2023; Wang et al., 2021)\n\n\nFinal Feature Importance Analysis\nThe final models will be analyzed to identify the most significant predictors of client-related workload. This will involve examining the feature importance scores from the best-performing models.\n\n\nSoftware and Tools\nWe will use R Statistical Software and the Tidyverse and TidyModels suite of packages for data manipulation and model building (R Core Team, 2024; Khun & Wickham 2020). This choice aligns with our familiarity with R and the study’s specific requirements. R Quarto Markdown will be used for documentation and reproducibility ."
  },
  {
    "objectID": "Caseweight Prospectus.html#existing-strategies-for-managing-caseloads",
    "href": "Caseweight Prospectus.html#existing-strategies-for-managing-caseloads",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Existing strategies for managing caseloads",
    "text": "Existing strategies for managing caseloads\nAccording to two audits of Ontario’s CYMH agencies conducted by the Auditor General (AG) starting in 2008 and most recently in 2016, a central problem that impedes the ability of agencies to efficiently meet this challenge, is determining reasonable provider to client workload ratios (Office of the Auditor General of Ontario, 2016)."
  },
  {
    "objectID": "Caseweight Prospectus.html#modeling-client-related-workload",
    "href": "Caseweight Prospectus.html#modeling-client-related-workload",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Modeling client-related workload",
    "text": "Modeling client-related workload\nIn planning our approach to modeling client-related work we hope to address several problems and limitations that emerged in the literature. The first, which dictated our chosen machine learning (ML) methodology, relates to the complexities that arise from modeling electronic health data(Joseph et al., 2023). EMH data poses unique challenges such as hidden clustering, non-independence of observations and multi-collinearity which ML methods, such as random forests and gradient boosted forests can better manage; albeit with varying levels of explainability (An et al., 2023; Chen et al., 2023; Zeleke et al., 2023). Though there is often a tradeoff between interpretability and increased predictive accuracy with many ML models, modern methods like SHapley Additive exPlanations (SHAP) offer an alternative way of understanding the contribution of inidividual features (varibles) to a specific prediction (Lundberg & Lee, n.d.).\n\nWhat is machine learning?\n\nMachine learning is a branch of artificial intelligence where computers are trained to perform tasks by identifying patterns within data instead of relying on explicitly programmed instructions. In mathematical terms, machine learning algorithms use statistical techniques to optimize a model’s parameters. This process involves minimizing a loss function that quantifies the difference between the model’s predictions and the actual data. For example, in supervised learning, the goal is to find a function F(x) that maps input features X to an output Y such that the predicted outcomes are as close as possible to the true outcome (Nielsen, 2016). This approach is highly effective for complex tasks like image recognition, natural language processing, and predictive analytics, where traditional rule-based programming is infeasible due to the high dimensionality and variability of the data.\n\nUnlike traditional methods that might only confirm linear relationships, ML algorithms are better equiped to identify high-dimensional interactions and non-obvious patterns. Techniques such as regularization and cross-validation further enhance the robustness of these models, maximizing their ability to generalize well to unseen data (Chen et al., 2023). Cross validation is a method used to evaluate how well a machine learning model will perform on new data. Instead of training a model on the entire dataset, data is divided into several parts or “folds”. The model is trained on some of the folds and tested on the remaining ones. This process is repeated multiple times, with each fold being used as a test set once. Depending on the model, the results are then averaged to give a more reliable estimate of the models performance and ensures that the model isn’t just performing well on a specific subset of the data. On the other hand, regularization is a technique used in ML that prevents overfitting which happens when a model performs well on training data but poorly on new, unseen data. Regularization helps by adding a penalty to a model’s complexity, essentially shrinking the influence or weight of potentially redundant parameters in the model (e.g. Lasso models).\nWe also intend to tackle the issue of inconsistent outcome measures. Previous studies often relied on flat proxy measures such as length of stay or number of appointments to serve as a proxy for workload which do not capture the variance in work intensity throughout the treatment period (CMHO, 2019; Martin et al., 2020). To address this, we will calculate a work measure on a per-diem basis, based on the total time logged in service of the client per week, which we hope will better reflect the intensity of work a flat count alone (CMHO, 2019; Wang et al., 2021).\nAdditionally, we will address several limitations related to specific client-level indicators like gender or race, which raise concerns about fairness and predictor bias. It is important to note that while race and gender may correlate with resource use, they may be confounded by systemic marginalization associated with an increased risk for mental health concerns (Gaines et al., 2003; Tran et al., 2019). To avoid perpetuating this marginalization, we will focus on variables that drive resource use directly such as age, diagnoses and symptom ratings, and exclude variables that indicate race, culture or ethnicity. Following Tran et al. (2019)’s recommendation to favor direct measures of client need, we plan to use interRAI screener+ items and scores specifically (Hirdes et al., 2020).\nFurthermore, to avoid challenges faced in prior research in disentangling the effects of client and provider-side drivers of work, we will focus solely on modeling client-side drivers as recommended by Tran et al. (2019)’s casemix recommendations that suggest including provider-side variables (i.e., years of experience or preferred modality) in modeling workload risks reinforcing systemic unfairness in case distribution. Moreover, we are interested in predicting workload from information collected at intake, for the purpose of aiding decision making around client assignment, a point in treatment time when provider related data is not yet available.\nFinally, adequate model evaluation was a significant area of weakness across the literature (Tran et al., 2019). Few studies employed robust cross-validation methods and even fewer tested their models on unseen data (Costa et al., 2015; Reid et al., 2021; Tran et al., 2019). To counter these limitations, we will utilize cross-validation folds within our training set for model building and tuning and will hold back a test set of unseen data for final model evaluation which will serve as a control for determining model accuracy. We will also randomly split our data in a group-wise fashion, ensuring that clients with multiple rows who are in the training set will not be included in the test set. This will ensure that we have a more robust measure of how well our model will perform on “unseen” clients. We will also clearly outline our choice of models, metrics and all R code will be made available for reproducibility (see Methods)."
  },
  {
    "objectID": "Caseweight Prospectus.html#client-related-time",
    "href": "Caseweight Prospectus.html#client-related-time",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Client-related time",
    "text": "Client-related time\nAn important workload metric that was born out of the AG audits and recommendations that followed, was a policy change requiring all publicly funded CYMH service providers in Ontario to report all direct-time time spent in the service of clients (MOH CYMH Service Description Schedules - Central Forms Repository (CFR), n.d.). Direct-time is defined as the number of hours spent in face-to-face interactions, phone or video-based communications, and meetings with parents and caregivers, while indirect hours involve client-related tasks like documentation, travel time, and consultations. In theory then, the sum of all direct and indirect hours logged by a clinician amounts to the overall “work” attributable to a given client. Direct and indirect hours logged will be a metric we use in our models to approximate the work attributed to an individual client. However, it is important to stress that the hours logged are only a proxy for client-related work, as hours alone may not convey differences in the emotional or mental effort required to treat different clients nor the stress associated with a case that may factor in to the actual workload experienced by the provider and associated with any given client–all provider side influences that are more difficult to measure.\nOne Ontario agency that has began using direct and indirect hours to track and manage caseloads, is Compass, the lead child and youth mental health agency in the districts of Sudbury and Manitoulin and the proposed site of the current study. Compass utilizes an electronic dashboard to monitor caseloads by tracking the associated direct and indirect time logged by service providers. The dashboard has been useful for comparing caseloads between clinicians and across teams, but offers less insight into how much work is associated with each case. Moreover, it doesn’t help in assigning new cases beyond indicating which providers have higher or lower case counts than others. In response to calls from both clinicians and managers, to develop a more thoughtful system for client assignment that considers not only counts, but the complexity of client need, the agency wondered whether psychological screening data collected at intake could be modeled to quantify client complexity by attributing a weight to each case that might aid in monitoring existing caseloads as well as inform new case assignment."
  },
  {
    "objectID": "Caseweight Prospectus.html#modeling-client-related-work-with-machine-learning",
    "href": "Caseweight Prospectus.html#modeling-client-related-work-with-machine-learning",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Modeling client-related work with machine learning",
    "text": "Modeling client-related work with machine learning\nIn planning our approach to modeling client-related work we hope to address several problems and limitations that emerged in the literature. The first, which dictated our chosen machine learning (ML) methodology, relates to the complexities that arise from modeling electronic health data (EMH) (Joseph et al., 2023). EMH data poses unique challenges such as hidden clustering, non-independence of observations and multi-collinearity which ML methods like random forests and gradient boosted forests can better manage; albeit with varying levels of explainability (An et al., 2023; Chen et al., 2023; Zeleke et al., 2023). Though there is often a trade-off between interpretability and increased predictive accuracy, modern post-hoc methods of determining features importance like SHapley Additive exPlanations (SHAP) offer an alternative way of understanding the contribution of inidividual features (varibles) to a specific prediction (Lundberg & Lee, n.d.).\n\nWhat is machine learning?\n\nMachine learning is a branch of artificial intelligence where computers are trained to perform tasks by identifying patterns within data instead of relying on explicitly programmed instructions. In mathematical terms, machine learning algorithms use statistical techniques to optimize a model’s parameters. This process involves minimizing a loss function that quantifies the difference between the model’s predictions and the actual data. For example, in supervised learning, the goal is to find a function F(x) that maps input features X to an output Y such that the predicted outcomes are as close as possible to the true outcome (Nielsen, 2016). This approach is highly effective for complex tasks like image recognition, natural language processing, and predictive analytics, where traditional rule-based programming is infeasible due to the high dimensionality and variability of the data.\n\nUnlike traditional methods that might only confirm linear relationships, ML algorithms are better equipped to identify high-dimensional interactions and non-obvious patterns. Techniques such as regularization and cross-validation further enhance the robustness of these models, maximizing their ability to generalize well to unseen data (Chen et al., 2023). Cross validation is a method used to evaluate how well a machine learning model will perform on new data. Instead of training a model on the entire dataset, data is divided into several parts or “folds”. The model is trained on some of the folds and tested on the remaining ones. This process is repeated multiple times, with each fold being used as a test set once. Depending on the model, the results are then averaged to give a more reliable estimate of the models performance and ensures that the model isn’t just performing well on a specific subset of the data. On the other hand, regularization is a technique used in ML that prevents overfitting which happens when a model performs well on training data but poorly on new, unseen data. Regularization helps by adding a penalty to a model’s complexity, essentially shrinking the influence or weight of potentially redundant parameters in the model (e.g. Lasso models).\nWe also intend to tackle the issue of inconsistent outcome measures. Previous studies often relied on flat proxy measures such as length of stay or number of appointments as a proxy for workload which doesn’t capture the variance in work intensity throughout the treatment period (CMHO, 2019; Martin et al., 2020). To address this, we will calculate a work measure on a per-diem basis, based on the total time logged in service of the client per week, which we hope will better reflect the intensity of work than a flat count (CMHO, 2019; Wang et al., 2021).\nAdditionally, we will address several limitations related to client-level indicators like gender and race which raise concerns about fairness and predictor bias. It is important to note that while race and gender may correlate with resource use, they may be confounded by systemic marginalization associated with an increased risk for mental health concerns (Gaines et al., 2003; Tran et al., 2019). To avoid perpetuating this marginalization, we will focus on variables that drive resource use directly such as age, diagnoses and symptom ratings, and exclude variables that indicate race, culture or ethnicity. Following Tran et al. (2019)’s recommendation to favor direct measures of client need, we plan to use interRAI screener+ items and scores specifically (Hirdes et al., 2020).\nFurthermore, to avoid challenges faced in prior research in disentangling the effects of client and provider-side drivers of work, we will focus solely on modeling client-side drivers as recommended by Tran et al. (2019) who suggest including provider-side variables (i.e., years of experience or preferred modality) risks reinforcing systemic unfairness in case distribution. Moreover, we want to predict workload to aid in decision making around client assignment, a point in time when provider related data is not yet available.\nFinally, adequate model evaluation was a significant area of weakness across the literature (Tran et al., 2019). Few studies employed robust cross-validation methods and even fewer tested their models on unseen data (Costa et al., 2015; Reid et al., 2021; Tran et al., 2019). To counter these limitations, we will utilize cross-validation folds within our training set for model training and tuning and will hold back a test set of unseen data for final model evaluation which will serve as a control for determining model accuracy. We will also split our data in a group-wise fashion, ensuring that clients with multiple rows who are in the training set will not be included in the test set. This will ensure that we have a more robust measure of how well our model will perform on “unseen” clients. We will also clearly outline our choice of models, metrics and all R code will be made available for reproducibility (see Methods)."
  },
  {
    "objectID": "Caseweight Prospectus.html#modelling-client-complexity",
    "href": "Caseweight Prospectus.html#modelling-client-complexity",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Modelling client complexity",
    "text": "Modelling client complexity\nAcross health domains (e.g., psychiatric, emergency medicine, community based mental health) various strategies have been employed to manage provider workload with different levels of service determined by characteristics like symptom severity or prior diagnoses (Johnson et al., 1998; Tran et al., 2019). These systems assume that though the needs of each individual in a population will be unique, there will be shared characteristics that determine the type of treatment they will need (e.g., family counselling versus substance use treatment). These groups represent the mix of cases or “case-mix” which can be viewed as a proxy for the types of care needs of the population. Case-mix classification systems are most often used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use (CMHO, 2019). At the agency level, the data within a case mix may contain the activity of individual providers such as the direct-hours attributable to individual clients within their caseloads and client-level characteristics like diagnoses, treatment history and current presenting symptoms (e.g., crisis intervention versus brief services). It may also contain demographics information like age, school district and the number of services offered. These data points are often stored in an electronic health care database and are captured using some kind of classification system."
  },
  {
    "objectID": "Caseweight Prospectus.html#a-measure-of-workload-direct-and-indirect-time",
    "href": "Caseweight Prospectus.html#a-measure-of-workload-direct-and-indirect-time",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "A measure of workload: direct and indirect time",
    "text": "A measure of workload: direct and indirect time\nAn important workload metric that was born out of Ontario’s CYMH AG audits and the recommendations that followed, was a policy change requiring all publicly funded CYMH service providers in Ontario to report the direct-time time spent in the service of clients (MOH CYMH Service Description Schedules - Central Forms Repository (CFR), n.d.). Direct-time is defined as the number of hours spent in face-to-face interactions, phone or video-based communications, and meetings with parents and caregivers, while indirect hours involve client-related tasks like documentation, travel time, and consultations. The sum of all direct and indirect hours logged by a clinician amounts to the overall “work” attributable to a given client. We will utilize direct and indirect hours to approximate the work attributed to an individual client in our models. However, it is important to stress that direct hours are only a proxy for client-related work, as hours alone may not convey differences in the emotional or mental effort required to treat different clients nor the stress associated with a case that may influence the perceived workload experienced by the provider and associated with any given client.\nOne Ontario agency began using direct and indirect hours to track and manage caseloads, is Compass, the lead child and youth mental health agency in the districts of Sudbury and Manitoulin and the proposed site of the current study. Compass utilizes an electronic dashboard to monitor caseloads by tracking the associated direct and indirect time logged. The dashboard has been useful for comparing caseloads between clinicians and across teams, but offers less insight into how much work is associated with each case. Moreover, it doesn’t help in assigning new cases beyond indicating which providers have higher or lower case counts than others. In response to calls from both clinicians and managers for a more thoughtful system for determining client assignment that considers not only counts, but the complexity of client need, the agency wondered whether psychological screening data collected at intake could be modeled to quantify client complexity by attributing a weight to each case that might aid in monitoring existing caseloads as well as inform new case assignment."
  },
  {
    "objectID": "Caseweight Prospectus.html#measuring-client-related-work",
    "href": "Caseweight Prospectus.html#measuring-client-related-work",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Measuring client-related work",
    "text": "Measuring client-related work\nGiven this state of affairs, if we had a way to predict the work associated with a given client in a without requiring dedicated staff to carefully evaluate each case before clinical assignment, agencies might more efficiently evaluate and manage provider caseloads. However modeling client characteristics and predicting the work that is driven by them, has proved difficult so far (CMHO, 2019; King, 2009; Tran et al., 2019). Despite initiatives such as the Quadruple Aim Framework meant to improve health outcomes, reduce costs, and improve provider work-life balance, the CYMH sector continues to face challenges in establishing a consistent, standardized measurement system, let alone a case assignment system that can accurately reflect the changing demands placed on staff (Arnetz et al., 2020; CMHO, 2019). Nevertheless, the new guidelines have informed policies meant to improve the tracking of client-related work which has enabled several agencies to expand efforts to understand and track workload in their own organization."
  },
  {
    "objectID": "Caseweight Prospectus.html#measuring-workload-direct-and-indirect-time",
    "href": "Caseweight Prospectus.html#measuring-workload-direct-and-indirect-time",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Measuring workload: direct and indirect time",
    "text": "Measuring workload: direct and indirect time\nAn important workload metric born out of Ontario’s CYMH AG audits and the recommendations that followed, is time (direct and indirect) that is spent with each client–a metric that all publicly funded CYMH service providers in Ontario are now required to report (Ministry of Health, 2024). Direct-time is defined as the number of hours spent in face-to-face interactions, phone or video-based communications, and meetings with parents and caregivers, while indirect hours involve client-related tasks like documentation, travel time, and consultations. The sum of all direct and indirect hours logged by a clinician amounts to the overall “work” attributable to a given client. In our models, we will utilize direct and indirect hours to approximate the work attributed to an individual client. It is important to note that time is only a proxy for client-related work as it doesn’t convey differences in the effort necessary to treat different clients, nor the stress associated with each case or even the workplace culture and supports that can influence the perceived workload experienced by the provider.\nOne Ontario agency that has begun using the new metric to track and manage caseloads, is Compass, the lead child and youth mental health agency in the districts of Sudbury and Manitoulin and the proposed site of the current study. Compass utilizes an electronic dashboard to monitor caseloads and the associated direct and indirect time logged. While the dashboard has been useful for comparing case counts between clinicians and teams, it offers less insight into the amount of work associated with each case. Moreover, it doesn’t help in assigning new cases beyond indicating which providers have higher or lower case counts than others. It does not allow administrators to evaluate the mix of cases (high to low needs) within each caseload, making it difficult to assign new clients in a way that is fair in terms of work. Based on requests from both leadership and clinicians themselves for an assignment tool that accounts for differences in the intensity of services needed, we began to wonder whether psychological screening data collected at intake might be modeled to quantify client complexity. If we could determine a weight for each case it might be easier to monitor existing caseloads as well as inform new case assignment.\n\nModelling client complexity\nAcross health domains (e.g., psychiatric, emergency medicine, community based mental health) various strategies have been employed to manage provider workload with different levels of service determined by characteristics like symptom severity or prior diagnoses (Johnson et al., 1998; Tran et al., 2019). These systems assume that though the needs of each individual in a population will be unique, there will be shared characteristics that determine the type of treatment they will need (e.g., family counselling versus substance use treatment). These groups represent the mix of cases or “case-mix” which can be viewed as a proxy for the types of care needs of the population. Case-mix classification systems are most often used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use (CMHO, 2019). At the agency level, the data within a case mix may contain the activity of individual providers such as the direct-hours attributable to individual clients within their caseloads and client-level characteristics like diagnoses, treatment history and current presenting symptoms (e.g., crisis intervention versus brief services). It may also contain demographics information like age, school district and the number of services offered. These data points are often stored in an electronic health care database and are captured using some kind of classification system."
  },
  {
    "objectID": "Caseweight Prospectus.html#themes-and-gaps-in-the-literature",
    "href": "Caseweight Prospectus.html#themes-and-gaps-in-the-literature",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Themes and gaps in the literature",
    "text": "Themes and gaps in the literature\nIn planning our own approach to modeling client-related work, we hope to address several problems and limitations that emerged in the literature. The first, which dictated our chosen machine learning (ML) methodology, relates to the complexities that arise from modeling electronic health data (EMH) where hidden clustering, non-independence of observations and multi-collinearity are all challenges that ML methods like random forests and gradient boosted forests can often better manage; albeit with varying levels of explainability (An et al., 2023; Chen et al., 2023; Zeleke et al., 2023). Though there is often a trade-off of interpretability with increased predictive accuracy, modern post-hoc methods like SHapley Additive exPlanations (SHAP) offer an alternative way of understanding the contribution of inidividual features to a specific prediction (Lundberg & Lee, 2017).\nWe also intend to tackle the lack of inconsistent outcome measures found in the literature. Previous studies often relied on flat measures such as length of stay or number of appointments as a proxy for workload which doesn’t capture the variance in work intensity throughout the treatment period (CMHO, 2019; Martin et al., 2020). To address this, we will calculate a work measure on a per-diem basis, based on the total time logged in service of the client per week, which we hope will better reflect the intensity of work than a flat count (CMHO, 2019; Wang et al., 2021).\nAdditionally, we intend to address concerns highlighted by Tran et al. (2019) where client-level indicators like gender and race are included in models and which raise concerns about fairness and predictor bias. Tran et al. (2019) importantly pointed out that while race and gender may correlate with resource use, the relationships are confounded by marginalization that may be drive increased risk for mental health concerns (Gaines et al., 2003; Tran et al., 2019). To avoid these pitfalls, we will focus solely on client-side drivers of work, specifically the mental health acuity features collected at intake, which have been shown to be more reliable drivers of resource use (Perlman et al., 2013; Tran et al., 2019). We plan to use interRAI screener+ items and scores specifically (Hirdes et al., 2020).\nFurthermore, we intend to focus solely on client-side drivers of work as recommended by Tran et al. (2019) who suggest including provider-side variables like years of experience or preferred therapeutic modality risks reinforcing systemic unfairness in case distribution where the more experienced clinicians may have the greater bulk of complex clients. Moreover, provider related information is not available in the client record at intake.\nFinally, to avoid considerable validation problems found across the literature, we will utilize cross-validation for model training and tuning and will hold back a test set of unseen data for final model evaluation. The test set we hold back will serve as a control for determining model generalizability (Costa et al., 2015; Reid et al., 2021; Tran et al., 2019). We will also split our data in a group-wise fashion, ensuring that clients with multiple rows will only be in either the training set or the test set, never both. This will ensure that we have a more robust measure of how well the final model will generalize to “unseen” clients. We will also clearly outline our choice of models, metrics and all R code will be made available for reproducibility (see Methods)."
  },
  {
    "objectID": "Caseweight Prospectus.html#predicting-client-related-work",
    "href": "Caseweight Prospectus.html#predicting-client-related-work",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Predicting client-related work",
    "text": "Predicting client-related work\nIn busy publicly funded CYMH agencies, manual review of a high volume of digital case files across hundreds of clients to make proactive care decisions is impractical, unsustainable and error-prone. Given this state of affairs, if there was a way to anticipate and monitor the work associated with a given case without requiring dedicated staff, agencies might more efficiently manage provider work. Research has already demonstrated the feasibility of predicting events associated with a wide range of healthcare problems, including hospital readmission and in-hospital death. However, the mental health literature is mostly limited to predicting specific types of events like risk of suicide, self-harm or onset of first psychosis–rather than predicting the overall resource needs associated with each case. Ultimately, much remains unknown about the feasibility of leveraging machine learning models to estimate work based on client-level factors and even a highly accurate predictive model would not guarantee improved mental health outcomes or long-term cost savings; therefore, it remains unclear whether new predictive technologies could provide tools that are useful to mental healthcare practitioners.\nTo date, modeling client characteristics and predicting the work that is driven by them, has proved difficult (CMHO, 2019; King, 2009; Tran et al., 2019).\n\nModelling client complexity\nAcross health domains (e.g., psychiatric, emergency medicine, community based mental health) various strategies have been employed to manage provider workload with different levels of service determined by characteristics like symptom severity or prior diagnoses (Johnson et al., 1998; Tran et al., 2019). These systems assume that though the needs of each individual in a population will be unique, there will be shared characteristics that determine the type of treatment they will need (e.g., family counselling versus substance use treatment). These groups represent the mix of cases or “case-mix” which can be viewed as a proxy for the types of care needs of the population. Case-mix classification systems are most often used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use (CMHO, 2019). At the agency level, the data within a case mix may contain the activity of individual providers such as the direct-hours attributable to individual clients within their caseloads and client-level characteristics like diagnoses, treatment history and current presenting symptoms (e.g., crisis intervention versus brief services). It may also contain demographics information like age, school district and the number of services offered. These data points are often stored in an electronic health care database and are captured using some kind of classification system."
  },
  {
    "objectID": "Caseweight Prospectus.html#a-novel-approach-to-modeling-case-mix",
    "href": "Caseweight Prospectus.html#a-novel-approach-to-modeling-case-mix",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "A novel approach to modeling case-mix",
    "text": "A novel approach to modeling case-mix\nConsidering several limitations outlined by prior research in modeling the high-dimensional, sparse data characteristic of digital client information systems, we next looked to a growing body of research that utilizes machine learning algorithms to model electronic health data.\nMachine learning (ML) is a branch of artificial intelligence that uses statistical techniques to enable computers to learn from data and make predictions without being explicitly programmed (Nielsen, 2016). ML algorithms are particularly well-suited to modeling the complex, high-dimensional data found in electronic health records (EHR) and have been used to predict a variety of outcomes in health care, from patient readmission to disease diagnosis (An et al., 2023; Chen et al., 2023).This capability makes ML particularly suitable for our purposes where the data is sparse, missing values, and has complex interdependencies among variables (An et al., 2023; Chen et al., 2023). Moreoever, ML, unlike traditional methods that might only confirm specific linear relationships, can identify high-dimensional interactions and non-obvious patterns. Techniques such as regularization and cross-validation further enhance the robustness of these models and maximizes their ability to generalize well to unseen data (Chen et al., 2023).Beyond prediction, ML also excels in discovering new relationships that might be validated in subsequent studies. By leveraging algorithms that optimize for predictive performance we may uncover patterns that could drive new hypotheses (Sheetal et al., 2023).\nA study that stood out to us examined the feasibility of utilizing ML algorithms to drive a visual representation of the work attributable to individual patients in a hospital setting (Benda et al., 2018). The idea was to build a live, dynamic visualization that could be used to compare cases and workloads across clinicians to improve patient assignment. The display was driven by an algorithm that predicted patient-level work based on a combination of diagnoses and the number of orders or “events” (e.g., tests, phone calls, diagnoses) found in a patient’s electronic health record (Benda et al., 2018). Although clinicians evaluated the tool positively, the algorithm underlying the display was found to inadequately account for actual workload, suggesting more refinements were needed (Benda et al., 2018).\nBuilding on Benda et al. (2018)’s study, Wang et al. (2021) focused on improving the dashboards underlying algorithm using several machine learning algorithms known for their robustness in modeling the sparse, heterogeneous data found in electronic health records. Both regression and classification algorithms were used to model several proxies for workload: i) overall length of stay, ii) number of events (e.g., tests ordered, medication administered), iii) density of events (count of events divided by length of stay) and iv) a binary outcome indicating high versus low demand patients. The accuracy of the model in predicting low versus high length of stay (LOS) was 70% with information from the first hour, 73% from the first two hours and 83% with data from the entire visit. Importantly, Wang et al. (2021) ’s work demonstrated the potential for machine learning techniques to predict client-related work from information collected at intake–which is what we are interested in doing and so resulted in the methodology we chose."
  },
  {
    "objectID": "Caseweight Prospectus.html#case-mix-classification-systems",
    "href": "Caseweight Prospectus.html#case-mix-classification-systems",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Case-Mix Classification Systems",
    "text": "Case-Mix Classification Systems\nCase-mix classification systems usually take one of two approaches to evaluating client-related work. Grouping systems assign people into classes in terms of their expected resource use, with each group having a specific weight attached to it (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population. Index systems on the other hand, combine different characteristics of a case to provide a continuous, numerical value which maps to expected resource use relative to an average case (Tran et al., 2019).\nWhile case-mix systems are widely used in the medical domain, to date, most mental health case-mix classification systems focus on acute care in hospital or other inpatient settings which are distinctly different than community based care in several ways (Tran et al., 2019). Typically, community based care involves a team of providers offering a wider range of services. For instance, urgent care, crisis intervention, brief services or longer-term treatment like counselling and therapy as well as group programs and day treatment provided in educational settings. Often these services are provided without clear diagnoses or well-defined treatment protocols, making them more complicated to model. For instance, a medical emergency will often come with a clear diagnoses like “broken arm” which has a recovery window that is easier to estimate. On the other hand, the subjective experience of anxiety or depression as well as the time it will take to recover is far less black and white.\nThe difficulty of modeling CYMH data is illustrated by the fact that only a handful of studies have looked at solving this problem, despite urgent need. Indeed, a 2019 scoping review of casemix literature in community-based mental health care domains found only a single case that looked at case-mix classification to predict mental health care resource in the CYMH domain (Martin et al., 2020; Tran et al., 2019). In that study, researchers modeled 4573 client records from eleven UK outpatient CYMH agencies, comparing cluster analysis, regression trees and a conceptual classification based on clinical best practice guidelines to accurately predict the number of appointments a client attended in treatment (Martin et al., 2020). While the researchers found the conceptual classification was as clinically meaningful as data-driven classification in accounting for number of appointments, they found little evidence to support the idea that either client complexity or context factors (with the exception of school attendance problems) were linked with overall appointment counts (Martin et al., 2020). Moveover, the models failed to explain significant variation in resource provision between workers despite clients exhibiting similar characteristics. Data quality problems and omission of important individual-level factors were cited as potential points of failure but suggested their results merited further testing and development.\nAnother group of researchers tried to predict the workload associated with clients at a community-based mental health centre for the elderly (Baillon et al., 2009). Using an 8-item case weighting scale (CWS) researchers identified factors staff felt contributed to demand for staff time. A multiple regression model was used to assign different weightings to each item based on the strength of its relationship with the outcome (an estimation of time spent on each client logged over a four-week period). The model was then used to predict the total time a client would utilize in a four-week period following the first appointment. Though the model was reported a success, accounting for 58% of the variance in time spent on client-related work, the sample consisted of only 87 cases and relied on a statistical method unsuitable for evaluating agreement between model predictions and actual observations, leaving it unclear how accurate the model actually was (Baillon et al., 2009; Mansournia et al., 2021). Moreover, inter-rater and re-rater reliability results indicated that the assessment, whether from a client’s self-report or a professional’s clinical opinion, did not necessarily relate to the amount of time needed by clients (Baillon et al., 2009).\nNext we looked to other domains where we found several measures of workload intensity had been developed to manage caseloads. For example, in general psychiatry, emergency medicine and nursing, researchers have used factors like sociodemographics, functional ability, and caregiver and social network characteristics to predict service utilization. One approach that Tran et al. (2019) suggested may be a good candidate for testing in community settings is a grouping methodology that sorts patients according to clinical characteristics obtained from standardized interRAI assessment data to estimate resource use (Hirdes et al., 2020; Perlman et al., 2013).\n\ninterRAI Child and Youth Mental Health Assessment Tool\ninterRAI is an international research network that develops clinical standards across a variety of health and social service settings that have developed a large toolkit of instruments used by health organizations worldwide to assess people at the point of care across a variety of domains including emergency medicine, emergency psychiatry and children and youth mental health. The collected data are meant to be used at the agency level for quality improvement activities, benchmarking, program planning and resource planning and at the system level to compare health data across regions and provinces (Hirdes et al., 2002). In Canada, interRAI is partnered with the Canadian Institute for Health Information (CIHI) who act as a custodian of interRAI standards and houses and monitors the collected interRAI data. Many interRAI instruments are used across Canada and internationally, but the Children and Youth Mental Health assessment (ChYMH) represents the first assessment designed specifically for children and youth (Stewart & Hamza, 2017).\nIn Ontario, two instruments are most often used in the CYMH sector: the Child and Youth Mental Health Screener+ (ChYMH-S) and the more comprehensive full ChYMH and its variants. The primary use of the CHYM-S is to support decision making related to triaging, placement, and service utilization while the full ChYMH and its associated Collaborative Action Plans (CAPs) are meant to assess, respond to and monitor the strengths, preferences and mental health needs of clients in in-patient and out-patient treatment. The ChYMH products are currently being utilized in over 60 mental health agencies across Ontario including Compass.\nThe full ChYMH includes over 400 items that together are meant to build a comprehensive picture of a client’s strengths, needs, functioning and areas of risk (Stewart et al., 2022), while the ChYMH-S is comprised of 106 items and is intended as a brief screener to identify young people who are in need of more comprehensive assessment. The screener is administered via a semi-structured interview to children and youth between 4-18 years in a variety of settings and is intended to take 15-20 minutes to complete. The current study will utilize screener data collected at intake to model client-related complexity and resulting work.\nImportantly, the ChYMH scales have demonstrated good predictive validity. For example, data from over 5000 children and youth placed in psychiatric settings in Ontario found that the Agressive Behaviour Scale was predictive of multiple control interventions, while the Severity of Self Harm Scale (SOS) was useful in predicting admission for risk of self-harm in youth between 10-17 years. In addition, individuals who score higher on scales like the Hyperactive/Distraction scale were more likely to have a provisional diagnosis of ADHD (Stewart et al., 2022).\nThough the various scales and items of the ChyMH demonstrate some predictive utility, it remains unclear how well these items might predict the actual work required to serve a given client. Within the CYMH domain we found one example of interRAI data being used to develop an algorithm to predict resource cost for children and youth with developmental disabilities with a cluster analysis (Stewart et al., 2020). Though the resulting Child and Youth Resource Index (ChYRI) could only explain 30% of the variance in per diem costs for community-based services (Stewart et al., 2020), the algorithm was nonetheless deemed a success and is still in use today. However, a lack of explanation of how the analysis was conducted, as well as public availability of resulting fit statistics, makes it unclear how and where the algorithm could be improved.\nMoreover, unlike the goal of the ChYRI, we are not interested in predicting client-related service cost. Instead our efforts are driven by the need to reduce wait times and make client assignment fairer and mindful of the work that is already on each clinicians plate.\n\n\nMachine learning, a novel approach to modeling case-mix\nConsidering several limitations outlined by prior research in modeling the high-dimensional, sparse data characteristic of digital client information systems, we next looked to a growing body of research that utilizes machine learning algorithms to model electronic health data.\nMachine learning (ML) is a branch of artificial intelligence that uses statistical techniques to enable computers to learn from data and make predictions without being explicitly programmed (Nielsen, 2016). ML algorithms are particularly well-suited to modeling the complex, high-dimensional data found in electronic health records (EHR) and have been used to predict a variety of outcomes in health care, from patient readmission to disease diagnosis (An et al., 2023; Chen et al., 2023).This capability makes ML particularly suitable for our purposes where the data is sparse, missing values, and has complex interdependencies among variables (An et al., 2023; Chen et al., 2023). Moreoever, ML, unlike traditional methods that might only confirm specific linear relationships, can identify high-dimensional interactions and non-obvious patterns. Techniques such as regularization and cross-validation further enhance the robustness of these models and maximizes their ability to generalize well to unseen data (Chen et al., 2023).Beyond prediction, ML also excels in discovering new relationships that might be validated in subsequent studies. By leveraging algorithms that optimize for predictive performance we may uncover patterns that could drive new hypotheses (Sheetal et al., 2023).\nA study that stood out to us examined the feasibility of utilizing ML algorithms to drive a visual representation of the work attributable to individual patients in a hospital setting (Benda et al., 2018). The idea was to build a live, dynamic visualization that could be used to compare cases and workloads across clinicians to improve patient assignment. The display was driven by an algorithm that predicted patient-level work based on a combination of diagnoses and the number of orders or “events” (e.g., tests, phone calls, diagnoses) found in a patient’s electronic health record (Benda et al., 2018). Although clinicians evaluated the tool positively, the algorithm underlying the display was found to inadequately account for actual workload, suggesting more refinements were needed (Benda et al., 2018).\nBuilding on Benda et al. (2018)’s study, Wang et al. (2021) focused on improving the dashboards underlying algorithm using several machine learning algorithms known for their robustness in modeling the sparse, heterogeneous data found in electronic health records. Both regression and classification algorithms were used to model several proxies for workload: i) overall length of stay, ii) number of events (e.g., tests ordered, medication administered), iii) density of events (count of events divided by length of stay) and iv) a binary outcome indicating high versus low demand patients. The accuracy of the model in predicting low versus high length of stay (LOS) was 70% with information from the first hour, 73% from the first two hours and 83% with data from the entire visit. Importantly, Wang et al. (2021) ’s work demonstrated the potential for machine learning techniques to predict client-related work from information collected at intake–which is what we are interested in doing and so resulted in the methodology we chose."
  },
  {
    "objectID": "Caseweight Prospectus.html#machine-learning-a-novel-approach-to-modeling-case-mix",
    "href": "Caseweight Prospectus.html#machine-learning-a-novel-approach-to-modeling-case-mix",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Machine learning, a novel approach to modeling case-mix",
    "text": "Machine learning, a novel approach to modeling case-mix\nConsidering several limitations outlined by prior research in modeling the high-dimensional, sparse data characteristic of digital client information systems, we next looked to a growing body of research that utilizes machine learning algorithms to model electronic health data.\nMachine learning (ML) is a branch of artificial intelligence that uses statistical techniques to enable computers to learn from data and make predictions without being explicitly programmed (Nielsen, 2016). ML algorithms are particularly well-suited to modeling the complex, high-dimensional data found in electronic health records (EHR) and have been used to predict a variety of outcomes in health care, from patient readmission to disease diagnosis (An et al., 2023; Chen et al., 2023).This capability makes ML particularly suitable for our purposes where the data is sparse, missing values, and has complex interdependencies among variables (An et al., 2023; Chen et al., 2023). Moreoever, ML, unlike traditional methods that might only confirm specific linear relationships, can identify high-dimensional interactions and non-obvious patterns. Techniques such as regularization and cross-validation further enhance the robustness of these models and maximizes their ability to generalize well to unseen data (Chen et al., 2023).Beyond prediction, ML also excels in discovering new relationships that might be validated in subsequent studies. By leveraging algorithms that optimize for predictive performance we may uncover patterns that could drive new hypotheses (Sheetal et al., 2023).\nA study that stood out to us examined the feasibility of utilizing ML algorithms to drive a visual representation of the work attributable to individual patients in a hospital setting (Benda et al., 2018). The idea was to build a live, dynamic visualization that could be used to compare cases and workloads across clinicians to improve patient assignment. The display was driven by an algorithm that predicted patient-level work based on a combination of diagnoses and the number of orders or “events” (e.g., tests, phone calls, diagnoses) found in a patient’s electronic health record (Benda et al., 2018). Although clinicians evaluated the tool positively, the algorithm underlying the display was found to inadequately account for actual workload, suggesting more refinements were needed (Benda et al., 2018).\nBuilding on Benda et al. (2018)’s study, Wang et al. (2021) focused on improving the dashboards underlying algorithm using several machine learning algorithms known for their robustness in modeling the sparse, heterogeneous data found in electronic health records. Both regression and classification algorithms were used to model several proxies for workload: i) overall length of stay, ii) number of events (e.g., tests ordered, medication administered), iii) density of events (count of events divided by length of stay) and iv) a binary outcome indicating high versus low demand patients. The accuracy of the model in predicting low versus high length of stay (LOS) was 70% with information from the first hour, 73% from the first two hours and 83% with data from the entire visit. Importantly, Wang et al. (2021) ’s work demonstrated the potential for machine learning techniques to predict client-related work from information collected at intake–which is what we are interested in doing and so resulted in the methodology we chose."
  },
  {
    "objectID": "Caseweight Prospectus.html#dataset",
    "href": "Caseweight Prospectus.html#dataset",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Dataset",
    "text": "Dataset\nThe de-identified data will include approximately 6000 records containing demographics information, referrals, diagnoses, risk and well-being assessments and crisis events for all outpatients. Cases outside of the ages of 5 and 17 years will be excluded. Intake assessments without subsequent treatment will be excluded from the dataset as detecting events that correspond to increased resource use correspond to different ground truths. There are no plans to exclude cases based on any other feature, including diagnoses, however if this changes for whatever reason they will be outlined in the documentation. For the remaining patients, predictions were queried and evaluated for the period following the first assessment before querying the model."
  },
  {
    "objectID": "Caseweight Prospectus.html#features-and-target-generation",
    "href": "Caseweight Prospectus.html#features-and-target-generation",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Features and target generation",
    "text": "Features and target generation\nWith the exception of static information like presenting concern or referral source, all EMHR data will include the associated date and time. The date and time refer to the moment when the specific event or assessment occurred—that is, the date and time that a client contacted compass or completed an assessment. To prepare the data for the modeling task, each client’s case records will be consolidated at a weekly level according to the date associated with the record. Following this process, we will generate evenly spaced time series for each client spanning from their first interaction with Compass to the study’s final week. The features and labels generated for each week will be computed using the data from dates prior to that week. Static data that is susceptible to change over time (for example, postal code or school board information) will be removed to mitigate the risk of retrospective leakage.\nLabel generation. To construct a continuous case-weight prediction target, the sum of client-related direct and indirect hours logged by clinicians and associated with a specific program and client, will be aggregated at the same weekly level as the features, based on the time recorded by the worker prior to that week. We also intend to examine which measure of client-related time is most stable and reliable over time both direct and indirect time or either on its own.\nFeatures generation. We will extract features from a total possible feature set of ___ features. Informed by Garriga et al. (2022), extraction will be performed according to six procedures:\n\nStatic or semi-static features. demographics data will be represented as constant values attributed to each se with age treated as a special case that changes each year.\nDiagnosis feature. Client will be assigned their latest valid diagnosed disorder or developmental disability or an ‘un-diagnosed’ label and then seperated into diagnostic groups according to the latest valid diagnosed disorder at the last week of the training set to avoid leakage into the validation and test sets. Any codes created will be added to the final paper.\nEMHR weekly aggregations. Records related to client-agency interactions will be aggregated on a weekly basis for each client. The resulting features will constitute counts per type of interaction, one-hot encoded according to their categorization. If a specific type of event did not occur in a given week a value of 0 will be assigned to the feature related to the corresponding type of event for the corresponding week.\nTime-elapsed features. At each client-week, for each type of interaction and category, we will construct a feature that counts the number of elapsed weeks since the last occurence of the corresponding event. If the client never experienced such an event type up to that point in time, an NA value will be used.\nLast crisis episode descriptors. For each crisis episode, a set of descriptors will be used to build feature for the subsequent weeks until the next crisis events occurs. If the client never had a crisis event up until that point in time, NA values will be used.\nLast assessment descriptors. For each assessment item, a set of descriptors will be used to build a feature for the subsequent weeks until the next assessment occurs. All clients will have at least one assessment to be included in the study.\nStatus features. For specific records, characterized by a start and end date, features for the corresponding weeks will be built by assigning their corresponding value (or category); otherwise they are set to Na.\n\nIn addition to record-based features, we will also add the week number (of a year 1-52) to account for seasonality effects.\nCaseweight prediction modeling and evaluation. The caseweight prediction task will be defined as a continuous regression problem to be performed on a weekly basis. For each week, the model will predict the weekly hours needed during the upcoming 28 days. Applying a rolling window approach will allow for a periodic update of the caseweight by incorporating newly available data (or the absence of it) at the beginning of each week. The approach is common in settings where predictions are to be used in real time and when data are continuously updated. In addition we will define a classification task for each client, predicting low, moderate or high resource use the following week.\n\nData Splitting\nTo maximize the generalizability of our models, we will apply a time-based 80/10/10 training/validation/test split:\nTraining data will start in the first week of January 2019 and will end the last week of June 2022. Validation data will start in the first week of January 2023 and end in the last week of December 2023. Test data will start in the first week of January 2024 and end in the last week of December 2024.\n10-fold, timed based cross-validation will be used to tune the models. The cross-validation folds will be created with a portion of the training data, subdividing it into 10 subsets, or “folds,” that will preserve the same time structure.\nPerformance evaluations will be conducted on a weekly basis and each week’s results will be used to build CIs on the evaluated metrics. All reported results will be computed using the test set if not otherwise indicated. (Figure 3).\nThe test set will act as a control group to evaluate the models’ performance on “unseen data” at the very end of the training process. By keeping the test set separate and untouched during training, we ensure that our final evaluation provides a better estimation of how the models will perform in practice. This final step is crucial for assessing the models’ generalizability and for identifying any overfitting that may have occurred during training.\n\nIndependent Variables (features/indicators of work)\nImportantly, feature engineering–the creation of new predictors based on existing variables in the dataset–will occur after data splitting on the training data to minimize the risk of data leakage that could inadvertently occur when creating new variables from the full dataset (Sheetal, Jiang, and Di Milia 2023).\nThe main criterion for inclusion in the model will be the variable’s availability in the electronic health record (EHR) system at intake. Features (variables) will include items and scales from the interRAI ChYMH Screener+. The interRAI ChYMH is a clinician-rated tool that is completed based on a semi-structured clinical interview and includes several scale scores, administrative items as well as other demographics information (Shannon L. Stewart and Hamza 2017).\nThe ChYMH-S includes 34 administrative and tracking items, 26 mental health indicators, 5 substance use indicators, 9 questions related to harm to self and others, 6 behaviour focused items, 1 cognitive item as well as items that look to track social relations, anxiety levels, medications, living arrangements, diagnoses, physical conditions, past interventions and current and past strengths and resilience (Shannon L. Stewart et al. 2022). The Depressive Severity Index, Anxiety Scale, Disruptive/Aggressive Behaviour Scale, Hyperactive/Distraction Scale and Internalizing/Externalizing scales are all included in the ChYMH-S. The instrument and the various scales and items it contains have been tested extensively in research worldwide, demonstrating reliable face, content, construct and predictive validity (Shannon L. Stewart and Hamza 2017; Shannon L. Stewart et al. 2022).\nA full list of all variables will be included in the final report.\n\n\n\n\n\n\nFigure 1: Clinical pathway\n\n\n\n\n\nTargets (dependent variables or workload proxies)\nInformed by Wang et al. (2021), we propose to measure the influence of our workload indicators on i) length of service (weeks), ii) hours of time spent directly with the client or caregivers either in person on the phone or video call (direct time), iii) hours spent indirectly on client-related work like writing a treatment plan or filling out case notes (indirect time), iii) combined direct and iv) a case density score (caseweight) calculated by dividing the sum of direct and indirect time divided by the number of weeks the client spent in service (see Equation 1). See Figure 2 for a visualization of the relationship between variables.\n\\[\nCaseweight = \\frac{Hours Spent}{Weeks In Service}\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 2: Modeling Caseweight–client-related work\n\n\n\nADD TABLE OF WORKLOAD PROXIES\nOutcome proxies will be modeled as a i) continuous index scores or a ii) classification grouping (low versus high). In practice, regression could be used to predict the number of hours a client might need per week across the episode of care, while the classification model will provide a quick flag for cases that are more high intensity than others.\nAs a first step, we will train models on data collected over the entire program length to determine whether the indicators have any utility in modelling the workload proxies (some clients will have multiple assessments across the treatment period). Next, we will train on data only from the first assessment to predict workload captured at follow up assessment (typically 3 months later). This will allow us to test whether a workload prediction in the earliest stages of a visit is feasible. We believe this will be a more difficult task to accurately model, as such, a simple binary prediction of low versus high intensity may be easier for the models to accurately predict than a continuous index score.\n\n\n\n\n\n\nFigure 3: Experimental procedure\n\n\n\n\n\n\nModel Selection\nWe plan to utilize the following supervised machine-learning algorithms for both regression and classification problems: i) Random Forest (RF) for its ability to handle large datasets with high dimensionality; ii) XGBoost, known for its high performance and predictive accuracy on tabular datasets, iii) LASSO and Ridge regression for their ability to manage high multicollinearity, and finally iv) linear regression to serve as a baseline model for continuous outcomes and generalized logistic regression for binary outcomes. All algorithms will be trained on the same training set and cross validation folds and evaluated on the same test set using the TidyModels suite of packages in R Studio. These algorithms were chosen based on their success modeling similarly complex, tabular data types and may grow to include other models in the final paper (Salditt, Humberg, and Nestler 2023; Sheetal, Jiang, and Di Milia 2023).\n\n\nValidation and Testing\nFinal models will be statistically compared and evaluated on the test set using the following performance metrics: i) Mean Absolute Error (MAE), and ii) Root Mean Squared Error (RMSE) for continues outcomes. For categorical outcomes, we will rely on accuracy and area under the curve (AUC). These evaluations will help determine the accuracy, generalizability and robustness of each model (Salditt, Humberg, and Nestler 2023; Wang et al. 2021) Final models will also be analyzed to identify which predictors were the most significant predictors of client-related workload using SHAP scores.\n\n\nSoftware and Tools\nWe will use R Statistical Software and the Tidyverse and TidyModels suite of packages for data manipulation and model building (R Core Team, 2024; Khun & Wickham 2020). This choice aligns with our familiarity with R and the study’s specific requirements. R Quarto Markdown will be used for documentation and reproducibility. During the model building process, there is a chance we may use Python as well in the RStudio environment and will report and document this choice thoroughly if we do (Van Rossum and Drake 1995)."
  },
  {
    "objectID": "Caseweight Prospectus.html#literatur",
    "href": "Caseweight Prospectus.html#literatur",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Literatur",
    "text": "Literatur\nTo date, the main strategy employed to determine provider workload based on client characteristics in CYMH relies on case-mix methodology borrowed from the medical domain (Johnson et al., 1998; Tran et al., 2019). Case-mix classification systems are used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use. Casemix algorithms assume that though the needs of an individual will be unique, there are shared characteristics that determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically these systems are informed by information contained in case records. At the agency level, case records contain a variety of information, including provider-level information such as the number of direct and indirect-hours attributable to individual clients, as well as client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services).\nCase-mix algorithms usually take one of two approaches to classification based on the expected work associated with features contained a patient’s health record (CMHO, 2019). Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight attached to it (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population. Often agencies rely on rudimentary case-mix algorithms to classify cases based on simple factors like age or program accessed. For example, a client accessing long-term counselling and therapy services may have a different weight attached in terms of expected resource use compared to a client accessing a one-session brief service. Index systems on the other hand, combine different case characteristics to provide a continuous, numerical value which maps to expected resource use (Tran et al., 2019).\nWhile case-mix systems are widely used in the medical domain, to date, most mental health case-mix classification systems focus on acute care in hospital or other inpatient settings which are distinctly different than community based care in several ways (Tran et al., 2019). Typically, community based care offers a wider range of services. From brief services to longer-term treatment like counselling and therapy as well as group programs, school-based treatment as well as crisis intervention offered in partnership with local hospitals. Unlike many inpatient settings, services provided in community settings often lack clear diagnoses and recovery timelines making them more complicated to model. For instance, a medical emergency like a broken arm has a predictable recovery window, treatment protocol and thereby cost associated, unlike the subjective experience of anxiety or depression where the time and effort needed to recover is less black and white.\nThe difficulty of modeling EMHR in the CYMH domain, is illustrated by the fact that only a handful of studies have looked at solving this problem, despite urgent need. Indeed, a 2019 scoping review of casemix literature in community-based mental health care domains found only a single case that looked at case-mix classification to predict mental health care resource (Martin et al., 2020; Tran et al., 2019). In that study, researchers modeled 4573 client records from eleven UK outpatient CYMH agencies, comparing cluster analysis, regression trees and a conceptual classification based on clinical best practice guidelines to predict the number of appointments a client attended in treatment (Martin et al., 2020). The researchers found that data-driven classification was no more clinically meaningful than conceptual classification in accounting for number of appointments; moreover, there was little evidence to support the idea that either client complexity or context factors (with the exception of school attendance problems) were linked to overall appointment counts (Martin et al., 2020). Moveover, the models failed to explain significant variation in resource provision between workers despite clients exhibiting similar characteristics. Data quality problems and omission of important individual-level factors were cited as potential points of failure but suggesting their results merited further testing and development.\nIn a related cohort, a group of researchers tried to predict the work associated with client features at a community-based mental health centre for the elderly (Baillon et al., 2009). Using an 8-item self-designed case weighting scale (CWS) researchers identified factors staff felt contributed to demand for time. A multiple regression model was used to assign different weightings to predictors based on the strength of its relationship with the outcome (an estimation of time spent on each client logged over a four-week period). The resulting coefficients were then added to a spreadsheet and used to predict the total time a client would utilize in a four-week period following the first appointment based on the 8 characteristics. Though the model was reported a success, accounting for 58% of the variance in time spent on client-related work, the sample consisted of only 87 cases leaving it unclear how accurate the model really was (Baillon et al., 2009; Mansournia et al., 2021). Moreover, inter-rater and re-rater reliability results indicated that the assessment, whether from a client’s self-report or a professional’s clinical opinion, did not necessarily relate to the amount of time needed by clients (Baillon et al., 2009).\n\nMachine learning, a novel approach to modeling case-mix\nConsidering the challenges outlined by prior research in modeling the high-dimensional, sparse data characteristic of EMHRs, we next looked to a growing body of research leveraging machine learning algorithms to model electronic health data. Machine learning (ML) is a branch of artificial intelligence that uses statistical techniques that enable computers to learn patterns from data to make accurate predictions (Nielsen, 2016). ML algorithms are particularly well-suited to modeling the complex, high-dimensional data found in EMHRs (An et al., 2023; Chen et al., 2023). Unlike traditional statistical methods that aim to confirm specific linear relationships, ML algorithms independently identify high-dimensional, non-linear patterns that provide the best predictions. In addition, ML methodologies contain many advanced techniques like regularization and cross-validation which can be used to optimize a model’s ability to generalize well to unseen data (Chen et al., 2023), making this approach particularly suitable for our purposes (An et al., 2023; Chen et al., 2023).\nWithin the inpatient mental health domain, machine learning has mostly been used to predict specific future events like substance relapse, self-harm and suicide risk. However a recent study leveraged ML to build a model that continuously monitors patient records to predict crisis-relapse over a 28 day period (Garriga et al., 2022). The winning XGBoost regression demonstrated good accuracy in distinguishing between cases who were likely and unlikely to experience a crisis in the next 28 days. Specifically, the model could correctly differentiate those at risk from those not at risk about 80% of the time. Morever, in post-hoc case study, healthcare professionals rated the predictions valuable for managing patient care in 64% of cases, helping them to prioritize patients more effectively and potentially prevent crises (Garriga et al., 2022). Though the author’s did not model the work directly as we hope to do, ‘crisis-risk’ served as a proxy for increased resource-demand which they hoped would help better anticipate demand to manage caseloads more efficiently.\nWith Garriga et al. (2022) ’s study in mind, the current research proposes to explore the feasability of predicting the work associated with a given case at different points along the client timeline. The assumption underlying the research, is that there are historical patterns that predict future mental health resource use and that such patterns can be identified in electronic mental health records (EMHR), despite its sparseness, noise, errors and systematic bias. To test our assumptions, we will utilize a retrospective, deidentified dataset containing a cohort of youth and families that utilized CYMH services between 2019 and the end of 2023 in Ontario to predict client-related work.\nThough the current study will be largely exploratory in nature, the work will be guided by several hypotheses. First, given past research we suspect that predictions will be weakest at the earliest stages of the client journey when information in the EMHR is limited to only an intake screener and basic demographic information, however, as information in the EMHR increases over time, we anticipate accuracy will increase. Furthermore, we anticipate that the best predictor of work for unseen clients will be mental health acuity features known to be linked to resource-use such as externalizing behaviours and prior self-harm or suicide attempts, but anticipate the best predictor of work for known clients will be time-based features such as time since first agency contact or time since last assessment. This hypothesis stems from existing literature indicating that these variables are critical drivers of resource use (Perlman et al., 2013; Tran et al., 2019)."
  },
  {
    "objectID": "Caseweight Prospectus.html#literature-review",
    "href": "Caseweight Prospectus.html#literature-review",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Literature Review",
    "text": "Literature Review\nTo date, the main strategy employed to determine provider workload based on client characteristics in CYMH relies on case-mix methodology borrowed from the medical domain (Johnson et al., 1998; Tran et al., 2019). Case-mix classification systems are used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use. Casemix algorithms assume that though the needs of an individual will be unique, there are shared characteristics that determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically these systems are informed by information contained in case records. At the agency level, case records contain a variety of information, including provider-level information such as the number of direct and indirect-hours attributable to individual clients, as well as client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services).\nCase-mix algorithms usually take one of two approaches to classification based on the expected work associated with features contained a patient’s health record (CMHO, 2019). Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight attached to it (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population. Often agencies rely on rudimentary case-mix algorithms to classify cases based on simple factors like age or program accessed. For example, a client accessing long-term counselling and therapy services may have a different weight attached in terms of expected resource use compared to a client accessing a one-session brief service. Index systems on the other hand, combine different case characteristics to provide a continuous, numerical value which maps to expected resource use (Tran et al., 2019).\nWhile case-mix systems are widely used in the medical domain, to date, most mental health case-mix classification systems focus on acute care in hospital or other inpatient settings which are distinctly different than community based care in several ways (Tran et al., 2019). Typically, community based care offers a wider range of services. From brief services to longer-term treatment like counselling and therapy as well as group programs, school-based treatment as well as crisis intervention offered in partnership with local hospitals. Unlike many inpatient settings, services provided in community settings often lack clear diagnoses and recovery timelines making them more complicated to model. For instance, a medical emergency like a broken arm has a predictable recovery window, treatment protocol and thereby cost associated, unlike the subjective experience of anxiety or depression where the time and effort needed to recover is less black and white.\nThe difficulty of modeling EMHR in the CYMH domain, is illustrated by the fact that only a handful of studies have looked at solving this problem, despite urgent need. Indeed, a 2019 scoping review of casemix literature in community-based mental health care domains found only a single case that looked at case-mix classification to predict mental health care resource (Martin et al., 2020; Tran et al., 2019). In that study, researchers modeled 4573 client records from eleven UK outpatient CYMH agencies, comparing cluster analysis, regression trees and a conceptual classification based on clinical best practice guidelines to predict the number of appointments a client attended in treatment (Martin et al., 2020). The researchers found that data-driven classification was no more clinically meaningful than conceptual classification in accounting for number of appointments; moreover, there was little evidence to support the idea that either client complexity or context factors (with the exception of school attendance problems) were linked to overall appointment counts (Martin et al., 2020). Moveover, the models failed to explain significant variation in resource provision between workers despite clients exhibiting similar characteristics. Data quality problems and omission of important individual-level factors were cited as potential points of failure but suggesting their results merited further testing and development.\nIn a related cohort, a group of researchers tried to predict the work associated with client features at a community-based mental health centre for the elderly (Baillon et al., 2009). Using an 8-item self-designed case weighting scale (CWS) researchers identified factors staff felt contributed to demand for time. A multiple regression model was used to assign different weightings to predictors based on the strength of its relationship with the outcome (an estimation of time spent on each client logged over a four-week period). The resulting coefficients were then added to a spreadsheet and used to predict the total time a client would utilize in a four-week period following the first appointment based on the 8 characteristics. Though the model was reported a success, accounting for 58% of the variance in time spent on client-related work, the sample consisted of only 87 cases leaving it unclear how accurate the model really was (Baillon et al., 2009; Mansournia et al., 2021). Moreover, inter-rater and re-rater reliability results indicated that the assessment, whether from a client’s self-report or a professional’s clinical opinion, did not necessarily relate to the amount of time needed by clients (Baillon et al., 2009).\n\nMachine learning, a novel approach to modeling case-mix\nConsidering the challenges outlined by prior research in modeling the high-dimensional, sparse data characteristic of EMHRs, we next looked to a growing body of research leveraging machine learning algorithms to model electronic health data. Machine learning (ML) is a branch of artificial intelligence that uses statistical techniques that enable computers to learn patterns from data to make accurate predictions (Nielsen, 2016). ML algorithms are particularly well-suited to modeling the complex, high-dimensional data found in EMHRs (An et al., 2023; Chen et al., 2023). Unlike traditional statistical methods that aim to confirm specific linear relationships, ML algorithms independently identify high-dimensional, non-linear patterns for the best predictions. ML strategies further contain many advanced techniques like regularization and cross-validation which can be used to optimize a model’s ability to generalize to unseen data (Chen et al., 2023), making this approach particularly suitable for our purposes (An et al., 2023; Chen et al., 2023).\nWithin the inpatient mental health domain, machine learning has mostly been used to predict specific events like substance relapse, self-harm and suicide risk. However a recent study leveraged ML to build a model that continuously monitors patient records to predict crisis-relapse over a 28 day period (Garriga et al., 2022). The winning XGBoost regression demonstrated good accuracy in distinguishing between cases who were likely and unlikely to experience a crisis in the next 28 days. Specifically, the model could correctly differentiate those at risk from those not at risk about 80% of the time (Garriga et al., 2022). Morever, in a subsequent post-hoc case study, healthcare professionals rated the predictions produced by the model valuable for managing patient care in 64% of cases, helping them to prioritize patients more effectively and potentially prevent crises (Garriga et al., 2022). Though the author’s did not model the resource use directly as we hope to do, ‘crisis-risk’ served as a proxy for work. By predicting crises, they could anticipate the increased resource-demand which they hoped could inform better case prioritization and managment."
  },
  {
    "objectID": "Caseweight Prospectus.html#the-current-study",
    "href": "Caseweight Prospectus.html#the-current-study",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "The current study",
    "text": "The current study\nWith Garriga et al. (2022) ’s study in mind, the current research proposes to explore the feasability of predicting the work associated with a given case in 28 day intervals. The assumption underlying the research, is that there are historical patterns that predict future mental health resource use and that such patterns can be identified in electronic mental health records (EMHR), despite its sparseness, noise, errors and systematic bias. To test our assumptions, we will utilize a retrospective, deidentified dataset containing a cohort of youth and families that utilized CYMH mental health services at a large CYMH agency between 2019 and the end of 2023 in Ontario.\nThough the study will be largely exploratory in nature, the work will be guided by several hypotheses. First, given past research we suspect that predictions will be weakest at the earliest stages of the client journey when information in the EMHR is limited to only an intake screener and basic demographic information, however, as information in the EMHR increases over time, we anticipate accuracy will increase. Furthermore, we anticipate that the best predictor of work for unseen clients will be mental health acuity features known to be linked to resource-use such as externalizing behaviours and prior self-harm or suicide attempts, but anticipate the best predictor of work for known clients will be time-based features such as time since first agency contact or time since last assessment. This hypothesis stems from existing literature indicating that these variables are critical drivers of resource use (Perlman et al. 2013; Tran et al. 2019)."
  },
  {
    "objectID": "Caseweight Prospectus.html#case-mix-review",
    "href": "Caseweight Prospectus.html#case-mix-review",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Case-mix Review",
    "text": "Case-mix Review\nTo date, the main strategy employed to determine provider workload based on client characteristics in CYMH relies on case-mix methodology borrowed from the medical domain (Tran et al. 2019; Johnson et al. 1998). Case-mix classification systems are used in the health care sector to help payers and agencies monitor cost by categorizing clients based on their expected resource use. Casemix algorithms assume that though the needs of an individual will be unique, there are shared characteristics that determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically these systems are informed by information contained in case records. At the agency level, case records contain a variety of information, including provider-level information such as the number of direct and indirect-hours attributable to individual clients, as well as client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services).\nCase-mix algorithms usually take one of two approaches to classification based on the expected work associated with features contained a patient’s health record (CMHO 2019). Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight attached to it (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population. Often agencies rely on rudimentary case-mix algorithms to classify cases based on simple factors like age or program accessed. For example, a client accessing long-term counseling and therapy services may have a different weight attached in terms of expected resource use compared to a client accessing a one-session brief service. Index systems on the other hand, combine different case characteristics to provide a continuous, numerical value which maps to expected resource use (Tran et al. 2019).\n\n\n\n\n\n\n\ng\n\n\n\nnode0\n\nClientID\n\n1234\n\n...\n\nN\n\nDate\n\nDate_0\n\n...\n\nN\n\n...\n\n \n\n \n\n \n\nCrisisEvent\n\n \n\n \n\n \n\n\n\nnode1\n...\n\n\n\nnode3\n\nClientID\n\n1234\n\n...\n\nN\n\nDate\n\nDate_0\n\n...\n\nN\n\n...\n\n \n\n \n\n \n\nContactType\n\n \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhealthcare_flowchart\n\n\ncluster_3\n\nMedical Dashboard\n\n\ncluster_0\n\nRaw Data\n\n\ncluster_1\n\nData Representation\n\n\ncluster_2\n\nFeature Store and Modeling\n\n\n\nPatientID\n\nPatient ID\n\n\n\nCrisisEvents\n\nCrisis Events\n\n\n\nPatientID-&gt;CrisisEvents\n\n\n\n\n\nDiagnoses\n\nDiagnoses\n\n\n\nPatientID-&gt;Diagnoses\n\n\n\n\n\nContacts\n\nContacts\n\n\n\nPatientID-&gt;Contacts\n\n\n\n\n\nPatientStatic\n\nStatic Information About Patient\n\n\n\nPatientID-&gt;PatientStatic\n\n\n\n\n\nCrisis\n\nCrisis Events Over Time\n\n\n\nCrisisEvents-&gt;Crisis\n\n\n\n\n\nDiagnosis\n\nDiagnosis Over Time\n\n\n\nDiagnoses-&gt;Diagnosis\n\n\n\n\n\nFeatureStore\n\nFeature Store (Patient Features, Weekly Features)\n\n\n\nCrisis-&gt;FeatureStore\n\n\n\n\n\nDiagnosis-&gt;FeatureStore\n\n\n\n\n\nPatientStatic-&gt;FeatureStore\n\n\n\n\n\nModelTraining\n\nModel Selection and Training\n\n\n\nFeatureStore-&gt;ModelTraining\n\n\n\n\n\nRiskPrediction\n\nPredicting Risk of Crisis Event\n\n\n\nModelTraining-&gt;RiskPrediction\n\n\n\n\n\nHighRiskPatients\n\nPatients at High Risk\n\n\n\nRiskPrediction-&gt;HighRiskPatients\n\n\n\n\n\nIndicators\n\nRisk Indicators\n\n\n\nHighRiskPatients-&gt;Indicators\n\n\n\n\n\nPatientRoles\n\nPatient Roles\n\n\n\nHighRiskPatients-&gt;PatientRoles\n\n\n\n\n\n\n\n\n\n\nWhile case-mix systems are widely used in the medical domain, to date, most mental health case-mix classification systems focus on acute care in hospital or other inpatient settings which are distinctly different than community based care in several ways (Tran et al. 2019). Typically, community based care offers a wider range of services. From brief services to longer-term treatment like counselling and therapy as well as group programs, school-based treatment as well as crisis intervention offered in partnership with local hospitals. Unlike many inpatient settings, services provided in community settings often lack clear diagnoses and recovery timelines making them more complicated to model. For instance, a medical emergency like a broken arm has a predictable recovery window, treatment protocol and thereby cost associated, unlike the subjective experience of anxiety or depression where the time and effort needed to recover is less black and white.\nThe difficulty of modeling EMHR in the CYMH domain, is illustrated by the fact that only a handful of studies have looked at solving this problem, despite urgent need. Indeed, a 2019 scoping review of casemix literature in community-based mental health care domains found only a single case that looked at case-mix classification to predict mental health care resource (Tran et al. 2019; Martin et al. 2020). In that study, researchers modeled 4573 client records from eleven UK outpatient CYMH agencies, comparing cluster analysis, regression trees and a conceptual classification based on clinical best practice guidelines to predict the number of appointments a client attended in treatment (Martin et al. 2020). The researchers found that data-driven classification was no more clinically meaningful than conceptual classification in accounting for number of appointments; moreover, there was little evidence to support the idea that either client complexity or context factors (with the exception of school attendance problems) were linked to overall appointment counts (Martin et al. 2020). Moveover, the models failed to explain significant variation in resource provision between workers despite clients exhibiting similar characteristics. Data quality problems and omission of important individual-level factors were cited as potential points of failure but suggesting their results merited further testing and development.\nIn a related cohort, a group of researchers tried to predict the work associated with client features at a community-based mental health centre for the elderly (Baillon et al. 2009). Using an 8-item self-designed case weighting scale (CWS) researchers identified factors staff felt contributed to demand for time. A multiple regression model was used to assign different weightings to predictors based on the strength of its relationship with the outcome (an estimation of time spent on each client logged over a four-week period). The resulting coefficients were then added to a spreadsheet and used to predict the total time a client would utilize in a four-week period following the first appointment based on the 8 characteristics. Though the model was reported a success, accounting for 58% of the variance in time spent on client-related work, the sample consisted of only 87 cases leaving it unclear how accurate the model really was (Baillon et al. 2009; Mansournia et al. 2021). Moreover, inter-rater and re-rater reliability results indicated that the assessment, whether from a client’s self-report or a professional’s clinical opinion, did not necessarily relate to the amount of time needed by clients (Baillon et al. 2009).\n\nMachine learning, a novel approach to modeling case-mix\nConsidering the challenges outlined by prior research in modeling the high-dimensional, sparse data characteristic of EMHRs, we next looked to a growing body of research leveraging machine learning algorithms to model electronic health data. Machine learning (ML) is a branch of artificial intelligence that uses statistical techniques that enable computers to learn patterns from data to make accurate predictions (Nielsen 2016). ML algorithms are particularly well-suited to modeling the complex, high-dimensional data found in EMHRs (Chen et al. 2023; An et al. 2023). Unlike traditional statistical methods that aim to confirm specific linear relationships, ML algorithms independently identify high-dimensional, non-linear patterns for the best predictions. ML strategies further contain many advanced techniques like regularization and cross-validation which can be used to optimize a model’s ability to generalize to unseen data (Chen et al. 2023), making this approach particularly suitable for our purposes (Chen et al. 2023; An et al. 2023).\nWithin the inpatient mental health domain, machine learning has mostly been used to predict specific events like substance relapse, self-harm and suicide risk. However a recent study leveraged ML to build a model that continuously monitors patient records to predict crisis-relapse over a 28 day period (Garriga et al. 2022). The winning XGBoost regression demonstrated good accuracy in distinguishing between cases who were likely and unlikely to experience a crisis in the next 28 days. Specifically, the model could correctly differentiate those at risk from those not at risk about 80% of the time (Garriga et al. 2022). Morever, in a subsequent post-hoc case study, healthcare professionals rated the predictions produced by the model valuable for managing patient care in 64% of cases, helping them to prioritize patients more effectively and potentially prevent crises (Garriga et al. 2022). Though the author’s did not model the resource use directly as we hope to do, ‘crisis-risk’ served as a proxy for work. By predicting crises, they could anticipate the increased resource-demand which they hoped could inform better case prioritization and managment."
  },
  {
    "objectID": "Caseweight Prospectus.html",
    "href": "Caseweight Prospectus.html",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "",
    "text": "Amidst growing demand for child and youth mental health services in Canada and beyond, human resource challenges have been identified as a significant area of concern (CMHO 2022). In Ontario, a 2020 survey of community CYMH centres revealed that 83% of agencies reported staffing shortages–59% of them direct-service, clinical roles (i.e., psychologists, psychotherapists, and social workers)–which is a concern, as without an adequate workforce, children, youth and their families experience longer wait times and gaps in service that impact access to treatment (CMHO 2022). Illustratively, the same CMHO survey reported that 28,000 children and youth in Ontario were waiting up to 2.5 years for mental health services (CMHO 2019, 2020; CYMHLAC 2019), some even “aging out” of the system before they are off the wait list. With over 70% of mental health and addiction problems starting before age seventeen, this is a problem. Not only is a critical opportunity for early intervention missed, but individual and family stress related to mental health challenges are compounded, increasing the burden to a public health care system, where in Ontario, hospitalization of youth with mental health and addictions issues are up by an estimated 90% (CMHO 2020; CYMHLAC 2019). At the same time, when demand outpaces staffing, existing providers are having to manage higher client volumes containing more complex cases, perpetuating a cycle of provider burnout, absenteeism and high turnover. For this reason, developing a way to anticipate and monitor the contribution of individual cases to the overall work of providers is critical to improving client outcomes and minimizing provider burnout.\nAccording to the most recent Auditor General’s Report on Child and Youth Mental Health, a central problem that impedes the ability of agencies to efficiently meet this challenge, is determining reasonable provider to client workload ratios that reflect the varying needs and intervention levels required by different clients (CMHO 2019). Typically, the most common metric used for estimating provider workload in community based settings are case counts, with the type of service coming in second (i.e., brief services versus counselling and therapy) (CMHO 2019). Case counts are typically used to determine how many new clients a provider has room for in their overall caseload and signal when a provider has reached their capacity (CMHO 2019). For example, an agency might set a target of 20 cases per provider for counselling and therapy services; meaning that providers without 20 cases, theoretically have room for more. In practice, the number of cases in a provider’s caseload should decrease as the complexity of individual cases within increase. However, the resources needed to manually evaluate and monitor the changing complexity and intensity of each case’s needs are beyond what most public agencies can provide (CMHO 2019). As a result, cases are most often assigned in a way that assumes that each case represents a similar amount of work.\nIn busy publicly funded CYMH agencies, manual review of a high volume of digital case files across hundreds of clients to make proactive care decisions is impractical, unsustainable and error-prone. Given this state of affairs, if there was a way to anticipate and monitor the work associated with a given case without requiring dedicated staff, agencies might more efficiently manage provider work. Research has already demonstrated the feasibility of predicting events associated with a wide range of healthcare problems, including hospital readmission and in-hospital death. However, the mental health literature is mostly limited to predicting specific types of events like risk of suicide, self-harm or onset of first psychosis–rather than predicting the overall resource needs associated with each case. Ultimately, much remains unknown about the feasibility of leveraging machine learning (ML) models to estimate work based on client-level factors. Moreover, even a highly accurate predictive model can’t guarantee improved mental health outcomes or long-term cost savings; therefore, it remains unclear whether new predictive technologies could provide tools that are useful to mental healthcare practitioners.\nWith this gap in mind, the current research proposes to explore the feasability of predicting the work associated with a given case at different points along the client timeline to examine whether such predictions could provide added value to clinical practice. The assumption underlying the research, is that there are historical patterns that predict future mental health resource use and that such patterns can be identified in electronic mental health records (EMHR), despite its sparseness, noise, errors and systematic bias."
  },
  {
    "objectID": "Presentation.html",
    "href": "Presentation.html",
    "title": "Beyond Counting Clients",
    "section": "",
    "text": "Providers reporting maxed caseloads\ncase complexity increasing over time\nlong waitlists for service\nat the same time, caseloads at historic lows\n\n\n\nRight column\n\n\n\nquantify the weight of cases to account for the reported increased complexity and find a way to determine who has the capacity to be assigned more youth."
  },
  {
    "objectID": "index.html#case-mix-review",
    "href": "index.html#case-mix-review",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Case-mix Review",
    "text": "Case-mix Review\nAcross health domains (e.g., psychiatric and emergency medicine), various strategies have been employed to manage provider workload by determining service levels with client characteristics like symptom severity or prior diagnoses (Tran et al., 2019; johnson?). These systems assume that although the needs of each individual in a population will be unique, there will be shared characteristics that determine the type of treatment they need (e.g., family counselling versus substance use treatment). These groups represent the mix of cases in a given caseload, which is often used to estimate cost based on the types of care the population needs and the time involved in servicing those needs (Johnson et al., 1998; Tran et al., 2019).\nCase-mix classification systems have been used in the healthcare sector to help payers and agencies monitor costs by categorizing clients based on their expected resource use (Johnson et al., 1998; Tran et al., 2019). Casemix algorithms assume that though the needs of an individual will be unique, shared characteristics determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically, these systems are informed by information contained in case records. At the agency level, case records contain a variety of information, including provider-level information like the number of direct and indirect hours attributable to individual clients, as well as client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services).\nTypically, case-mix systems take one of two approaches to classification (CMHO, 2019). Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population. For example, a client accessing long-term counselling and therapy services might be assigned a different weight in terms of expected resource use than a client accessing a one-session brief service. Index systems, on the other hand, combine different case characteristics to provide a value that maps to expected resource use or acuity of needs (e.g. a case weight or case complexity score that ranges from 0, the least complex, to 1, the most complex) (Tran et al., 2019). Indexing systems are often used to triage cases by assigning a score to new clients based on answers to an intake assessment. Often, there is a threshold score above which clients are considered acute and may receive services more quickly; at the same time, scores below a specific threshold may not qualify for publicly funded services. For instance, a youth reporting thoughts of suicide or other self-harming behaviour might be scored higher than a youth reporting problems focusing in school.\nWhile the term algorithm gives the impression that these are sophisticated mathematical models, they are usually conceptual, rules-based frameworks that rely on manual review of client files rather than an automated, data-driven tool. Although research has begun to explore how machine learning could automate and streamline classification processes, prior work has focused primarily on inpatient acute-care settings, which differ markedly from community-based outpatient settings (Tran et al., 2019), where clear diagnostic criteria and predictable recovery paths are more variable. For instance, the healing process for a broken arm has a relatively fixed timeline and treatment protocol. However, recovery from anxiety or depression is more nuanced and individualized, making it inherently more complicated to model.\nThe difficulties inherent in modelling electronic mental health data are underscored by the fact that only a handful of studies have looked at solving this problem despite an urgent need. Indeed, a 2019 scoping review of casemix literature in community-based mental health care found only a single case that looked at data-driven methods to predict mental health care resource (Martin et al., 2020; Tran et al., 2019). In that study, researchers modelled 4573 client records from eleven UK outpatient CYMH agencies, comparing cluster analysis, regression trees and a conceptual classification based on clinical best practice guidelines to predict the number of appointments a client attended in treatment (Martin et al., 2020), finding the data-driven classification no more clinically meaningful than conceptual classification in accounting for number of appointments. There was little evidence to support the idea that either client complexity or context factors (with the exception of school attendance problems) were linked to overall appointment counts (Martin et al., 2020). Moreover, the models failed to explain significant variations in resource provision between workers despite clients exhibiting similar characteristics. Data quality problems and omission of critical individual-level factors were cited as potential points of failure, but suggesting their results merited further testing and development.\nIn a related cohort, researchers tried to predict the work associated with client features at a community-based mental health centre for the elderly (Baillon et al., 2009). Using an 8-item self-designed case weighting scale (CWS), researchers identified factors that staff felt contributed to the demand for time. A multiple regression model was used to assign different weightings to predictors based on the strength of their relationship with the outcome (an estimation of time spent on each client logged over four weeks). The resulting coefficients were then added to a spreadsheet and used to predict the total time a client would utilize in a four-week period following the first appointment based on the eight characteristics. Though the model was reported a success, accounting for 58% of the variance in time spent on client-related work, the sample consisted of only 87 cases leaving it unclear how accurate the model was (Baillon et al., 2009; Mansournia et al., 2021). Moreover, inter-rater and re-rater reliability results indicated that the assessment, whether from a client’s self-report or a professional’s clinical opinion, did not necessarily relate to the amount of time needed by clients (Baillon et al., 2009).\n\nMachine learning, a novel approach to modeling case-mix\nConsidering the challenges outlined by prior research in modelling the high-dimensional, sparse data characteristic of EHRs, we next looked to a growing body of research leveraging machine learning algorithms to model electronic health data. Machine learning (ML) is a branch of artificial intelligence that uses statistical techniques that enable computers to learn patterns from data without explicit instructions (Nielsen, 2016). In mathematical terms, machine learning algorithms use statistical techniques to optimize a model’s parameters. This process involves minimizing a loss function that quantifies the difference between the model’s predictions and the actual data. For example, in supervised learning, the goal is to find a function f(x) that maps input features X to an output Y such that the predicted outcomes are as close as possible to the actual outcome (Nielsen, 2016). This approach is highly effective for complex tasks like image recognition, natural language processing, and predictive analytics, where traditional rule-based programming is infeasible due to the high dimensionality and variability of the data. ML algorithms are particularly well-suited to model complex, high-dimensional data in EHRs (An et al., 2023; Chen et al., 2023).\nWithin the inpatient mental health domain, machine learning has mainly been used to predict specific events like substance relapse, self-harm and suicide risk. However, a recent study leveraged ML to build a model that continuously monitors patient records to predict crisis relapse over 28 days (Garriga et al., 2022). The winning XGBoost regression demonstrated good accuracy in distinguishing between cases that were likely and unlikely to experience a crisis in the next 28 days. Specifically, the model could correctly differentiate those at risk from those not at risk about 80% of the time (Garriga et al., 2022). Moreover, in a subsequent post-hoc case study, healthcare professionals rated the predictions produced by the model as valuable for managing patient care in 64% of cases, helping them to prioritize patients more effectively and potentially prevent crises (Garriga et al., 2022). Though the authors did not model the resource use directly as we hope to do, ‘crisis risk’ served as a proxy for work. By predicting crises, they hoped to anticipate increased resource demand, which might better inform case prioritization and management."
  },
  {
    "objectID": "index.html#the-current-study",
    "href": "index.html#the-current-study",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "The current study",
    "text": "The current study\nBuilding on insights from Garriga et al. (2022), the current research aims to explore the feasibility of estimating the number of weekly provider hours a case may require, assessed at 28-day intervals. The underlying assumption is that historical patterns can reliably predict future mental health resource use like provider-workload and that these patterns are identifiable in electronic mental health records (EHR).\nTo test these assumptions, we will analyze a retrospective, deidentified dataset from a large child and youth mental health (CYMH) agency in Ontario, Canada, encompassing data from clients served between 2019 and early 2024. Although largely exploratory, the study will be guided by several hypotheses. First, as informed by Garriga et al. (2022), we hypothesize that workload prediction will be weakest early in the client journey when available EHR data is limited to intake screener results and basic demographic information. However, as more data accumulates over the course of treatment—such as session attendance and crisis events—we anticipate prediction accuracy will significantly improve.\nConsistent with Garriga et al. (2022)’s, we expect that for new clients, factors such as a lack of family support and risk of harm to self or others will most strongly predict provider hours needed. For known clients, we hypothesize that time-based factors, such as the frequency of no-shows and the number of crisis events, will be more predictive of workload demands.\nFinally, we expect that the winning machine learning algorithm will outperform a baseline model that will be designed to reflect the way agencies typically estimate resource needs today. This baseline model will rely on the conceptual approach often used in practice, where resource allocation is based on the type of service a client is accessing (e.g., counselling and therapy services being assigned greater weight than brief interventions) (CMHO, 2019). By comparing these approaches, the study aims to evaluate the extent to which data-driven machine-learning models can enhance workload prediction in CYMH settings."
  },
  {
    "objectID": "index.html#data-security",
    "href": "index.html#data-security",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Security",
    "text": "Data Security\nGiven the sensitivity of mental health data, strict privacy and security measures will be enforced throughout the research process. Necessary ethical approvals will be obtained from relevant ethics boards, including both Compass Child and Youth Family Services and Laurentian University’s institutional review board. An exemption for the use of de-identified data will also be required from both institutions.\nDe-identified clinical data will be extracted from Compass’s electronic health information system, which is maintained by the agency. The data will be de-identified at extraction using the Health Insurance Portability and Accountability Act (HIPAA) Safe Harbor Method (OCR, 2012). This process ensures that all directly identifying information, such as names, addresses, birth dates, and postal codes, is removed. Additionally, unique client identification codes will be encrypted using a hashing system to prevent re-identification.\nTo further enhance security, the dataset will remain under the custody of Compass at all times. Data analysis will be conducted solely by the principal researcher on a password-protected machine belonging to Compass. Model results, summary statistics, and visualizations will only include aggregate metrics, focusing on predictor and model performance. No individual scores or identifiers linked to clients or small subgroups will be reported. Approval from Compass will be obtained before any findings are disseminated in external reports or presentations."
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Dataset",
    "text": "Dataset\nThe de-identified data will include approximately 6000 records containing demographic information, referrals, diagnoses, risk and well-being assessments and crisis events for all outpatients. Cases younger than five and older than 17 will be excluded, as Compass’ core services are only offered to children and youth under 18. There are no plans to exclude cases based on any other feature, including diagnoses; however, if, for whatever reason, this changes, it will be outlined in the documentation."
  },
  {
    "objectID": "index.html#procedure",
    "href": "index.html#procedure",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Procedure",
    "text": "Procedure\nThe following steps outline the proposed process, which will consist of four phases: 1) data cleaning, preprocessing and exploration; 2) data splitting; 3) aggregating workload proxies (direct and indirect services hours) that will be used as stand-ins for actual workload; 4) identifying and aggregating indicators of workload (i.e., independent variables/features) that will be used to model our proxies; 5) modelling the relationship between the indicators and proxies with machine learning algorithms of varying complexity; and 6) evaluating the accuracy of predictions on the unseen test data (see Figure 2).\n\n\n\n\nFigure 2\n\n\nExperimental procedure\n\n\n\n\n\n\n\n\nNote. Flowchart of the experimental procedure. Data will be split into training and test sets by client ID. The training set will be used to train the models using 10-fold group cross-validation, while the test set will act as a control group to evaluate the models’ performance on unseen data."
  },
  {
    "objectID": "index.html#features-and-target-generation",
    "href": "index.html#features-and-target-generation",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Features and target generation",
    "text": "Features and target generation\nImportantly, feature engineering–the creation of new predictors based on existing variables in the dataset–will occur after data splitting on the training data to minimize the risk of data leakage that could inadvertently occur when creating new variables from the full dataset (Sheetal et al., 2023). The main criterion for inclusion in the model will be the variable’s availability in the electronic health record (EHR) system at intake. There will be four primary types of predictor: i) time based; ii) count based; iii) recent information; iv) static and semi-static information .\nWith the exception of static information like presenting concern or referral source, all EMHR data will include the associated date and time. The date and time refer to the moment when the specific event or assessment occurred—that is, the date and time that there was a contact with a client. To prepare the data for the modeling task, each client’s case records will be consolidated at a weekly level according to the date associated with the record. Following this process, evenly spaced time series will be generated for each client spanning from their first interaction with Compass to the study’s final week. The features and labels generated for each week will be computed using the data from dates prior to that week. Static data that is susceptible to change over time (for example, postal code or school board information) will be removed to mitigate the risk of retrospective leakage.\nOutcome (target) generation. To construct a continuous case-weight prediction target, the sum of client-related direct and indirect hours logged by clinicians and associated with a specific program and client will be aggregated at the same weekly level as the features, based on the time recorded by the worker prior to that week. We also intend to examine which measure of client-related time is most stable and reliable over time both direct and indirect time or either on its own.\nFeatures generation. We will extract features from a total possible feature set of approximately 250 features. Informed by Garriga et al. (2022), extraction will be performed according to six data characteristics:\n\nStatic or semi-static features. demographics data will be represented as constant values attributed to each case with age treated as a special case that changes each year.\nDiagnosis feature. Client will be assigned their latest valid diagnosed developmental disability or psychological disorder (if contained in the record) or an ‘un-diagnosed’ label and then seperated into diagnostic groups according to the latest valid diagnosed disorder at the last week of the training set to avoid leakage into the validation and test sets. Any codes created for this feature will be added to the final paper.\nEHR weekly aggregations. Records related to client-agency interactions will be aggregated on a weekly basis for each client. The resulting features will constitute counts of each type interaction and one-hot encoded according to their categorization. One-hot coding is a type of dummy variable where if a specific event did occur there will be a 1 for that week and if not a value of 0 will be assigned to the feature related to the corresponding event for the corresponding week.\nTime-elapsed features. At each client-week, for each type of interaction and category, we will construct a feature that counts the number of elapsed weeks since the last occurence of the corresponding event. If the client never experienced such an event type up to that point in time, an NA value will be used.\nLast crisis episode descriptors. For each crisis episode, a set of descriptors will be used to build feature for the subsequent weeks until the next crisis events occurs. If the client never had a crisis event up until that point in time, NA values will be used.\nLast assessment descriptors. For each assessment item, a set of descriptors will be used to build a feature for the subsequent weeks until the next assessment occurs. All clients will have at least one assessment to be included in the study. A penalty will reduce the value of assessment scores the further away from the assessment date they are for each aggregated week.\nStatus features. For specific records, characterized by a start and end date, features for the corresponding weeks will be built by assigning their corresponding value (or category); otherwise they are set to NA.\n\nIn addition to record-based features, we will also add the week number (of a year 1-52) to account for seasonality effects.\nCaseweight prediction modeling and evaluation. The caseweight prediction task will be defined as both a continuous regression problem to be performed on a weekly basis and a continuous classification problem on the same timeline (low, medium, high intensity). For each week, the model will predict the weekly hours needed during the upcoming 28 days. Applying a rolling window approach will allow for a periodic update of the caseweight by incorporating newly available data (or the absence of it) at the beginning of each week. The approach is common in settings where predictions are to be used in real time and when data are continuously updated (Garriga et al., 2022).\n\nData Splitting\nTo maximize the generalizability of our models, our aim is to apply a time-based 80/10/10 training/validation/test split depending on the amount of data left over after data cleaning. Roughly, training data will start in the first week of January 2019 and end the last week of June 2022. Validation data will start in the first week of January 2023 and end in the last week of December 2023. Test data will start in the first week of January 2024 and end in the last week of December 2024. We may omit data from the first 6 months of the COVID pandemic due to disruptions in normal service delivery that continued until new policies and procedures could be implemented. The irregularity of these data may interfere with the model identifying typical seasonal trends.\n10-fold, timed based cross-validation will be used to tune model parameters. The cross-validation folds will be created with a portion of the training data, subdividing it into 10 subsets, or “folds,” that will preserve the same time structure. Performance evaluations will be conducted on a weekly basis and each week’s results will be used to build confidence intervals on the evaluated metrics. All reported results will be computed using the test set if not otherwise indicated. (Figure 3). The test set will act as a control group to evaluate the models’ performance on “unseen data” at the very end of the training process. By keeping the test set separate and untouched during training, we ensure that our final evaluation provides a better estimation of how the models will perform in practice. This final step is crucial for assessing the models’ generalizability and for identifying any overfitting that may have occurred during training.\n\nFeatures (indicators of work).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime based\nCount based\nLatest available assessment / contact information\nStatic/semi-static information\n\n\n\n\nWeeks since last crisis event\nCount of crisis events\nMental health acuity scores (e.g., depression, anxiety, internalizing, externalizing etc.)\nAge, gender, school district\n\n\nWeeks since first contact\nCount of no-shows\nIdentified risks such as substance use, self-harm or suicide risk\nADHD/Autism diagnosis\n\n\nWeeks since last no-show\nCounts of substances used\nCurrent services accessed\nMental health diagnosis\n\n\nWeeks since last contact\nCounts of phone calls\nIdentified symptoms\nLearning disability diagnosis\n\n\nWeeks since substance misuse identified\nCount of previous completed services\nPreviously indicated need (CHAMPS)\n\n\n\n\nWeeks since self/harm identified\nDischarge and referral counts\nRecent contact with CAS\n\n\n\n\nWeeks since suicide risk identified\nNumber of current services\nRecent psychological consult\n\n\n\n\nWeeks since last discharge\n\n\nRecent diagnosis\n\n\n\n\nWeeks since last crisis episode\n\n\nCurrent caregiver and family support\n\n\n\n\nWeeks since first visit\n\n\n\n\n\n\n\n\nWeeks since referral\n\n\n\n\n\n\n\n\n\n\n\n\n\nA full list of all variables will be included in the final report.\n\n\n\n\nFigure 1\n\n\nClinical pathway\n\n\n\n\n\n\n\n\nNote. Flow chart of client selection process. Clients who complete an intake assessment will be screened for inclusion. Only clients referred to Counselling and Therapy (CT) services will be included in the analysis. Predictions will include: client-related work at follow-up assessment and client-related work at end of treatment.\n\n\n\n\n\nFigure 2\n\n\nModeling Caseweight–client-related work\n\n\n\n\n\n\n\n\nNote. Using indicators of client-related work (e.g. depression scores, anxiety scores, etc.) in the electronic health record (EHR)to predict workload proxies. Adapted from Predictors of Workload, by Wang et al. (2021).\n\n\n\n\n\nFigure 3\n\n\nExperimental procedure\n\n\n\n\n\n\n\n\nNote. Flowchart of the experimental procedure. Data will be split into training and test sets by client ID. The training set will be used to train the models using 10-fold group cross-validation, while the test set will act as a control group to evaluate the models’ performance on unseen data.\n\n\n\n\nModel Selection\nWe plan to utilize the following supervised machine-learning algorithms for both regression and classification problems: i) Random Forest (RF) for its ability to handle large datasets with high dimensionality; ii) XGBoost, known for its high performance and predictive accuracy on tabular datasets, iii) LASSO and Ridge regression for their ability to manage high multicollinearity, and finally iv) linear regression to serve as a baseline model for continuous outcomes and generalized logistic regression for binary outcomes. All algorithms will be trained on the same training set and cross validation folds and evaluated on the same test set using the TidyModels suite of packages in R Studio. These algorithms were chosen based on their success modeling similarly complex, tabular data types and may grow to include other models in the final paper (Salditt et al., 2023; Sheetal et al., 2023).\n\n\n\n\n\n\n\n\n\n\n\n\nRegression & Classification Models Proposed\nJustification\n\n\n\n\nClinical baseline\n\n\n\n\nXGBoost\n\n\n\n\nRegression (linear & multivariable logistic)\n\n\n\n\nRandom Forest\n\n\n\n\nNaïve Bayes\n\n\n\n\nFeed Forward Neural Network\n\n\n\n\n\n\n\n\n\n\n\nValidation and Testing\nFinal models will be statistically compared and evaluated on the test set using the following performance metrics: i) Mean Absolute Error (MAE), and ii) Root Mean Squared Error (RMSE) for continues outcomes. For categorical outcomes, we will rely on accuracy and area under the curve (AUC). These evaluations will help determine the accuracy, generalizability and robustness of each model (Salditt et al., 2023; Wang et al., 2021) Final models will also be analyzed to identify which predictors were the most significant predictors of client-related workload using SHAP scores.\n\n\nSoftware and Tools\nWe will use R Statistical Software and the Tidyverse and TidyModels suite of packages for data manipulation and model building (R Core Team, 2024; Khun & Wickham 2020). This choice aligns with our familiarity with R and the study’s specific requirements. R Quarto Markdown will be used for documentation and reproducibility. During the model building process, there is a chance we may use Python as well in the RStudio environment and will report and document this choice thoroughly if we do (Van Rossum & Drake, 1995)."
  },
  {
    "objectID": "index.html#data-preprocessing",
    "href": "index.html#data-preprocessing",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nAfter de-identification, data preprocessing will include cleaning, joining data frames and handling missing values. Decisions regarding missing data will be made on a case-by-case basis, with details on imputation or exclusion documented in the final report. Any data normalization procedures will also be reported. To ensure reproducibility, the Python data scripts used for preprocessing will be publicly available.\nAll data points will include an associated date and time, reflecting the moment a specific event or assessment occurred. These timestamps will guide the aggregation of each client’s case records into weekly evenly spaced time series for each client, spanning their first interaction with Compass to the last (see Figure 1). Features and labels for each week will be computed at the start of the week from data that was aggregated the week before, ensuring temporal consistency and avoiding data leakage. Additionally, static data prone to change over time (e.g., postal code or school board information) will be excluded to mitigate the risk of retrospective leakage. Retrospective data leakage occurs when information from the future (relative to the prediction point in time) inadvertently influences the model during training or evaluation. This typically happens in retrospective studies where datasets contain time-stamped records, and the temporal order of events is not carefully maintained during data preprocessing or feature engineering."
  },
  {
    "objectID": "index.html#validation-and-testing",
    "href": "index.html#validation-and-testing",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Validation and Testing",
    "text": "Validation and Testing\nFinal models will be statistically compared and evaluated on the test set using appropriate performance metrics depending on whether it is a regression task (mean absolute error or root mean squared error) or classification task (accuracy, precision, recall and area under the curve). The evaluations will help determine each model’s accuracy, generalizability and robustness (Salditt et al., 2023; Wang et al., 2021). Final models will also be analyzed to identify which predictors were the most important in terms of estimating client-related work.\nFurthermore, to enhance the interpretability of our model, we plan to implement SHapley Additive exPlanations (SHAP) for feature analysis (Lundberg & Lee, 2017). SHAP is a method that helps quantify the contribution of each feature to the model’s predictions, providing insights into how specific client characteristics and historical data points influence predicted weekly clinician hours. Interpretability is essential in a mental health care setting, as decisions directly impact client care and resource allocation (Feretzakis et al., 2024). Clinicians and administrators need to understand not only the predicted workload but also the driving factors behind each prediction to ensure fair, personalized, and transparent decision-making. For instance, if certain factors like recent diagnoses or patterns of no-shows are highly influential, this can guide intervention strategies and inform staffing decisions tailored to client needs. SHAP’s ability to provide such detailed, interpretable explanations makes it a critical tool for ensuring that the model’s predictions are aligned with clinical understanding and ethical care practices (Feretzakis et al., 2024)."
  },
  {
    "objectID": "index.html#software-and-tools",
    "href": "index.html#software-and-tools",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Software and Tools",
    "text": "Software and Tools\nPython will be used as the primary programming language for model development and evaluation with support from R Statistical Software (Van Rossum & Drake, 1995). Quarto Markdown will facilitate documentation and ensure reproducibility, with all workflows executed within the Positron IDE environment (Positron, 2024). Positron is a next-generation data science integrated development environment (IDE) developed by Posit PBC. It is built on Code OSS and designed to support multiple programming languages, including R and Python, providing an extensible and familiar environment for reproducible authoring and publishing (Positron, 2024)."
  },
  {
    "objectID": "index.html#case-mix-history",
    "href": "index.html#case-mix-history",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Case-mix History",
    "text": "Case-mix History\nAcross health domains, particularly emergency medicine, various strategies have been employed to manage provider workload by mapping service levels to client characteristics like symptom severity or prior diagnoses (Johnson et al., 1998; Tran et al., 2019). Case-mix classification systems have been used in the healthcare sector to help payers and agencies monitor costs by categorizing clients based on their expected resource use (Johnson et al., 1998; Tran et al., 2019). Casemix algorithms assume that though the needs of an individual will be unique, shared characteristics determine the type and intensity of treatment needed (e.g., family counselling versus crisis intervention). Typically, these systems are informed by information contained in patient (case) records. At the agency level, case records contain various information, including provider-level information like the number of direct and indirect hours associated with individual clients and client-level characteristics like diagnoses, treatment history, referral source and presenting symptoms (e.g., crisis intervention versus brief services).\nTypically, case-mix systems take one of two approaches to classification (CMHO, 2019). Grouping systems assign people to classes in terms of their expected resource use, with each group having a specific weight (e.g., time-intensive treatment versus brief treatment) relative to the average case in the population (Johnson et al., 1998; Tran et al., 2019). For example, a client accessing long-term counselling and therapy services might be assigned a greater weight in terms of expected resource use than a client accessing a one-session brief service. Index systems, on the other hand, combine different case characteristics to provide a value that maps to expected resource use or acuity of needs (e.g. a case weight or case complexity score that ranges from 0, the least complex, to 1, the most complex) (Developing Caseload/Workload Guidelines for Ontario’s Child and Youth Mental Health Sector, 2019; Tran et al., 2019). Indexing systems are often used to triage cases by assigning a score to new clients based on answers to an intake assessment. Often, there is a threshold score above which clients are considered acute and may receive services more quickly; at the same time, scores below a specific threshold may not qualify for publicly funded services at all. For instance, a youth reporting thoughts of suicide or other self-harming behaviour will likely index higher than a youth reporting problems remaining focused in school (CMHO, 2019).\nCase-mix algorithms are typically conceptual, rules-based frameworks that rely on predefined factors known or hypothesized to affect client care needs (Tran et al., 2019). These frameworks are informed by clinical expertise, existing research, or policy guidelines and often use well-defined variables such as demographic characteristics, diagnoses, or treatment types. In contrast, data-driven frameworks employ empirical analysis, leveraging statistical or machine learning techniques to identify patterns and groupings in client populations without relying on prior assumptions (Garriga et al., 2022; Martin et al., 2020; Tran et al., 2019). A data-driven approach holds the potential to uncover novel insights that conceptual frameworks may overlook.\nWhile a hybrid approach—combining conceptual expertise for clinical validity with data-driven methods for automation and insight discovery—is ideal, the complexity of modeling EHR data has limited the development of reliable data-driven frameworks, particularly in mental health service delivery (Tran et al., 2019). Existing research has primarily focused on acute, inpatient hospital settings, which differ substantially from community-based outpatient care (Aminizadeh et al., 2023; Garriga et al., 2022; Tran et al., 2019). In inpatient settings, conditions often have clear diagnostic criteria and predictable recovery trajectories, such as the relatively fixed timeline and treatment protocol for a broken arm. In contrast, recovery from mental health conditions like anxiety or depression is inherently more subjective, making modelling these data significantly more challenging (garriga2023?).\nThe challenges of modelling electronic mental health data are underscored by the limited body of research addressing this problem despite its urgency (Tran et al., 2019). A 2019 scoping review of case-mix literature in community-based mental health care identified only one study that employed data-driven methods to predict mental health care resource needs in children and youth populations (Martin et al., 2020; Tran et al., 2019). That study analyzed 4,573 client records from 11 UK outpatient CYMH agencies, comparing a conceptual ‘clinical-judgement’ framework to cluster analysis and negative binomial regression to predict the number of appointments a client would attend during treatment (Martin et al., 2020). While the data-driven classification did as well as the conceptual classification, the researchers suggest that data quality issues (systematic errors introduced by data entry or subjective ratings) and omission of important individual-level factors that were not contained in the EHR impacted the accuracy of their models (Martin et al., 2020).\nIn a related cohort, researchers attempted to predict the workload associated with client characteristics at a community-based mental health center for the elderly, aiming to develop a more accurate representation of workload than simple case counts could provide (Baillon et al., 2009). Using an eight-item, self-designed Case Weighting Scale (CWS), they identified factors that staff perceived as contributing to time demands. After an initial assessment, clinicians would complete the CWS for each client, assigning scores based on factors such as family support, communication difficulties or risk of harm to self or others. These scores were input into a multiple regression model, which generated an estimate of the total time that the client would need over a four-week period. The model accounted for 58% of the variance in time spent on client-related work, which they considered a success. However, the sample size of only 87 cases raises concerns about the model’s generalizability and accuracy (Baillon et al., 2009). Additionally, inter-rater and re-rater reliability results suggested that the assessments, whether derived from client self-reports or clinicians’ professional opinions, did not consistently align with the time required for client care (Baillon et al., 2009). Nevertheless, the study does provide a basis for understanding how client characteristics might be leveraged to predict workload in mental health care settings–particularly with more sophisticated models.\n\nMachine learning, a novel approach to modeling case-mix\nBuilding on the limitations of traditional approaches like regression-based models in the Case Weighting Scale (CWS) study, machine learning (ML) offers a promising alternative for predicting mental health resource needs. Unlike conventional methods, ML algorithms learn directly from data without prior programming and are equipped to handle the high-dimensional nature of EHRs making them well-suited for mapping complex relationships between client features, such as depression scores or prior no-shows with outcomes like weekly service hours (An et al., 2023a; Chen et al., 2023). Supervised ML models, aim to optimize a function f(x) that predicts an outcome Y (e.g., hours per week) from input features X, minimizing the difference between predictions and actual data. ML’s ability to uncover patterns in messy data presents a clear advantage for addressing the challenges of modelling client characteristics to predict workload (An et al., 2023b; Chen et al., 2023).\nWithin the mental health domain, ML has mainly been used to predict specific events like substance relapse (Kinreich et al., 2021), self-harm, and suicide risk (Simon et al., 2018; Walsh et al., 2017). For example, Kinreich et al. (2021) used ML to predict a change in drinking behaviour in a population diagnosed with alcohol use disorder (AUD). Combining features like brain connectivity, genetic risk scores and demographic information like age, they achieved 86% accuracy in identifying patients whose AUD had gone into remission, enabling clinicians to provide targeted interventions such as additional counselling sessions or closer monitoring (Kinreich et al., 2021). Another study leveraged ML to monitor patient records and predict crisis relapse in 28-day windows based on EHR data(Garriga et al., 2022). The top performing XGBoost model correctly differentiated those at risk from those not at risk for crisis relapse about 80% of the time (Garriga et al., 2022), and in a subsequent post-hoc case study, clinicians rated the predictions as useful for managing patient care in 64% of cases, reporting the estimates helped prioritize patients more effectively, potentially preventing crises (Garriga et al., 2022). Though the authors did not model resource use directly as we hope to do, ‘crisis risk’ served as a proxy for work. By predicting crises, they aimed to anticipate increased resource demand, allowing for better-informed case prioritization and management. Together, these examples demonstrate the utility of ML in identifying high-risk situations, highlighting its potential to enhance resource planning and improve care delivery in mental health settings."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Overview",
    "text": "Overview\nThis study aims to estimate the weekly provider-hours needed (direct and indirect service) at regular stages in the client journey using machine learning predictive models. The analysis will utilize a retrospective dataset from Compass Child and Youth Family Services, the largest CYMH agency in northern Ontario. Compass serves a culturally and socially diverse population of children, youth, and families, making it a representative setting for this study."
  },
  {
    "objectID": "index.html#data-set",
    "href": "index.html#data-set",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Set",
    "text": "Data Set\nThe dataset will include de-identified client records with completed initial intake assessments for clients active between January 1, 2019, and December 31, 2024. Only cases with a completed initial screener will be included to ensure the availability of baseline data for generating meaningful predictions. Cases younger than five and older than 17 will be excluded, as Compass’ core services are only offered to children and youth under 18. There are no plans to exclude cases based on any other feature, including diagnoses; however, if, for whatever reason, this changes, it will be outlined in the documentation. The de-identified data will include approximately 6000 EHRs containing hundreds of datapoints such as demographic information, referrals, diagnoses, risk and well-being assessments and crisis events for all outpatients. All variables left over after initial variable reduction will be included in the final report. For an overview of the data flow from raw electronic health records (EHRs) to the derived weekly features used in the predictive model structure, see Figure 1.\n\n\n\n\nFigure 1\n\n\nData Flow Pipeline\n\n\n\n\n\n\n\n\nNote. Data flow from raw client records to the derived features used in the predictive model. The top section represents the raw data structure containing rows of client-specific information, including dates, programs, contact types, and contact durations. The middle section visualizes a sample client timeline, mapping key events such as assessment, no-shows, face-to-face contacts, and discharges, which are stored in the EHR. The bottom section shows the weekly aggregate feature set created from these events, with features such as days since last contact and direct hours that were logged for that case in the week prior. The weekly aggregates will be used for model selection and training to predict weekly workload (e.g., weekly caseweight)"
  },
  {
    "objectID": "index.html#data-splitting",
    "href": "index.html#data-splitting",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Splitting",
    "text": "Data Splitting\nTo maintain temporal consistency and maximize the generalizability of the models, the plan is to conduct a time-based 80/10/10 split for training, validation, and testing with careful thought to seasonal aspects of our data. Typically, there are fewer clients accessing service in summer months compared to months that school is in. For this reason, utilizing only a half years data for a testing would risk influencing predictions. Data splitting will be based on chronological order, as follows:\nTraining Data: January 2019 to March 2023 - 79.69% Validation Data: April 2023 to September 2023 - 9.38% Test Data: October 2023 to April 2024. - 10.94%\nData from the first six months of the COVID-19 pandemic may need to be excluded, depending on its irregularity and impact on service delivery. This will be addressed during data cleaning, with details reported in the final documentation.\nTo tune model parameters and ensure robust evaluation, we will use time-based cross-validation. Cross-validation is a method used to assess how well a model is likely to perform on unseen data. In Cross-validation the training data is divided into sequential, time-based subsets, or “folds,” preserving the chronological order of the data. For each fold, the model parameters will be tuned on earlier time periods and tested on later ones, simulating real-world prediction scenarios where past data is used to forecast future outcomes. “Tuning model parameters” involves adjusting hyperparameters, which are internal settings that control how the model learns from the data. Examples include the depth of a decision tree, the number of trees in a random forest, or the learning rate in a neural network. The goal is to find the combination of hyperparameters that minimizes the error between the model’s predictions and the true values. This ensures that the model’s generalizability to new, unseen data has been thoroughly tested, while still accounting for the temporal nature of the dataset.\nThe test set will act as an unseen control to evaluate the final models’ performance at the very end after training and tuning. It will remain untouched during model development to provide an estimation of how the models will perform in real-world scenarios. Final evaluations will include assessments of generalizability, overfitting, and the overall stability of predictions.\n\nOutcome (target) generation\nCaseweight prediction modeling and evaluation. The case weight prediction task will be defined as a continuous regression problem to be performed weekly and a continuous classification problem on the same timeline (low, medium, and high intensity). For each week, the model will estimate the weekly hours needed during the upcoming 28 days. A rolling window approach will be applied to allow for a periodic update of the case weight by incorporating newly available data (or the absence of it) at the beginning of each week. This approach is common when predictions are used in real-time and when data are continuously updated (Garriga et al., 2022).\nTo construct a continuous case-weight prediction target, the sum of client-related direct and indirect hours logged by clinicians and associated with a specific program and client will be aggregated at the same weekly level as the features, based on the time recorded by the worker prior to that week. We also intend to examine which measure of client-related time is most stable and reliable over time: the summed direct and indirect time or either on its own.\n\n\nFeatures (predictors) generation.\nWe will extract features from a total possible feature set of approximately 250 features. See Table 1 for a list of proposed feature groupings and variables that we aim to include. Informed by Garriga et al. (2022)’s methodology, extraction will be performed according to six data characteristics:\nStatic or semi-static features. Demographics data will be represented as constant values attributed to each case, with age treated as a special case that changes yearly.\nDiagnosis features. Client will be assigned their latest valid diagnosed developmental disability or psychological disorder (if contained in the record) or an ‘un-diagnosed’ label and then separated into diagnostic groups according to the latest valid diagnosed disorder at the last week of the training set to avoid leakage into the validation and test sets. Any codes created for this feature will be added to the final paper.\nEHR weekly aggregations. Records related to client-agency interactions will be aggregated on a weekly basis for each client. The resulting features will constitute counts of each type interaction and one-hot encoded according to their categorization. One-hot coding is a type of dummy variable where if a specific event did occur, there will be a 1 for that week, and if not, a value of 0 will be assigned to the feature related to the corresponding event for the corresponding week.\nTime-elapsed features. For each type of interaction and category and each client week, we will construct a feature that counts the number of weeks since the last occurrence of the corresponding event. If the client has never experienced such an event type up to that point in time, an NA value will be used.\nLast crisis episode descriptors. For each crisis episode, a set of descriptors will be used to build features for the subsequent weeks until the next crisis occurs. If the client has never had a crisis event up until that point in time, NA values will be used.\nLast assessment descriptors. For each assessment item, a set of descriptors will be used to build a feature for the subsequent weeks until the following assessment occurs. All clients will have at least one assessment to be included in the study. A penalty will reduce the value of assessment scores the further away from the assessment date they are for each aggregated week.\nStatus features. For specific records, characterized by a start and end date, features for the corresponding weeks will be built by assigning their corresponding value (or category); otherwise, they are set to NA.\nIn addition to record-based features, we will add the week number (of a year 1-52) to account for seasonality effects.\n\n\n\n\n\nTable 1\n\n\nPlanned Features (Predictors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime based\nCount based\nLatest available assessment / contact information\nStatic/semi-static information\n\n\n\n\nWeeks since last crisis event\nCount of crisis events\nMental health acuity scores (e.g., depression, anxiety, internalizing, externalizing etc.)\nAge, gender, school district\n\n\nWeeks since first contact\nCount of no-shows\nIdentified risks such as substance use, self-harm or suicide risk\nADHD/Autism diagnosis\n\n\nWeeks since last no-show\nCounts of substances used\nCurrent services accessed\nMental health diagnosis\n\n\nWeeks since last contact\nCounts of phone calls\nIdentified symptoms\nLearning disability diagnosis\n\n\nWeeks since substance misuse identified\nCount of previous completed services\nPreviously indicated need (CHAMPS)\n\n\n\n\nWeeks since self/harm identified\nDischarge and referral counts\nRecent contact with CAS\n\n\n\n\nWeeks since suicide risk identified\nNumber of current services\nRecent psychological consult\n\n\n\n\nWeeks since last discharge\n\n\nRecent diagnosis\n\n\n\n\nWeeks since last crisis episode\n\n\nCurrent caregiver and family support\n\n\n\n\nWeeks since first visit\n\n\n\n\n\n\n\n\nWeeks since referral\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Splitting\nTo maximize our models’ generalizability, we aim to apply a time-based 80/10/10 training/validation/test split depending on the amount of data left over after data cleaning. Roughly, training data will start in the first week of January 2019 and end in the last week of April 2023. Validation data will start in the first week of May 2023 and end in the last week of October 2023. Test data will start in the first week of November 2024 and end at the end of April 2024. Data from the first 6 months of the COVID-19 pandemic may need to be included due to disruptions in normal service delivery that continued until new policies and procedures could be implemented, depending on the irregularity of these data.\nTimed-based cross-validation will be used to tune model parameters. The cross-validation folds will be created with a portion of the training data, subdividing it into ten subsets, or “folds,” that will preserve the same time structure. Evaluations will be conducted on weekly predictions, and each week’s results will be used to build confidence intervals for the evaluated metrics. All reported results will be computed using the test set if not otherwise indicated. The test set will act as a control group to evaluate the models’ performance on “unseen data” at the end of the training and tuning processes. Keeping the test set separate and untouched during training ensures that our final evaluation provides a better estimation of how the models will perform in practice. This final step is crucial for assessing the models’ generalizability and for identifying any over-fitting that may have occurred during training.\nA final and complete list of all variables will be included in the final report.\n\n\nModel Selection\nWe plan to utilize the following supervised machine-learning algorithms for both regression and classification problems: i) Random Forest (RF) for its ability to handle large datasets with high dimensionality; ii) XGBoost, known for its predictive accuracy on tabular datasets; iii) Random Forest; iv) Feed Forward Neural Network, and finally iv) linear and logistic regression will be used to model our clinical baseline. All algorithms will be trained on the same aggregated training set and cross-validation folds and evaluated on the same test set. These algorithms were chosen based on their success in modelling similarly complex, tabular data types and may grow to include other models in the final paper (Salditt et al., 2023; Sheetal et al., 2023). See Table 2 for a list of proposed models.\nMachine Learning Models and Classifiers. We plan to utilize versions of the following supervised machine-learning algorithms depending on the specific target outcome. Since we will be modelling and comparing a continuous and a classification outcome, this approach will allow us to assess performance across different types of predictions. XGBoost, an implementation of gradient boosting machines (GBMs), will serve as our primary algorithm due to its ability to handle missing data and robustness to scaling factors, eliminating the need for imputation or scaling. GBMs build a sequence of decision trees, where each tree improves on the performance of prior iterations, making them well-suited for our case weight prediction task. To benchmark performance, we will compare XGBoost to a selection of state-of-the-art machine learning classifiers, including logistic regression, naive Bayes, random forest, and neural networks (specifically, multi-layer perceptrons), all of which have been successfully applied to similar prediction tasks with electronic health records (EHRs). We will apply standard scaling and imputation as needed for these classifiers to ensure comparable conditions. We will conduct 100 hyperparameter optimization trials for each classifier to identify optimal parameters, with detailed search spaces provided in the supplementary materials.\nHyperparameter Tuning and Feature Selection. To optimize the models’ hyper-parameters, we will use a Bayesian optimization approach to maximize the area under the receiver operating characteristic curve (AUROC) for classification outcomes and the mean squared error (MSE and MAE) for continuous outcomes. Specifically, we will use Hyperopt, a sequential model-based optimization algorithm that applies Bayesian optimization via the Tree-structured Parzen Estimator, which accommodates a variety of distributions across search spaces. This flexibility will make Hyperopt particularly effective for tuning hyper-parameters across all classifiers. We will use the same approach for feature selection, grouping features by information gain and adding a binary indicator to determine whether each feature should be selected for the model. See Table 1 for details on feature groupings and the feature selection.\n\n\n\n\n\nTable 2\n\n\nPlanned Machine Learning Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression & Classification Models Proposed\nJustification\n\n\n\n\nClinical baseline\n\n\n\n\nXGBoost\n\n\n\n\nRegression (linear & multivariable logistic)\n\n\n\n\nRandom Forest\n\n\n\n\nNaïve Bayes\n\n\n\n\nFeed Forward Neural Network"
  },
  {
    "objectID": "index.html#data-splitting-and-cross-validation",
    "href": "index.html#data-splitting-and-cross-validation",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Data Splitting and cross-validation",
    "text": "Data Splitting and cross-validation\nTo maintain temporal consistency and maximize the generalizability of the models, the plan is to conduct a time-based 80/10/10 split for training, validation, and testing with careful thought to seasonal aspects of our data. Typically, there are fewer clients accessing service in summer months compared to months that school is in. For this reason, utilizing only a half years data for a testing would risk influencing predictions. Data splitting will be based on chronological order, as follows:\nTraining Data: January 2019 to March 2023 - 79.69% Validation Data: April 2023 to September 2023 - 9.38% Test Data: October 2023 to April 2024. - 10.94%\nData from the first six months of the COVID-19 pandemic may need to be excluded, depending on its irregularity and impact on service delivery. This will be addressed during data cleaning, with details reported in the final documentation.\nTo tune model parameters and ensure robust evaluation, we will use time-based cross-validation. Cross-validation is a method used to assess how well a model is likely to perform on unseen data. In Cross-validation the training data is divided into sequential, time-based subsets, or “folds,” preserving the chronological order of the data. For each fold, the model parameters will be tuned on earlier time periods and tested on later ones, simulating real-world prediction scenarios where past data is used to forecast future outcomes. “Tuning model parameters” involves adjusting hyperparameters, which are internal settings that control how the model learns from the data. Examples include the depth of a decision tree, the number of trees in a random forest, or the learning rate in a neural network. The goal is to find the combination of hyperparameters that minimizes the error between the model’s predictions and the true values. This ensures that the model’s generalizability to new, unseen data has been thoroughly tested, while still accounting for the temporal nature of the dataset.\nThe test set will act as an unseen control to evaluate the final models’ performance at the very end after training and tuning. It will remain untouched during model development to provide an estimation of how the models will perform in real-world scenarios. Keeping the test set separate and untouched during training ensures that our final evaluation provides a better estimation of how the models will perform in practice. This final step is crucial for assessing the models’ generalizability and for identifying any over-fitting that may have occurred during training."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Feature Generation (Independent Variables)",
    "text": "Feature Generation (Independent Variables)\nFeatures will be extracted from a possible set of approximately 400 variables. A complete list of proposed feature groupings and variables is provided in Table 1. Following the methodology outlined in Garriga et al. (2022), feature extraction will be categorized into six main types:\nStatic or Semi-Static Features. Demographic data will be represented as fixed values for each case. Age will be treated as a special case, recalculated annually to reflect changes over time.\nDiagnostic Features. Each client will be assigned their most recent valid diagnosis if any (e.g., developmental disability, psychological disorder, or “undiagnosed”). Diagnoses will be grouped by category, using the latest valid entry up to the end of the training period to prevent data leakage. Any classification codes generated for these features will be documented in the final report.\nEHR Weekly Aggregations. Weekly records of client-agency interactions will be aggregated for each client. These aggregated features will include counts of interaction types (e.g., appointments, no-shows) and one-hot encoded variables indicating whether a specific event occurred within the week. For one-hot encoding, a value of 1 indicates the event occurred, while 0 indicates it did not.\nTime-Elapsed Features. For each event type and week, a feature will record the number of weeks since the last occurrence of the event. If the event has never occurred up to that point, the feature will be set to NA.\nLast Crisis Episode Descriptors. Details from the most recent crisis episode (e.g., type, severity, resolution) will be used to create features for subsequent weeks until the next crisis occurs. If no crisis has occurred, the feature will be set to NA.\nLast Assessment Descriptors. For each assessment item, features will be created based on the most recent assessment data, with values decaying over time to reflect diminishing relevance. This decay will apply until the next assessment occurs. All clients will have at least one assessment to ensure inclusion in the study.\nStatus Features. For records with a start and end date (e.g., program intake and discharge), features will assign values (or categories) corresponding to the active weeks. For weeks where the record is not applicable, the feature will be set to NA.\nSeasonality Effects. In addition to record-based features, we will add the week number (of a year 1-52) to account for seasonality effects.\nA final and complete list of all variables will be included in the final report.\n\n\n\n\n\nTable 1\n\n\nPlanned Features (Predictors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime based\nCount based\nLatest available assessment / contact information\nStatic/semi-static information\n\n\n\n\nWeeks since last crisis event\nCount of crisis events\nMental health acuity scores (e.g., depression, anxiety, internalizing, externalizing etc.)\nAge, gender, school district\n\n\nWeeks since first contact\nCount of no-shows\nIdentified risks such as substance use, self-harm or suicide risk\nADHD/Autism diagnosis\n\n\nWeeks since last no-show\nCounts of substances used\nCurrent services accessed\nMental health diagnosis\n\n\nWeeks since last contact\nCounts of phone calls\nIdentified symptoms\nLearning disability diagnosis\n\n\nWeeks since substance misuse identified\nCount of previous completed services\nPreviously indicated need (CHAMPS)\n\n\n\n\nWeeks since self/harm identified\nDischarge and referral counts\nRecent contact with CAS\n\n\n\n\nWeeks since suicide risk identified\nNumber of current services\nRecent psychological consult\n\n\n\n\nWeeks since last discharge\n\n\nRecent diagnosis\n\n\n\n\nWeeks since last crisis episode\n\n\nCurrent caregiver and family support\n\n\n\n\nWeeks since first visit\n\n\n\n\n\n\n\n\nWeeks since referral"
  },
  {
    "objectID": "index.html#target-generation-dependent-variable",
    "href": "index.html#target-generation-dependent-variable",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Target Generation (Dependent Variable)",
    "text": "Target Generation (Dependent Variable)\nThe caseweight prediction task will involve two modeling approaches: a continuous regression problem to estimate weekly provider hours and a classification problem to categorize workload intensity into low, medium, and high levels. Examining both approaches allows for flexibility in how predictions are used in practice (Wang et al., 2021). The continuous regression model provides precise estimates of weekly hours, which are valuable for detailed planning and resource allocation. In contrast, the classification model simplifies workload prediction into actionable categories, which may be more practical for agencies to integrate into decision-making workflows, especially in contexts where exact estimates are less critical or harder to act on (Wang et al., 2021).\nPredictions will be generated weekly, with the model estimating the average weekly provider hours required for the upcoming 28 days using information from weeks prior. To support periodic updates, a rolling window approach will be applied, incorporating newly available data (or the absence of data) at the beginning of each week. This approach, commonly used in real-time predictive systems, allows for continuous refinement of predictions as additional information becomes available (Garriga et al., 2022).\nThe target variable for the regression task will be constructed by aggregating client-related direct and indirect hours logged by clinicians every Friday. These hours will be summed at the weekly level, corresponding to the feature engineering timeline, and aligned with the time recorded prior to each prediction week to prevent data leakage. We will also examine the stability and reliability of the target measure in two forms: the combined total of direct and indirect hours and the number of direct hours on its own, which may be a more stable measure of client-related work than non-direct hours which clinicians may not log consistently."
  },
  {
    "objectID": "index.html#model-selection",
    "href": "index.html#model-selection",
    "title": "Beyond counting clients: Developing a measure of clinician workload with machine learning",
    "section": "Model Selection",
    "text": "Model Selection\nA range of supervised machine learning algorithms were selected to address both regression (continuous provider hours) and classification (categories of provider hours) tasks. Models were selected based on how well-suited they are to handling high-dimensional, tabular datasets like electronic health records (EHRs).\nRandom Forest (RF) is an ensemble learning method that constructs multiple decision trees during training and outputs either the most common classifications or the average predictions from individual trees. RF was chosen for its ability to handle large datasets with numerous features, manage missing data effectively, and capture complex, non-linear relationships. Its built-in feature importance metrics also enhance interpretability, making it a strong candidate for understanding which variables drive predictions.\nXGBoost, a highly efficient implementation of gradient boosting machines (GBMs), was selected due to its superior predictive accuracy, scalability, and ability to handle sparse datasets with missing values. Gradient boosting combines weak learners (typically decision trees) iteratively, optimizing for residual errors at each step to minimize a specified loss function. XGBoost’s regularization techniques, such as shrinkage and column sampling, help prevent overfitting, while its computational efficiency makes it well-suited for large datasets.\nFeed-forward neural networks (FNNs), a class of deep learning models, were included for their flexibility in modelling complex non-linear interactions among variables. FNNs consist of interconnected layers of nodes where each node applies an activation function to transform input data. These networks are particularly useful when relationships between variables are intricate and not easily captured by tree-based methods.\nRecurrent neural networks (RNNs) were added to leverage the sequential nature of the dataset. Unlike FNNs, RNNs include recurrent connections that allow the model to retain information about previous inputs, enabling it to capture temporal dependencies in time-series data. This makes RNNs particularly well-suited for tasks where past events influence future outcomes, such as predicting changes in weekly provider workload based on prior patterns.\nFurthermore, a baseline model will be implemented to replicate how new clients are typically assigned in practices without sophisticated casemix algorithms for comparison. The baseline will rely on a simplified feature set, containing the programming they are accessing and their age. By evaluating all of the models against this baseline, we can better estimate whether machine learning approaches offer any improvement over traditional methods of estimating provider workload.\nEach model will be trained on the same training set and evaluated using identical cross-validation splits to ensure consistency in comparisons. Hyperparameter optimization will be conducted for all algorithms, with 100 trials per model, focusing on minimizing Mean Absolute Error (MAE) for regression tasks and maximizing the Area Under the Receiver Operating Characteristic Curve (AUROC) for classification tasks. This process will ensure that the models are fine-tuned to achieve optimal performance.\nAll models will be compared against the baseline and one another to assess relative performance across both regression and classification tasks. Detailed hyperparameter search spaces and tuning procedures will be documented in the supplementary materials. (Salditt et al., 2023; Sheetal et al., 2023)."
  }
]